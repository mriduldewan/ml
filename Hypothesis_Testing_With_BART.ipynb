{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing with Pre-trained BART Model\n",
    "\n",
    "This code demonstrates hypothesis testing using a pre-trained BartForSequenceClassification model from Facebook's facebook/bart-large-mnli model hub. The model is based on the BART (Bidirectional and Autoregressive Transformers) architecture and has been specifically trained on the Multi-Genre Natural Language Inference (MNLI) dataset.\n",
    "\n",
    "Why BART MNLI for Hypothesis Testing?\n",
    "\n",
    "MNLI is a dataset focused on recognizing entailment, contradiction, and neutral relationships between a premise and a hypothesis. This pre-trained model can leverage its understanding of these relationships to assess the likelihood that the hypothesis holds true given the provided premise.\n",
    "\n",
    "Benefits of using BART MNLI:\n",
    "\n",
    "- *Leverages Existing Knowledge*: By using a pre-trained MNLI model, we benefit from its extensive training on identifying entailment and contradiction, which are crucial aspects of hypothesis testing.\n",
    "- *Reduces Development Time*: Utilizing a pre-trained model saves time and resources compared to training a new model from scratch for this specific task.\n",
    "- *Adaptability*: This approach can be potentially adapted to different hypothesis testing scenarios by adjusting the premise and hypothesis construction based on the domain of the text.\n",
    "\n",
    "This BART model can be fine-tuned for improved performance on our specific hypothesis testing task. Here's why fine-tuning can be beneficial:\n",
    "\n",
    "- *Domain Adaptation*: The pre-trained MNLI model provides a strong foundation in understanding entailment and contradiction. However, fine-tuning on your specific domain data (e.g., sports news in your example) allows the model to adjust its parameters to better capture the nuances of your hypothesis testing needs.\n",
    "- *Improved Accuracy*: By fine-tuning, you can potentially improve the model's ability to distinguish between true and false hypotheses within your domain. The model can learn from your labeled data (premise, hypothesis, true/false labels) to become more accurate in its predictions.\n",
    "- *Focus on Specific Relationships*: While MNLI covers entailment, contradiction, and neutral, your hypothesis testing might focus primarily on entailment. Fine-tuning can help the model prioritize this specific relationship, potentially leading to sharper predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd95ebb536de40ce9816ccacee764e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load model pretrained on MNLI\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "\n",
    "# Load a BART model pre-trained on the Multi-Genre Natural Language Inference (MNLI) dataset\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "model = BartForSequenceClassification.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text sequence to analyze as a premise and the hypothesis to test\n",
    "premise = 'Matildas have reached the QF in the 2023 world cup'\n",
    "\n",
    "hypothesis = 'This text is about women'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode the premise and hypothesis text for model input\n",
    "input_ids = tokenizer.encode(premise, hypothesis, return_tensors='pt')\n",
    "\n",
    "# Get model predictions for entailment, contradiction, and neutral scores\n",
    "logits = model(input_ids)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the label is true: 98.53%\n"
     ]
    }
   ],
   "source": [
    "# Focus on entailment and contradiction scores, discarding the neutral prediction\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "\n",
    "# Calculate probabilities for entailment and contradiction\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "\n",
    "# Extract the probability of the hypothesis being true based on entailment\n",
    "true_prob = probs[:,1].item() * 100\n",
    "\n",
    "print(f'Probability that the label is true: {true_prob:0.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
