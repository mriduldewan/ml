{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Convolutional Neural Networks\n",
    "\n",
    "In this article we'll build a simple convolutional neural network in PyTorch and train it to recognize handwritten digits using the MNIST dataset. Training a classifier on the MNIST dataset can be regarded as the hello world of image recognition.\n",
    "\n",
    "> Dataset: MNIST contains 70,000 images of handwritten digits: 60,000 for training and 10,000 for testing. The images are grayscale, 28x28 pixels, and centered to reduce preprocessing and get started quicker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "# Choose the appropriate device based on availability (CUDA or CPU)\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # For Windows\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # For M1 Mac\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1239f1470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 32\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "log_interval = 100\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train and test data\n",
    "\n",
    "We also need DataLoaders for the dataset. This is where TorchVision comes into play. Let's use load the MNIST dataset directly. We'll use a batch_size of 64 for training and size 1000 for testing on this dataset. The values 0.1307 and 0.3081 used for the Normalize() transformation below are the global mean and standard deviation of the MNIST dataset, so we'll take them as a given here. TorchVision offers a lot of handy transformations, such as cropping or normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and transform train dataset\n",
    "\n",
    "# The values 0.1307 and 0.3081 used for the Normalize() transformation below are the global mean and standard deviation of the MNIST dataset, we'll take them as a given here.\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                              datasets.MNIST('MNIST', train=True, download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                                ])),\n",
    "                              batch_size=batch_size_train, \n",
    "                              shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                              datasets.MNIST('MNIST', train=False, download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.1307,), (0.3081,))\n",
    "                              ])),\n",
    "                              batch_size=batch_size_test, \n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generating some samples from the dataloader\n",
    "examples = enumerate(train_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "# Shape of the sample\n",
    "\"\"\"\n",
    "Interpreting the data - \n",
    "    64 : size of the train batch\n",
    "    1 : Number of channels. Its 1 since its a gray scale image\n",
    "    28, 28 : Resolution of the image\n",
    "\n",
    "\"\"\"\n",
    "example_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)  \n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a neural network\n",
    "\n",
    "Now we're ready to define our neural network. In this case, we'll create a network that consists of 3 fully-connected layers:\n",
    "\n",
    "- An input layer that receives an input value for each feature (in this case, the four penguin measurements) and applies a ReLU activation function.\n",
    "- A hidden layer that receives ten inputs and applies a ReLU activation function.\n",
    "- An output layer that generates a non-negative numeric output for each class (which a loss function will translate into classification probabilities for each of the possible target labels).\n",
    "\n",
    "> Additional note: Even though we are using dropout, we are still calling it a fully connected layet. The presence or absence of dropout doesn't affect the fundamental definition of a fully connected layer. Dropout modifies the training process by temporarily disabling connections, promoting robustness.\n",
    "> Here's an analogy: Imagine a fully connected layer as a densely woven net. Dropout is like temporarily removing some threads from the net during training. The overall structure remains a net, but it becomes less prone to getting stuck on specific connections. Therefore, you can absolutely call a layer with dropout a fully connected layer. The term \"fully connected\" refers to the overall architecture, while dropout addresses a specific training technique for regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=10, out_channels=20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        # 320 -> 50\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # 50 -> 10\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # transform to logits\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model instance from the network\n",
    "clf = CNNClassifier().to(device)\n",
    "\n",
    "# Define Kaiming Normal weight initialization function\n",
    "def weights_init(m):\n",
    "    if type(m) == nn.Conv2d or type(m) == nn.Linear:\n",
    "        init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "# Apply weight initialization to the model\n",
    "clf.apply(weights_init)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "opt = optim.SGD(clf.parameters(), lr=learning_rate, momentum=momentum)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Empty list to store loss and accuracy for each epoch\n",
    "acc_history= []\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs)]\n",
    "\n",
    "\n",
    "# Train the model\n",
    "def train(epoch):\n",
    "    clf.train() # Set model in training mode\n",
    "\n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        opt.zero_grad()\n",
    "        output = clf(data)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        opt.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "          print('Training Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch+1, batch_id * len(data), len(train_loader.dataset),\n",
    "            100. * batch_id / len(train_loader), loss.item()))\n",
    "          \n",
    "          train_counter.append((batch_id*batch_size_train) + ((epoch)*len(train_loader.dataset)))\n",
    "          \n",
    "def eval():\n",
    "    clf.eval() # set model in inference mode (need this to avoid backpropagation)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            output = clf(data)\n",
    "            test_loss += criterion(output, label)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "    \n",
    "    # Calculate the average test loss\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "\n",
    "    # Print test loss and accuracy for each epoch    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 1 [0/60000 (0%)]\tLoss: 9.031775\n",
      "Training Epoch: 1 [32/60000 (0%)]\tLoss: 6.298464\n",
      "Training Epoch: 1 [64/60000 (0%)]\tLoss: 3.922478\n",
      "Training Epoch: 1 [96/60000 (0%)]\tLoss: 2.450428\n",
      "Training Epoch: 1 [128/60000 (0%)]\tLoss: 2.609453\n",
      "Training Epoch: 1 [160/60000 (0%)]\tLoss: 2.245797\n",
      "Training Epoch: 1 [192/60000 (0%)]\tLoss: 2.135039\n",
      "Training Epoch: 1 [224/60000 (0%)]\tLoss: 2.381904\n",
      "Training Epoch: 1 [256/60000 (0%)]\tLoss: 2.346053\n",
      "Training Epoch: 1 [288/60000 (0%)]\tLoss: 2.177135\n",
      "Training Epoch: 1 [320/60000 (1%)]\tLoss: 2.113847\n",
      "Training Epoch: 1 [352/60000 (1%)]\tLoss: 2.159397\n",
      "Training Epoch: 1 [384/60000 (1%)]\tLoss: 2.286891\n",
      "Training Epoch: 1 [416/60000 (1%)]\tLoss: 2.228845\n",
      "Training Epoch: 1 [448/60000 (1%)]\tLoss: 2.120340\n",
      "Training Epoch: 1 [480/60000 (1%)]\tLoss: 2.191727\n",
      "Training Epoch: 1 [512/60000 (1%)]\tLoss: 2.072265\n",
      "Training Epoch: 1 [544/60000 (1%)]\tLoss: 2.248223\n",
      "Training Epoch: 1 [576/60000 (1%)]\tLoss: 2.446813\n",
      "Training Epoch: 1 [608/60000 (1%)]\tLoss: 2.221024\n",
      "Training Epoch: 1 [640/60000 (1%)]\tLoss: 2.209044\n",
      "Training Epoch: 1 [672/60000 (1%)]\tLoss: 2.155333\n",
      "Training Epoch: 1 [704/60000 (1%)]\tLoss: 2.267549\n",
      "Training Epoch: 1 [736/60000 (1%)]\tLoss: 2.198578\n",
      "Training Epoch: 1 [768/60000 (1%)]\tLoss: 2.126041\n",
      "Training Epoch: 1 [800/60000 (1%)]\tLoss: 1.963279\n",
      "Training Epoch: 1 [832/60000 (1%)]\tLoss: 1.942679\n",
      "Training Epoch: 1 [864/60000 (1%)]\tLoss: 2.084598\n",
      "Training Epoch: 1 [896/60000 (1%)]\tLoss: 2.077279\n",
      "Training Epoch: 1 [928/60000 (2%)]\tLoss: 1.977942\n",
      "Training Epoch: 1 [960/60000 (2%)]\tLoss: 2.126770\n",
      "Training Epoch: 1 [992/60000 (2%)]\tLoss: 2.136158\n",
      "Training Epoch: 1 [1024/60000 (2%)]\tLoss: 2.161341\n",
      "Training Epoch: 1 [1056/60000 (2%)]\tLoss: 1.905472\n",
      "Training Epoch: 1 [1088/60000 (2%)]\tLoss: 1.731116\n",
      "Training Epoch: 1 [1120/60000 (2%)]\tLoss: 1.818876\n",
      "Training Epoch: 1 [1152/60000 (2%)]\tLoss: 2.174053\n",
      "Training Epoch: 1 [1184/60000 (2%)]\tLoss: 1.919695\n",
      "Training Epoch: 1 [1216/60000 (2%)]\tLoss: 2.256499\n",
      "Training Epoch: 1 [1248/60000 (2%)]\tLoss: 2.177413\n",
      "Training Epoch: 1 [1280/60000 (2%)]\tLoss: 1.700048\n",
      "Training Epoch: 1 [1312/60000 (2%)]\tLoss: 2.055748\n",
      "Training Epoch: 1 [1344/60000 (2%)]\tLoss: 1.994242\n",
      "Training Epoch: 1 [1376/60000 (2%)]\tLoss: 1.857927\n",
      "Training Epoch: 1 [1408/60000 (2%)]\tLoss: 1.989330\n",
      "Training Epoch: 1 [1440/60000 (2%)]\tLoss: 1.752604\n",
      "Training Epoch: 1 [1472/60000 (2%)]\tLoss: 1.879003\n",
      "Training Epoch: 1 [1504/60000 (3%)]\tLoss: 2.044409\n",
      "Training Epoch: 1 [1536/60000 (3%)]\tLoss: 1.850031\n",
      "Training Epoch: 1 [1568/60000 (3%)]\tLoss: 1.994390\n",
      "Training Epoch: 1 [1600/60000 (3%)]\tLoss: 2.047794\n",
      "Training Epoch: 1 [1632/60000 (3%)]\tLoss: 2.145883\n",
      "Training Epoch: 1 [1664/60000 (3%)]\tLoss: 1.959206\n",
      "Training Epoch: 1 [1696/60000 (3%)]\tLoss: 1.866413\n",
      "Training Epoch: 1 [1728/60000 (3%)]\tLoss: 1.728772\n",
      "Training Epoch: 1 [1760/60000 (3%)]\tLoss: 1.976071\n",
      "Training Epoch: 1 [1792/60000 (3%)]\tLoss: 2.064561\n",
      "Training Epoch: 1 [1824/60000 (3%)]\tLoss: 1.715716\n",
      "Training Epoch: 1 [1856/60000 (3%)]\tLoss: 1.799146\n",
      "Training Epoch: 1 [1888/60000 (3%)]\tLoss: 1.689131\n",
      "Training Epoch: 1 [1920/60000 (3%)]\tLoss: 1.720387\n",
      "Training Epoch: 1 [1952/60000 (3%)]\tLoss: 2.079834\n",
      "Training Epoch: 1 [1984/60000 (3%)]\tLoss: 1.520940\n",
      "Training Epoch: 1 [2016/60000 (3%)]\tLoss: 1.466346\n",
      "Training Epoch: 1 [2048/60000 (3%)]\tLoss: 1.511662\n",
      "Training Epoch: 1 [2080/60000 (3%)]\tLoss: 1.619217\n",
      "Training Epoch: 1 [2112/60000 (4%)]\tLoss: 1.759252\n",
      "Training Epoch: 1 [2144/60000 (4%)]\tLoss: 1.530401\n",
      "Training Epoch: 1 [2176/60000 (4%)]\tLoss: 1.645937\n",
      "Training Epoch: 1 [2208/60000 (4%)]\tLoss: 1.593796\n",
      "Training Epoch: 1 [2240/60000 (4%)]\tLoss: 1.734447\n",
      "Training Epoch: 1 [2272/60000 (4%)]\tLoss: 1.840323\n",
      "Training Epoch: 1 [2304/60000 (4%)]\tLoss: 1.389265\n",
      "Training Epoch: 1 [2336/60000 (4%)]\tLoss: 1.637668\n",
      "Training Epoch: 1 [2368/60000 (4%)]\tLoss: 1.676674\n",
      "Training Epoch: 1 [2400/60000 (4%)]\tLoss: 1.801157\n",
      "Training Epoch: 1 [2432/60000 (4%)]\tLoss: 1.660401\n",
      "Training Epoch: 1 [2464/60000 (4%)]\tLoss: 1.770092\n",
      "Training Epoch: 1 [2496/60000 (4%)]\tLoss: 1.706305\n",
      "Training Epoch: 1 [2528/60000 (4%)]\tLoss: 1.400049\n",
      "Training Epoch: 1 [2560/60000 (4%)]\tLoss: 1.331474\n",
      "Training Epoch: 1 [2592/60000 (4%)]\tLoss: 1.825708\n",
      "Training Epoch: 1 [2624/60000 (4%)]\tLoss: 1.465000\n",
      "Training Epoch: 1 [2656/60000 (4%)]\tLoss: 1.499666\n",
      "Training Epoch: 1 [2688/60000 (4%)]\tLoss: 1.383046\n",
      "Training Epoch: 1 [2720/60000 (5%)]\tLoss: 1.729892\n",
      "Training Epoch: 1 [2752/60000 (5%)]\tLoss: 2.079523\n",
      "Training Epoch: 1 [2784/60000 (5%)]\tLoss: 1.362958\n",
      "Training Epoch: 1 [2816/60000 (5%)]\tLoss: 1.425006\n",
      "Training Epoch: 1 [2848/60000 (5%)]\tLoss: 1.744159\n",
      "Training Epoch: 1 [2880/60000 (5%)]\tLoss: 1.987000\n",
      "Training Epoch: 1 [2912/60000 (5%)]\tLoss: 1.460899\n",
      "Training Epoch: 1 [2944/60000 (5%)]\tLoss: 1.734265\n",
      "Training Epoch: 1 [2976/60000 (5%)]\tLoss: 1.611293\n",
      "Training Epoch: 1 [3008/60000 (5%)]\tLoss: 1.926964\n",
      "Training Epoch: 1 [3040/60000 (5%)]\tLoss: 1.898335\n",
      "Training Epoch: 1 [3072/60000 (5%)]\tLoss: 1.472716\n",
      "Training Epoch: 1 [3104/60000 (5%)]\tLoss: 1.817346\n",
      "Training Epoch: 1 [3136/60000 (5%)]\tLoss: 1.642098\n",
      "Training Epoch: 1 [3168/60000 (5%)]\tLoss: 1.651371\n",
      "Training Epoch: 1 [3200/60000 (5%)]\tLoss: 1.825195\n",
      "Training Epoch: 1 [3232/60000 (5%)]\tLoss: 1.436151\n",
      "Training Epoch: 1 [3264/60000 (5%)]\tLoss: 1.279929\n",
      "Training Epoch: 1 [3296/60000 (5%)]\tLoss: 1.576643\n",
      "Training Epoch: 1 [3328/60000 (6%)]\tLoss: 1.453389\n",
      "Training Epoch: 1 [3360/60000 (6%)]\tLoss: 1.615302\n",
      "Training Epoch: 1 [3392/60000 (6%)]\tLoss: 1.652853\n",
      "Training Epoch: 1 [3424/60000 (6%)]\tLoss: 1.326953\n",
      "Training Epoch: 1 [3456/60000 (6%)]\tLoss: 1.624108\n",
      "Training Epoch: 1 [3488/60000 (6%)]\tLoss: 1.567823\n",
      "Training Epoch: 1 [3520/60000 (6%)]\tLoss: 1.644440\n",
      "Training Epoch: 1 [3552/60000 (6%)]\tLoss: 1.690460\n",
      "Training Epoch: 1 [3584/60000 (6%)]\tLoss: 1.594634\n",
      "Training Epoch: 1 [3616/60000 (6%)]\tLoss: 1.281638\n",
      "Training Epoch: 1 [3648/60000 (6%)]\tLoss: 1.121766\n",
      "Training Epoch: 1 [3680/60000 (6%)]\tLoss: 1.888090\n",
      "Training Epoch: 1 [3712/60000 (6%)]\tLoss: 1.654978\n",
      "Training Epoch: 1 [3744/60000 (6%)]\tLoss: 1.956490\n",
      "Training Epoch: 1 [3776/60000 (6%)]\tLoss: 1.026192\n",
      "Training Epoch: 1 [3808/60000 (6%)]\tLoss: 1.360634\n",
      "Training Epoch: 1 [3840/60000 (6%)]\tLoss: 1.386364\n",
      "Training Epoch: 1 [3872/60000 (6%)]\tLoss: 1.480340\n",
      "Training Epoch: 1 [3904/60000 (7%)]\tLoss: 1.505726\n",
      "Training Epoch: 1 [3936/60000 (7%)]\tLoss: 1.460349\n",
      "Training Epoch: 1 [3968/60000 (7%)]\tLoss: 1.408490\n",
      "Training Epoch: 1 [4000/60000 (7%)]\tLoss: 1.856944\n",
      "Training Epoch: 1 [4032/60000 (7%)]\tLoss: 1.398882\n",
      "Training Epoch: 1 [4064/60000 (7%)]\tLoss: 1.155049\n",
      "Training Epoch: 1 [4096/60000 (7%)]\tLoss: 1.088122\n",
      "Training Epoch: 1 [4128/60000 (7%)]\tLoss: 1.294544\n",
      "Training Epoch: 1 [4160/60000 (7%)]\tLoss: 1.349343\n",
      "Training Epoch: 1 [4192/60000 (7%)]\tLoss: 1.330073\n",
      "Training Epoch: 1 [4224/60000 (7%)]\tLoss: 1.142334\n",
      "Training Epoch: 1 [4256/60000 (7%)]\tLoss: 1.562096\n",
      "Training Epoch: 1 [4288/60000 (7%)]\tLoss: 1.452559\n",
      "Training Epoch: 1 [4320/60000 (7%)]\tLoss: 1.182417\n",
      "Training Epoch: 1 [4352/60000 (7%)]\tLoss: 1.197356\n",
      "Training Epoch: 1 [4384/60000 (7%)]\tLoss: 1.206686\n",
      "Training Epoch: 1 [4416/60000 (7%)]\tLoss: 1.134615\n",
      "Training Epoch: 1 [4448/60000 (7%)]\tLoss: 1.467774\n",
      "Training Epoch: 1 [4480/60000 (7%)]\tLoss: 1.070104\n",
      "Training Epoch: 1 [4512/60000 (8%)]\tLoss: 1.179682\n",
      "Training Epoch: 1 [4544/60000 (8%)]\tLoss: 1.131278\n",
      "Training Epoch: 1 [4576/60000 (8%)]\tLoss: 1.094487\n",
      "Training Epoch: 1 [4608/60000 (8%)]\tLoss: 1.322921\n",
      "Training Epoch: 1 [4640/60000 (8%)]\tLoss: 1.467481\n",
      "Training Epoch: 1 [4672/60000 (8%)]\tLoss: 1.158617\n",
      "Training Epoch: 1 [4704/60000 (8%)]\tLoss: 1.222992\n",
      "Training Epoch: 1 [4736/60000 (8%)]\tLoss: 1.741612\n",
      "Training Epoch: 1 [4768/60000 (8%)]\tLoss: 1.121870\n",
      "Training Epoch: 1 [4800/60000 (8%)]\tLoss: 1.257672\n",
      "Training Epoch: 1 [4832/60000 (8%)]\tLoss: 1.533147\n",
      "Training Epoch: 1 [4864/60000 (8%)]\tLoss: 1.131330\n",
      "Training Epoch: 1 [4896/60000 (8%)]\tLoss: 1.072142\n",
      "Training Epoch: 1 [4928/60000 (8%)]\tLoss: 1.196415\n",
      "Training Epoch: 1 [4960/60000 (8%)]\tLoss: 1.356992\n",
      "Training Epoch: 1 [4992/60000 (8%)]\tLoss: 1.162755\n",
      "Training Epoch: 1 [5024/60000 (8%)]\tLoss: 1.278316\n",
      "Training Epoch: 1 [5056/60000 (8%)]\tLoss: 1.386399\n",
      "Training Epoch: 1 [5088/60000 (8%)]\tLoss: 1.267390\n",
      "Training Epoch: 1 [5120/60000 (9%)]\tLoss: 0.789560\n",
      "Training Epoch: 1 [5152/60000 (9%)]\tLoss: 1.442325\n",
      "Training Epoch: 1 [5184/60000 (9%)]\tLoss: 1.107204\n",
      "Training Epoch: 1 [5216/60000 (9%)]\tLoss: 1.306484\n",
      "Training Epoch: 1 [5248/60000 (9%)]\tLoss: 1.791141\n",
      "Training Epoch: 1 [5280/60000 (9%)]\tLoss: 1.341991\n",
      "Training Epoch: 1 [5312/60000 (9%)]\tLoss: 1.308642\n",
      "Training Epoch: 1 [5344/60000 (9%)]\tLoss: 1.323412\n",
      "Training Epoch: 1 [5376/60000 (9%)]\tLoss: 1.182425\n",
      "Training Epoch: 1 [5408/60000 (9%)]\tLoss: 0.948615\n",
      "Training Epoch: 1 [5440/60000 (9%)]\tLoss: 0.963449\n",
      "Training Epoch: 1 [5472/60000 (9%)]\tLoss: 1.274867\n",
      "Training Epoch: 1 [5504/60000 (9%)]\tLoss: 1.309085\n",
      "Training Epoch: 1 [5536/60000 (9%)]\tLoss: 1.571451\n",
      "Training Epoch: 1 [5568/60000 (9%)]\tLoss: 1.662488\n",
      "Training Epoch: 1 [5600/60000 (9%)]\tLoss: 1.616129\n",
      "Training Epoch: 1 [5632/60000 (9%)]\tLoss: 1.070187\n",
      "Training Epoch: 1 [5664/60000 (9%)]\tLoss: 1.669915\n",
      "Training Epoch: 1 [5696/60000 (9%)]\tLoss: 1.771004\n",
      "Training Epoch: 1 [5728/60000 (10%)]\tLoss: 1.331122\n",
      "Training Epoch: 1 [5760/60000 (10%)]\tLoss: 1.449140\n",
      "Training Epoch: 1 [5792/60000 (10%)]\tLoss: 1.110954\n",
      "Training Epoch: 1 [5824/60000 (10%)]\tLoss: 1.255200\n",
      "Training Epoch: 1 [5856/60000 (10%)]\tLoss: 1.490301\n",
      "Training Epoch: 1 [5888/60000 (10%)]\tLoss: 1.083654\n",
      "Training Epoch: 1 [5920/60000 (10%)]\tLoss: 1.596370\n",
      "Training Epoch: 1 [5952/60000 (10%)]\tLoss: 1.095221\n",
      "Training Epoch: 1 [5984/60000 (10%)]\tLoss: 1.317427\n",
      "Training Epoch: 1 [6016/60000 (10%)]\tLoss: 1.197308\n",
      "Training Epoch: 1 [6048/60000 (10%)]\tLoss: 1.111581\n",
      "Training Epoch: 1 [6080/60000 (10%)]\tLoss: 1.181138\n",
      "Training Epoch: 1 [6112/60000 (10%)]\tLoss: 1.213701\n",
      "Training Epoch: 1 [6144/60000 (10%)]\tLoss: 1.392777\n",
      "Training Epoch: 1 [6176/60000 (10%)]\tLoss: 1.203876\n",
      "Training Epoch: 1 [6208/60000 (10%)]\tLoss: 1.171207\n",
      "Training Epoch: 1 [6240/60000 (10%)]\tLoss: 0.910802\n",
      "Training Epoch: 1 [6272/60000 (10%)]\tLoss: 1.105929\n",
      "Training Epoch: 1 [6304/60000 (11%)]\tLoss: 1.222829\n",
      "Training Epoch: 1 [6336/60000 (11%)]\tLoss: 1.260098\n",
      "Training Epoch: 1 [6368/60000 (11%)]\tLoss: 1.064434\n",
      "Training Epoch: 1 [6400/60000 (11%)]\tLoss: 1.096905\n",
      "Training Epoch: 1 [6432/60000 (11%)]\tLoss: 1.649712\n",
      "Training Epoch: 1 [6464/60000 (11%)]\tLoss: 0.917092\n",
      "Training Epoch: 1 [6496/60000 (11%)]\tLoss: 1.163770\n",
      "Training Epoch: 1 [6528/60000 (11%)]\tLoss: 0.969439\n",
      "Training Epoch: 1 [6560/60000 (11%)]\tLoss: 1.106551\n",
      "Training Epoch: 1 [6592/60000 (11%)]\tLoss: 1.046444\n",
      "Training Epoch: 1 [6624/60000 (11%)]\tLoss: 0.928291\n",
      "Training Epoch: 1 [6656/60000 (11%)]\tLoss: 0.929871\n",
      "Training Epoch: 1 [6688/60000 (11%)]\tLoss: 1.491437\n",
      "Training Epoch: 1 [6720/60000 (11%)]\tLoss: 1.212345\n",
      "Training Epoch: 1 [6752/60000 (11%)]\tLoss: 1.199306\n",
      "Training Epoch: 1 [6784/60000 (11%)]\tLoss: 1.383618\n",
      "Training Epoch: 1 [6816/60000 (11%)]\tLoss: 0.874164\n",
      "Training Epoch: 1 [6848/60000 (11%)]\tLoss: 1.143296\n",
      "Training Epoch: 1 [6880/60000 (11%)]\tLoss: 1.452577\n",
      "Training Epoch: 1 [6912/60000 (12%)]\tLoss: 0.801008\n",
      "Training Epoch: 1 [6944/60000 (12%)]\tLoss: 1.138541\n",
      "Training Epoch: 1 [6976/60000 (12%)]\tLoss: 1.038120\n",
      "Training Epoch: 1 [7008/60000 (12%)]\tLoss: 1.181585\n",
      "Training Epoch: 1 [7040/60000 (12%)]\tLoss: 1.176990\n",
      "Training Epoch: 1 [7072/60000 (12%)]\tLoss: 1.201436\n",
      "Training Epoch: 1 [7104/60000 (12%)]\tLoss: 1.193406\n",
      "Training Epoch: 1 [7136/60000 (12%)]\tLoss: 1.353407\n",
      "Training Epoch: 1 [7168/60000 (12%)]\tLoss: 1.198888\n",
      "Training Epoch: 1 [7200/60000 (12%)]\tLoss: 1.057692\n",
      "Training Epoch: 1 [7232/60000 (12%)]\tLoss: 1.219695\n",
      "Training Epoch: 1 [7264/60000 (12%)]\tLoss: 1.107704\n",
      "Training Epoch: 1 [7296/60000 (12%)]\tLoss: 0.927540\n",
      "Training Epoch: 1 [7328/60000 (12%)]\tLoss: 0.921934\n",
      "Training Epoch: 1 [7360/60000 (12%)]\tLoss: 1.021351\n",
      "Training Epoch: 1 [7392/60000 (12%)]\tLoss: 1.430279\n",
      "Training Epoch: 1 [7424/60000 (12%)]\tLoss: 0.764334\n",
      "Training Epoch: 1 [7456/60000 (12%)]\tLoss: 1.247216\n",
      "Training Epoch: 1 [7488/60000 (12%)]\tLoss: 1.217354\n",
      "Training Epoch: 1 [7520/60000 (13%)]\tLoss: 1.060858\n",
      "Training Epoch: 1 [7552/60000 (13%)]\tLoss: 0.957420\n",
      "Training Epoch: 1 [7584/60000 (13%)]\tLoss: 1.288959\n",
      "Training Epoch: 1 [7616/60000 (13%)]\tLoss: 1.189998\n",
      "Training Epoch: 1 [7648/60000 (13%)]\tLoss: 0.645408\n",
      "Training Epoch: 1 [7680/60000 (13%)]\tLoss: 1.141006\n",
      "Training Epoch: 1 [7712/60000 (13%)]\tLoss: 0.700495\n",
      "Training Epoch: 1 [7744/60000 (13%)]\tLoss: 1.103240\n",
      "Training Epoch: 1 [7776/60000 (13%)]\tLoss: 0.931086\n",
      "Training Epoch: 1 [7808/60000 (13%)]\tLoss: 1.456431\n",
      "Training Epoch: 1 [7840/60000 (13%)]\tLoss: 1.672680\n",
      "Training Epoch: 1 [7872/60000 (13%)]\tLoss: 1.361562\n",
      "Training Epoch: 1 [7904/60000 (13%)]\tLoss: 0.900221\n",
      "Training Epoch: 1 [7936/60000 (13%)]\tLoss: 1.132842\n",
      "Training Epoch: 1 [7968/60000 (13%)]\tLoss: 0.914066\n",
      "Training Epoch: 1 [8000/60000 (13%)]\tLoss: 1.186936\n",
      "Training Epoch: 1 [8032/60000 (13%)]\tLoss: 0.750398\n",
      "Training Epoch: 1 [8064/60000 (13%)]\tLoss: 1.075765\n",
      "Training Epoch: 1 [8096/60000 (13%)]\tLoss: 0.920110\n",
      "Training Epoch: 1 [8128/60000 (14%)]\tLoss: 1.090711\n",
      "Training Epoch: 1 [8160/60000 (14%)]\tLoss: 1.165682\n",
      "Training Epoch: 1 [8192/60000 (14%)]\tLoss: 1.092000\n",
      "Training Epoch: 1 [8224/60000 (14%)]\tLoss: 0.678630\n",
      "Training Epoch: 1 [8256/60000 (14%)]\tLoss: 1.037026\n",
      "Training Epoch: 1 [8288/60000 (14%)]\tLoss: 1.001299\n",
      "Training Epoch: 1 [8320/60000 (14%)]\tLoss: 1.113427\n",
      "Training Epoch: 1 [8352/60000 (14%)]\tLoss: 1.180663\n",
      "Training Epoch: 1 [8384/60000 (14%)]\tLoss: 1.211213\n",
      "Training Epoch: 1 [8416/60000 (14%)]\tLoss: 0.748164\n",
      "Training Epoch: 1 [8448/60000 (14%)]\tLoss: 0.837863\n",
      "Training Epoch: 1 [8480/60000 (14%)]\tLoss: 0.823553\n",
      "Training Epoch: 1 [8512/60000 (14%)]\tLoss: 0.867426\n",
      "Training Epoch: 1 [8544/60000 (14%)]\tLoss: 0.692945\n",
      "Training Epoch: 1 [8576/60000 (14%)]\tLoss: 0.663358\n",
      "Training Epoch: 1 [8608/60000 (14%)]\tLoss: 1.004536\n",
      "Training Epoch: 1 [8640/60000 (14%)]\tLoss: 1.630458\n",
      "Training Epoch: 1 [8672/60000 (14%)]\tLoss: 1.244506\n",
      "Training Epoch: 1 [8704/60000 (15%)]\tLoss: 0.673008\n",
      "Training Epoch: 1 [8736/60000 (15%)]\tLoss: 0.947787\n",
      "Training Epoch: 1 [8768/60000 (15%)]\tLoss: 1.486811\n",
      "Training Epoch: 1 [8800/60000 (15%)]\tLoss: 1.162930\n",
      "Training Epoch: 1 [8832/60000 (15%)]\tLoss: 1.108566\n",
      "Training Epoch: 1 [8864/60000 (15%)]\tLoss: 0.859040\n",
      "Training Epoch: 1 [8896/60000 (15%)]\tLoss: 1.144108\n",
      "Training Epoch: 1 [8928/60000 (15%)]\tLoss: 0.952678\n",
      "Training Epoch: 1 [8960/60000 (15%)]\tLoss: 0.806380\n",
      "Training Epoch: 1 [8992/60000 (15%)]\tLoss: 1.007217\n",
      "Training Epoch: 1 [9024/60000 (15%)]\tLoss: 1.169988\n",
      "Training Epoch: 1 [9056/60000 (15%)]\tLoss: 1.478563\n",
      "Training Epoch: 1 [9088/60000 (15%)]\tLoss: 1.495023\n",
      "Training Epoch: 1 [9120/60000 (15%)]\tLoss: 0.898152\n",
      "Training Epoch: 1 [9152/60000 (15%)]\tLoss: 0.646937\n",
      "Training Epoch: 1 [9184/60000 (15%)]\tLoss: 1.024396\n",
      "Training Epoch: 1 [9216/60000 (15%)]\tLoss: 1.333351\n",
      "Training Epoch: 1 [9248/60000 (15%)]\tLoss: 1.197437\n",
      "Training Epoch: 1 [9280/60000 (15%)]\tLoss: 0.980123\n",
      "Training Epoch: 1 [9312/60000 (16%)]\tLoss: 1.315652\n",
      "Training Epoch: 1 [9344/60000 (16%)]\tLoss: 1.095125\n",
      "Training Epoch: 1 [9376/60000 (16%)]\tLoss: 1.125622\n",
      "Training Epoch: 1 [9408/60000 (16%)]\tLoss: 0.995240\n",
      "Training Epoch: 1 [9440/60000 (16%)]\tLoss: 1.165327\n",
      "Training Epoch: 1 [9472/60000 (16%)]\tLoss: 1.168773\n",
      "Training Epoch: 1 [9504/60000 (16%)]\tLoss: 1.212003\n",
      "Training Epoch: 1 [9536/60000 (16%)]\tLoss: 0.863314\n",
      "Training Epoch: 1 [9568/60000 (16%)]\tLoss: 0.997271\n",
      "Training Epoch: 1 [9600/60000 (16%)]\tLoss: 0.582325\n",
      "Training Epoch: 1 [9632/60000 (16%)]\tLoss: 1.460949\n",
      "Training Epoch: 1 [9664/60000 (16%)]\tLoss: 0.919250\n",
      "Training Epoch: 1 [9696/60000 (16%)]\tLoss: 1.039266\n",
      "Training Epoch: 1 [9728/60000 (16%)]\tLoss: 0.902567\n",
      "Training Epoch: 1 [9760/60000 (16%)]\tLoss: 0.719237\n",
      "Training Epoch: 1 [9792/60000 (16%)]\tLoss: 0.717951\n",
      "Training Epoch: 1 [9824/60000 (16%)]\tLoss: 1.076899\n",
      "Training Epoch: 1 [9856/60000 (16%)]\tLoss: 0.878124\n",
      "Training Epoch: 1 [9888/60000 (16%)]\tLoss: 0.957994\n",
      "Training Epoch: 1 [9920/60000 (17%)]\tLoss: 0.989801\n",
      "Training Epoch: 1 [9952/60000 (17%)]\tLoss: 0.904725\n",
      "Training Epoch: 1 [9984/60000 (17%)]\tLoss: 0.895860\n",
      "Training Epoch: 1 [10016/60000 (17%)]\tLoss: 1.167297\n",
      "Training Epoch: 1 [10048/60000 (17%)]\tLoss: 1.388012\n",
      "Training Epoch: 1 [10080/60000 (17%)]\tLoss: 0.666394\n",
      "Training Epoch: 1 [10112/60000 (17%)]\tLoss: 0.656487\n",
      "Training Epoch: 1 [10144/60000 (17%)]\tLoss: 0.535691\n",
      "Training Epoch: 1 [10176/60000 (17%)]\tLoss: 1.034551\n",
      "Training Epoch: 1 [10208/60000 (17%)]\tLoss: 1.007193\n",
      "Training Epoch: 1 [10240/60000 (17%)]\tLoss: 1.018997\n",
      "Training Epoch: 1 [10272/60000 (17%)]\tLoss: 0.892378\n",
      "Training Epoch: 1 [10304/60000 (17%)]\tLoss: 0.955510\n",
      "Training Epoch: 1 [10336/60000 (17%)]\tLoss: 0.859108\n",
      "Training Epoch: 1 [10368/60000 (17%)]\tLoss: 1.219299\n",
      "Training Epoch: 1 [10400/60000 (17%)]\tLoss: 1.073465\n",
      "Training Epoch: 1 [10432/60000 (17%)]\tLoss: 1.434786\n",
      "Training Epoch: 1 [10464/60000 (17%)]\tLoss: 0.790526\n",
      "Training Epoch: 1 [10496/60000 (17%)]\tLoss: 1.032589\n",
      "Training Epoch: 1 [10528/60000 (18%)]\tLoss: 0.910240\n",
      "Training Epoch: 1 [10560/60000 (18%)]\tLoss: 1.250865\n",
      "Training Epoch: 1 [10592/60000 (18%)]\tLoss: 0.985732\n",
      "Training Epoch: 1 [10624/60000 (18%)]\tLoss: 1.391750\n",
      "Training Epoch: 1 [10656/60000 (18%)]\tLoss: 1.265583\n",
      "Training Epoch: 1 [10688/60000 (18%)]\tLoss: 1.123328\n",
      "Training Epoch: 1 [10720/60000 (18%)]\tLoss: 0.866141\n",
      "Training Epoch: 1 [10752/60000 (18%)]\tLoss: 1.205617\n",
      "Training Epoch: 1 [10784/60000 (18%)]\tLoss: 0.673385\n",
      "Training Epoch: 1 [10816/60000 (18%)]\tLoss: 0.886671\n",
      "Training Epoch: 1 [10848/60000 (18%)]\tLoss: 0.865617\n",
      "Training Epoch: 1 [10880/60000 (18%)]\tLoss: 1.498240\n",
      "Training Epoch: 1 [10912/60000 (18%)]\tLoss: 1.339984\n",
      "Training Epoch: 1 [10944/60000 (18%)]\tLoss: 0.956373\n",
      "Training Epoch: 1 [10976/60000 (18%)]\tLoss: 0.591347\n",
      "Training Epoch: 1 [11008/60000 (18%)]\tLoss: 1.161741\n",
      "Training Epoch: 1 [11040/60000 (18%)]\tLoss: 0.814482\n",
      "Training Epoch: 1 [11072/60000 (18%)]\tLoss: 1.109857\n",
      "Training Epoch: 1 [11104/60000 (19%)]\tLoss: 0.938523\n",
      "Training Epoch: 1 [11136/60000 (19%)]\tLoss: 0.963431\n",
      "Training Epoch: 1 [11168/60000 (19%)]\tLoss: 0.868496\n",
      "Training Epoch: 1 [11200/60000 (19%)]\tLoss: 1.013763\n",
      "Training Epoch: 1 [11232/60000 (19%)]\tLoss: 0.693307\n",
      "Training Epoch: 1 [11264/60000 (19%)]\tLoss: 0.672543\n",
      "Training Epoch: 1 [11296/60000 (19%)]\tLoss: 1.059405\n",
      "Training Epoch: 1 [11328/60000 (19%)]\tLoss: 0.859132\n",
      "Training Epoch: 1 [11360/60000 (19%)]\tLoss: 0.793829\n",
      "Training Epoch: 1 [11392/60000 (19%)]\tLoss: 1.162831\n",
      "Training Epoch: 1 [11424/60000 (19%)]\tLoss: 0.730423\n",
      "Training Epoch: 1 [11456/60000 (19%)]\tLoss: 0.777027\n",
      "Training Epoch: 1 [11488/60000 (19%)]\tLoss: 1.063335\n",
      "Training Epoch: 1 [11520/60000 (19%)]\tLoss: 0.765502\n",
      "Training Epoch: 1 [11552/60000 (19%)]\tLoss: 1.054142\n",
      "Training Epoch: 1 [11584/60000 (19%)]\tLoss: 0.832101\n",
      "Training Epoch: 1 [11616/60000 (19%)]\tLoss: 0.593104\n",
      "Training Epoch: 1 [11648/60000 (19%)]\tLoss: 0.753739\n",
      "Training Epoch: 1 [11680/60000 (19%)]\tLoss: 1.093307\n",
      "Training Epoch: 1 [11712/60000 (20%)]\tLoss: 0.719980\n",
      "Training Epoch: 1 [11744/60000 (20%)]\tLoss: 1.050609\n",
      "Training Epoch: 1 [11776/60000 (20%)]\tLoss: 0.997442\n",
      "Training Epoch: 1 [11808/60000 (20%)]\tLoss: 0.837291\n",
      "Training Epoch: 1 [11840/60000 (20%)]\tLoss: 0.804131\n",
      "Training Epoch: 1 [11872/60000 (20%)]\tLoss: 1.061796\n",
      "Training Epoch: 1 [11904/60000 (20%)]\tLoss: 0.740004\n",
      "Training Epoch: 1 [11936/60000 (20%)]\tLoss: 0.600332\n",
      "Training Epoch: 1 [11968/60000 (20%)]\tLoss: 1.000968\n",
      "Training Epoch: 1 [12000/60000 (20%)]\tLoss: 0.622522\n",
      "Training Epoch: 1 [12032/60000 (20%)]\tLoss: 1.149727\n",
      "Training Epoch: 1 [12064/60000 (20%)]\tLoss: 1.087385\n",
      "Training Epoch: 1 [12096/60000 (20%)]\tLoss: 0.827725\n",
      "Training Epoch: 1 [12128/60000 (20%)]\tLoss: 0.861848\n",
      "Training Epoch: 1 [12160/60000 (20%)]\tLoss: 0.878647\n",
      "Training Epoch: 1 [12192/60000 (20%)]\tLoss: 1.049504\n",
      "Training Epoch: 1 [12224/60000 (20%)]\tLoss: 0.821996\n",
      "Training Epoch: 1 [12256/60000 (20%)]\tLoss: 1.227212\n",
      "Training Epoch: 1 [12288/60000 (20%)]\tLoss: 0.624074\n",
      "Training Epoch: 1 [12320/60000 (21%)]\tLoss: 1.128341\n",
      "Training Epoch: 1 [12352/60000 (21%)]\tLoss: 1.196771\n",
      "Training Epoch: 1 [12384/60000 (21%)]\tLoss: 1.256860\n",
      "Training Epoch: 1 [12416/60000 (21%)]\tLoss: 1.185797\n",
      "Training Epoch: 1 [12448/60000 (21%)]\tLoss: 0.896915\n",
      "Training Epoch: 1 [12480/60000 (21%)]\tLoss: 0.942212\n",
      "Training Epoch: 1 [12512/60000 (21%)]\tLoss: 0.714024\n",
      "Training Epoch: 1 [12544/60000 (21%)]\tLoss: 1.293609\n",
      "Training Epoch: 1 [12576/60000 (21%)]\tLoss: 0.856671\n",
      "Training Epoch: 1 [12608/60000 (21%)]\tLoss: 1.092743\n",
      "Training Epoch: 1 [12640/60000 (21%)]\tLoss: 0.859082\n",
      "Training Epoch: 1 [12672/60000 (21%)]\tLoss: 1.204101\n",
      "Training Epoch: 1 [12704/60000 (21%)]\tLoss: 1.099920\n",
      "Training Epoch: 1 [12736/60000 (21%)]\tLoss: 0.893489\n",
      "Training Epoch: 1 [12768/60000 (21%)]\tLoss: 1.202893\n",
      "Training Epoch: 1 [12800/60000 (21%)]\tLoss: 0.919881\n",
      "Training Epoch: 1 [12832/60000 (21%)]\tLoss: 1.074961\n",
      "Training Epoch: 1 [12864/60000 (21%)]\tLoss: 1.460371\n",
      "Training Epoch: 1 [12896/60000 (21%)]\tLoss: 0.776602\n",
      "Training Epoch: 1 [12928/60000 (22%)]\tLoss: 1.061635\n",
      "Training Epoch: 1 [12960/60000 (22%)]\tLoss: 1.102740\n",
      "Training Epoch: 1 [12992/60000 (22%)]\tLoss: 1.186205\n",
      "Training Epoch: 1 [13024/60000 (22%)]\tLoss: 1.011932\n",
      "Training Epoch: 1 [13056/60000 (22%)]\tLoss: 1.004676\n",
      "Training Epoch: 1 [13088/60000 (22%)]\tLoss: 0.891094\n",
      "Training Epoch: 1 [13120/60000 (22%)]\tLoss: 0.495798\n",
      "Training Epoch: 1 [13152/60000 (22%)]\tLoss: 0.604672\n",
      "Training Epoch: 1 [13184/60000 (22%)]\tLoss: 0.779088\n",
      "Training Epoch: 1 [13216/60000 (22%)]\tLoss: 1.099704\n",
      "Training Epoch: 1 [13248/60000 (22%)]\tLoss: 1.027919\n",
      "Training Epoch: 1 [13280/60000 (22%)]\tLoss: 0.840198\n",
      "Training Epoch: 1 [13312/60000 (22%)]\tLoss: 0.862099\n",
      "Training Epoch: 1 [13344/60000 (22%)]\tLoss: 0.949385\n",
      "Training Epoch: 1 [13376/60000 (22%)]\tLoss: 0.968988\n",
      "Training Epoch: 1 [13408/60000 (22%)]\tLoss: 0.849450\n",
      "Training Epoch: 1 [13440/60000 (22%)]\tLoss: 0.577858\n",
      "Training Epoch: 1 [13472/60000 (22%)]\tLoss: 0.931749\n",
      "Training Epoch: 1 [13504/60000 (23%)]\tLoss: 0.817440\n",
      "Training Epoch: 1 [13536/60000 (23%)]\tLoss: 0.933139\n",
      "Training Epoch: 1 [13568/60000 (23%)]\tLoss: 0.900396\n",
      "Training Epoch: 1 [13600/60000 (23%)]\tLoss: 0.581988\n",
      "Training Epoch: 1 [13632/60000 (23%)]\tLoss: 0.722312\n",
      "Training Epoch: 1 [13664/60000 (23%)]\tLoss: 0.787276\n",
      "Training Epoch: 1 [13696/60000 (23%)]\tLoss: 0.578805\n",
      "Training Epoch: 1 [13728/60000 (23%)]\tLoss: 1.014076\n",
      "Training Epoch: 1 [13760/60000 (23%)]\tLoss: 0.907872\n",
      "Training Epoch: 1 [13792/60000 (23%)]\tLoss: 0.884017\n",
      "Training Epoch: 1 [13824/60000 (23%)]\tLoss: 0.994949\n",
      "Training Epoch: 1 [13856/60000 (23%)]\tLoss: 0.758974\n",
      "Training Epoch: 1 [13888/60000 (23%)]\tLoss: 1.184309\n",
      "Training Epoch: 1 [13920/60000 (23%)]\tLoss: 0.635591\n",
      "Training Epoch: 1 [13952/60000 (23%)]\tLoss: 1.261889\n",
      "Training Epoch: 1 [13984/60000 (23%)]\tLoss: 0.630748\n",
      "Training Epoch: 1 [14016/60000 (23%)]\tLoss: 1.082275\n",
      "Training Epoch: 1 [14048/60000 (23%)]\tLoss: 1.609334\n",
      "Training Epoch: 1 [14080/60000 (23%)]\tLoss: 1.080577\n",
      "Training Epoch: 1 [14112/60000 (24%)]\tLoss: 0.681907\n",
      "Training Epoch: 1 [14144/60000 (24%)]\tLoss: 0.605942\n",
      "Training Epoch: 1 [14176/60000 (24%)]\tLoss: 1.078432\n",
      "Training Epoch: 1 [14208/60000 (24%)]\tLoss: 0.541326\n",
      "Training Epoch: 1 [14240/60000 (24%)]\tLoss: 0.639347\n",
      "Training Epoch: 1 [14272/60000 (24%)]\tLoss: 0.843376\n",
      "Training Epoch: 1 [14304/60000 (24%)]\tLoss: 0.607413\n",
      "Training Epoch: 1 [14336/60000 (24%)]\tLoss: 0.974164\n",
      "Training Epoch: 1 [14368/60000 (24%)]\tLoss: 1.166101\n",
      "Training Epoch: 1 [14400/60000 (24%)]\tLoss: 1.370961\n",
      "Training Epoch: 1 [14432/60000 (24%)]\tLoss: 0.754067\n",
      "Training Epoch: 1 [14464/60000 (24%)]\tLoss: 0.881778\n",
      "Training Epoch: 1 [14496/60000 (24%)]\tLoss: 0.596327\n",
      "Training Epoch: 1 [14528/60000 (24%)]\tLoss: 0.539677\n",
      "Training Epoch: 1 [14560/60000 (24%)]\tLoss: 0.882014\n",
      "Training Epoch: 1 [14592/60000 (24%)]\tLoss: 0.812868\n",
      "Training Epoch: 1 [14624/60000 (24%)]\tLoss: 1.070549\n",
      "Training Epoch: 1 [14656/60000 (24%)]\tLoss: 1.064997\n",
      "Training Epoch: 1 [14688/60000 (24%)]\tLoss: 1.158457\n",
      "Training Epoch: 1 [14720/60000 (25%)]\tLoss: 0.974213\n",
      "Training Epoch: 1 [14752/60000 (25%)]\tLoss: 0.794743\n",
      "Training Epoch: 1 [14784/60000 (25%)]\tLoss: 1.140775\n",
      "Training Epoch: 1 [14816/60000 (25%)]\tLoss: 0.788069\n",
      "Training Epoch: 1 [14848/60000 (25%)]\tLoss: 1.142666\n",
      "Training Epoch: 1 [14880/60000 (25%)]\tLoss: 0.378192\n",
      "Training Epoch: 1 [14912/60000 (25%)]\tLoss: 1.099688\n",
      "Training Epoch: 1 [14944/60000 (25%)]\tLoss: 0.873284\n",
      "Training Epoch: 1 [14976/60000 (25%)]\tLoss: 0.560404\n",
      "Training Epoch: 1 [15008/60000 (25%)]\tLoss: 1.018750\n",
      "Training Epoch: 1 [15040/60000 (25%)]\tLoss: 0.776417\n",
      "Training Epoch: 1 [15072/60000 (25%)]\tLoss: 0.953932\n",
      "Training Epoch: 1 [15104/60000 (25%)]\tLoss: 0.910850\n",
      "Training Epoch: 1 [15136/60000 (25%)]\tLoss: 0.999118\n",
      "Training Epoch: 1 [15168/60000 (25%)]\tLoss: 1.027760\n",
      "Training Epoch: 1 [15200/60000 (25%)]\tLoss: 0.922026\n",
      "Training Epoch: 1 [15232/60000 (25%)]\tLoss: 0.493482\n",
      "Training Epoch: 1 [15264/60000 (25%)]\tLoss: 1.005596\n",
      "Training Epoch: 1 [15296/60000 (25%)]\tLoss: 1.287403\n",
      "Training Epoch: 1 [15328/60000 (26%)]\tLoss: 0.970187\n",
      "Training Epoch: 1 [15360/60000 (26%)]\tLoss: 0.795465\n",
      "Training Epoch: 1 [15392/60000 (26%)]\tLoss: 0.827123\n",
      "Training Epoch: 1 [15424/60000 (26%)]\tLoss: 0.713613\n",
      "Training Epoch: 1 [15456/60000 (26%)]\tLoss: 0.900091\n",
      "Training Epoch: 1 [15488/60000 (26%)]\tLoss: 1.404529\n",
      "Training Epoch: 1 [15520/60000 (26%)]\tLoss: 0.606836\n",
      "Training Epoch: 1 [15552/60000 (26%)]\tLoss: 0.671068\n",
      "Training Epoch: 1 [15584/60000 (26%)]\tLoss: 0.758247\n",
      "Training Epoch: 1 [15616/60000 (26%)]\tLoss: 0.654341\n",
      "Training Epoch: 1 [15648/60000 (26%)]\tLoss: 0.766475\n",
      "Training Epoch: 1 [15680/60000 (26%)]\tLoss: 1.116401\n",
      "Training Epoch: 1 [15712/60000 (26%)]\tLoss: 0.816612\n",
      "Training Epoch: 1 [15744/60000 (26%)]\tLoss: 0.883438\n",
      "Training Epoch: 1 [15776/60000 (26%)]\tLoss: 1.278405\n",
      "Training Epoch: 1 [15808/60000 (26%)]\tLoss: 0.806822\n",
      "Training Epoch: 1 [15840/60000 (26%)]\tLoss: 1.011239\n",
      "Training Epoch: 1 [15872/60000 (26%)]\tLoss: 0.662789\n",
      "Training Epoch: 1 [15904/60000 (27%)]\tLoss: 0.590605\n",
      "Training Epoch: 1 [15936/60000 (27%)]\tLoss: 1.026375\n",
      "Training Epoch: 1 [15968/60000 (27%)]\tLoss: 0.876714\n",
      "Training Epoch: 1 [16000/60000 (27%)]\tLoss: 1.097153\n",
      "Training Epoch: 1 [16032/60000 (27%)]\tLoss: 0.714819\n",
      "Training Epoch: 1 [16064/60000 (27%)]\tLoss: 0.948981\n",
      "Training Epoch: 1 [16096/60000 (27%)]\tLoss: 0.881014\n",
      "Training Epoch: 1 [16128/60000 (27%)]\tLoss: 1.041921\n",
      "Training Epoch: 1 [16160/60000 (27%)]\tLoss: 0.711781\n",
      "Training Epoch: 1 [16192/60000 (27%)]\tLoss: 0.631080\n",
      "Training Epoch: 1 [16224/60000 (27%)]\tLoss: 0.574561\n",
      "Training Epoch: 1 [16256/60000 (27%)]\tLoss: 0.956091\n",
      "Training Epoch: 1 [16288/60000 (27%)]\tLoss: 1.222551\n",
      "Training Epoch: 1 [16320/60000 (27%)]\tLoss: 0.810009\n",
      "Training Epoch: 1 [16352/60000 (27%)]\tLoss: 0.935392\n",
      "Training Epoch: 1 [16384/60000 (27%)]\tLoss: 1.110819\n",
      "Training Epoch: 1 [16416/60000 (27%)]\tLoss: 0.966153\n",
      "Training Epoch: 1 [16448/60000 (27%)]\tLoss: 0.872675\n",
      "Training Epoch: 1 [16480/60000 (27%)]\tLoss: 0.740113\n",
      "Training Epoch: 1 [16512/60000 (28%)]\tLoss: 0.725923\n",
      "Training Epoch: 1 [16544/60000 (28%)]\tLoss: 0.762923\n",
      "Training Epoch: 1 [16576/60000 (28%)]\tLoss: 0.993008\n",
      "Training Epoch: 1 [16608/60000 (28%)]\tLoss: 0.818556\n",
      "Training Epoch: 1 [16640/60000 (28%)]\tLoss: 0.539949\n",
      "Training Epoch: 1 [16672/60000 (28%)]\tLoss: 0.828061\n",
      "Training Epoch: 1 [16704/60000 (28%)]\tLoss: 0.620246\n",
      "Training Epoch: 1 [16736/60000 (28%)]\tLoss: 0.537181\n",
      "Training Epoch: 1 [16768/60000 (28%)]\tLoss: 0.657004\n",
      "Training Epoch: 1 [16800/60000 (28%)]\tLoss: 0.741606\n",
      "Training Epoch: 1 [16832/60000 (28%)]\tLoss: 0.565200\n",
      "Training Epoch: 1 [16864/60000 (28%)]\tLoss: 0.580020\n",
      "Training Epoch: 1 [16896/60000 (28%)]\tLoss: 0.893894\n",
      "Training Epoch: 1 [16928/60000 (28%)]\tLoss: 0.588253\n",
      "Training Epoch: 1 [16960/60000 (28%)]\tLoss: 0.492393\n",
      "Training Epoch: 1 [16992/60000 (28%)]\tLoss: 0.665430\n",
      "Training Epoch: 1 [17024/60000 (28%)]\tLoss: 1.007278\n",
      "Training Epoch: 1 [17056/60000 (28%)]\tLoss: 1.109140\n",
      "Training Epoch: 1 [17088/60000 (28%)]\tLoss: 0.966552\n",
      "Training Epoch: 1 [17120/60000 (29%)]\tLoss: 0.790634\n",
      "Training Epoch: 1 [17152/60000 (29%)]\tLoss: 0.865507\n",
      "Training Epoch: 1 [17184/60000 (29%)]\tLoss: 0.949481\n",
      "Training Epoch: 1 [17216/60000 (29%)]\tLoss: 0.897239\n",
      "Training Epoch: 1 [17248/60000 (29%)]\tLoss: 0.742817\n",
      "Training Epoch: 1 [17280/60000 (29%)]\tLoss: 0.895286\n",
      "Training Epoch: 1 [17312/60000 (29%)]\tLoss: 0.723281\n",
      "Training Epoch: 1 [17344/60000 (29%)]\tLoss: 0.770658\n",
      "Training Epoch: 1 [17376/60000 (29%)]\tLoss: 0.750994\n",
      "Training Epoch: 1 [17408/60000 (29%)]\tLoss: 1.023206\n",
      "Training Epoch: 1 [17440/60000 (29%)]\tLoss: 0.937238\n",
      "Training Epoch: 1 [17472/60000 (29%)]\tLoss: 0.837607\n",
      "Training Epoch: 1 [17504/60000 (29%)]\tLoss: 1.203422\n",
      "Training Epoch: 1 [17536/60000 (29%)]\tLoss: 0.524507\n",
      "Training Epoch: 1 [17568/60000 (29%)]\tLoss: 0.767952\n",
      "Training Epoch: 1 [17600/60000 (29%)]\tLoss: 0.361182\n",
      "Training Epoch: 1 [17632/60000 (29%)]\tLoss: 1.161708\n",
      "Training Epoch: 1 [17664/60000 (29%)]\tLoss: 0.786647\n",
      "Training Epoch: 1 [17696/60000 (29%)]\tLoss: 0.805592\n",
      "Training Epoch: 1 [17728/60000 (30%)]\tLoss: 0.204856\n",
      "Training Epoch: 1 [17760/60000 (30%)]\tLoss: 0.858743\n",
      "Training Epoch: 1 [17792/60000 (30%)]\tLoss: 0.616111\n",
      "Training Epoch: 1 [17824/60000 (30%)]\tLoss: 0.498745\n",
      "Training Epoch: 1 [17856/60000 (30%)]\tLoss: 0.581576\n",
      "Training Epoch: 1 [17888/60000 (30%)]\tLoss: 0.622915\n",
      "Training Epoch: 1 [17920/60000 (30%)]\tLoss: 1.056748\n",
      "Training Epoch: 1 [17952/60000 (30%)]\tLoss: 0.619337\n",
      "Training Epoch: 1 [17984/60000 (30%)]\tLoss: 0.587747\n",
      "Training Epoch: 1 [18016/60000 (30%)]\tLoss: 0.719216\n",
      "Training Epoch: 1 [18048/60000 (30%)]\tLoss: 0.447156\n",
      "Training Epoch: 1 [18080/60000 (30%)]\tLoss: 0.673415\n",
      "Training Epoch: 1 [18112/60000 (30%)]\tLoss: 0.559290\n",
      "Training Epoch: 1 [18144/60000 (30%)]\tLoss: 0.441572\n",
      "Training Epoch: 1 [18176/60000 (30%)]\tLoss: 1.059367\n",
      "Training Epoch: 1 [18208/60000 (30%)]\tLoss: 1.155928\n",
      "Training Epoch: 1 [18240/60000 (30%)]\tLoss: 1.092198\n",
      "Training Epoch: 1 [18272/60000 (30%)]\tLoss: 0.843311\n",
      "Training Epoch: 1 [18304/60000 (31%)]\tLoss: 0.877831\n",
      "Training Epoch: 1 [18336/60000 (31%)]\tLoss: 0.861556\n",
      "Training Epoch: 1 [18368/60000 (31%)]\tLoss: 1.136032\n",
      "Training Epoch: 1 [18400/60000 (31%)]\tLoss: 1.053359\n",
      "Training Epoch: 1 [18432/60000 (31%)]\tLoss: 1.017900\n",
      "Training Epoch: 1 [18464/60000 (31%)]\tLoss: 0.832165\n",
      "Training Epoch: 1 [18496/60000 (31%)]\tLoss: 1.104421\n",
      "Training Epoch: 1 [18528/60000 (31%)]\tLoss: 0.533093\n",
      "Training Epoch: 1 [18560/60000 (31%)]\tLoss: 0.797652\n",
      "Training Epoch: 1 [18592/60000 (31%)]\tLoss: 0.772114\n",
      "Training Epoch: 1 [18624/60000 (31%)]\tLoss: 1.307679\n",
      "Training Epoch: 1 [18656/60000 (31%)]\tLoss: 0.896645\n",
      "Training Epoch: 1 [18688/60000 (31%)]\tLoss: 0.710250\n",
      "Training Epoch: 1 [18720/60000 (31%)]\tLoss: 0.742701\n",
      "Training Epoch: 1 [18752/60000 (31%)]\tLoss: 0.585705\n",
      "Training Epoch: 1 [18784/60000 (31%)]\tLoss: 0.576097\n",
      "Training Epoch: 1 [18816/60000 (31%)]\tLoss: 0.603473\n",
      "Training Epoch: 1 [18848/60000 (31%)]\tLoss: 0.584224\n",
      "Training Epoch: 1 [18880/60000 (31%)]\tLoss: 0.804959\n",
      "Training Epoch: 1 [18912/60000 (32%)]\tLoss: 0.525014\n",
      "Training Epoch: 1 [18944/60000 (32%)]\tLoss: 1.012922\n",
      "Training Epoch: 1 [18976/60000 (32%)]\tLoss: 1.068383\n",
      "Training Epoch: 1 [19008/60000 (32%)]\tLoss: 0.647989\n",
      "Training Epoch: 1 [19040/60000 (32%)]\tLoss: 0.826093\n",
      "Training Epoch: 1 [19072/60000 (32%)]\tLoss: 0.449359\n",
      "Training Epoch: 1 [19104/60000 (32%)]\tLoss: 0.490758\n",
      "Training Epoch: 1 [19136/60000 (32%)]\tLoss: 0.423309\n",
      "Training Epoch: 1 [19168/60000 (32%)]\tLoss: 0.426051\n",
      "Training Epoch: 1 [19200/60000 (32%)]\tLoss: 0.466465\n",
      "Training Epoch: 1 [19232/60000 (32%)]\tLoss: 0.600447\n",
      "Training Epoch: 1 [19264/60000 (32%)]\tLoss: 0.571419\n",
      "Training Epoch: 1 [19296/60000 (32%)]\tLoss: 0.893187\n",
      "Training Epoch: 1 [19328/60000 (32%)]\tLoss: 0.748787\n",
      "Training Epoch: 1 [19360/60000 (32%)]\tLoss: 1.051886\n",
      "Training Epoch: 1 [19392/60000 (32%)]\tLoss: 1.089209\n",
      "Training Epoch: 1 [19424/60000 (32%)]\tLoss: 0.870424\n",
      "Training Epoch: 1 [19456/60000 (32%)]\tLoss: 0.497622\n",
      "Training Epoch: 1 [19488/60000 (32%)]\tLoss: 0.349123\n",
      "Training Epoch: 1 [19520/60000 (33%)]\tLoss: 0.882685\n",
      "Training Epoch: 1 [19552/60000 (33%)]\tLoss: 0.634479\n",
      "Training Epoch: 1 [19584/60000 (33%)]\tLoss: 0.871014\n",
      "Training Epoch: 1 [19616/60000 (33%)]\tLoss: 0.943034\n",
      "Training Epoch: 1 [19648/60000 (33%)]\tLoss: 1.013985\n",
      "Training Epoch: 1 [19680/60000 (33%)]\tLoss: 0.944658\n",
      "Training Epoch: 1 [19712/60000 (33%)]\tLoss: 0.596052\n",
      "Training Epoch: 1 [19744/60000 (33%)]\tLoss: 0.789492\n",
      "Training Epoch: 1 [19776/60000 (33%)]\tLoss: 0.635120\n",
      "Training Epoch: 1 [19808/60000 (33%)]\tLoss: 0.558572\n",
      "Training Epoch: 1 [19840/60000 (33%)]\tLoss: 1.115393\n",
      "Training Epoch: 1 [19872/60000 (33%)]\tLoss: 0.839507\n",
      "Training Epoch: 1 [19904/60000 (33%)]\tLoss: 0.493532\n",
      "Training Epoch: 1 [19936/60000 (33%)]\tLoss: 0.825191\n",
      "Training Epoch: 1 [19968/60000 (33%)]\tLoss: 0.545420\n",
      "Training Epoch: 1 [20000/60000 (33%)]\tLoss: 0.968098\n",
      "Training Epoch: 1 [20032/60000 (33%)]\tLoss: 0.750048\n",
      "Training Epoch: 1 [20064/60000 (33%)]\tLoss: 0.884739\n",
      "Training Epoch: 1 [20096/60000 (33%)]\tLoss: 0.586653\n",
      "Training Epoch: 1 [20128/60000 (34%)]\tLoss: 0.733604\n",
      "Training Epoch: 1 [20160/60000 (34%)]\tLoss: 1.133093\n",
      "Training Epoch: 1 [20192/60000 (34%)]\tLoss: 0.666592\n",
      "Training Epoch: 1 [20224/60000 (34%)]\tLoss: 0.637852\n",
      "Training Epoch: 1 [20256/60000 (34%)]\tLoss: 0.470000\n",
      "Training Epoch: 1 [20288/60000 (34%)]\tLoss: 0.995399\n",
      "Training Epoch: 1 [20320/60000 (34%)]\tLoss: 0.964483\n",
      "Training Epoch: 1 [20352/60000 (34%)]\tLoss: 0.728716\n",
      "Training Epoch: 1 [20384/60000 (34%)]\tLoss: 0.819106\n",
      "Training Epoch: 1 [20416/60000 (34%)]\tLoss: 0.575940\n",
      "Training Epoch: 1 [20448/60000 (34%)]\tLoss: 1.069796\n",
      "Training Epoch: 1 [20480/60000 (34%)]\tLoss: 0.872768\n",
      "Training Epoch: 1 [20512/60000 (34%)]\tLoss: 0.806836\n",
      "Training Epoch: 1 [20544/60000 (34%)]\tLoss: 1.045727\n",
      "Training Epoch: 1 [20576/60000 (34%)]\tLoss: 1.351796\n",
      "Training Epoch: 1 [20608/60000 (34%)]\tLoss: 0.662254\n",
      "Training Epoch: 1 [20640/60000 (34%)]\tLoss: 0.747723\n",
      "Training Epoch: 1 [20672/60000 (34%)]\tLoss: 0.623926\n",
      "Training Epoch: 1 [20704/60000 (35%)]\tLoss: 0.549766\n",
      "Training Epoch: 1 [20736/60000 (35%)]\tLoss: 0.634302\n",
      "Training Epoch: 1 [20768/60000 (35%)]\tLoss: 1.195191\n",
      "Training Epoch: 1 [20800/60000 (35%)]\tLoss: 0.943870\n",
      "Training Epoch: 1 [20832/60000 (35%)]\tLoss: 0.584352\n",
      "Training Epoch: 1 [20864/60000 (35%)]\tLoss: 0.757466\n",
      "Training Epoch: 1 [20896/60000 (35%)]\tLoss: 1.245566\n",
      "Training Epoch: 1 [20928/60000 (35%)]\tLoss: 0.509079\n",
      "Training Epoch: 1 [20960/60000 (35%)]\tLoss: 0.851039\n",
      "Training Epoch: 1 [20992/60000 (35%)]\tLoss: 0.332817\n",
      "Training Epoch: 1 [21024/60000 (35%)]\tLoss: 0.772931\n",
      "Training Epoch: 1 [21056/60000 (35%)]\tLoss: 0.823837\n",
      "Training Epoch: 1 [21088/60000 (35%)]\tLoss: 0.791903\n",
      "Training Epoch: 1 [21120/60000 (35%)]\tLoss: 0.636168\n",
      "Training Epoch: 1 [21152/60000 (35%)]\tLoss: 0.656733\n",
      "Training Epoch: 1 [21184/60000 (35%)]\tLoss: 0.662799\n",
      "Training Epoch: 1 [21216/60000 (35%)]\tLoss: 1.016530\n",
      "Training Epoch: 1 [21248/60000 (35%)]\tLoss: 0.957440\n",
      "Training Epoch: 1 [21280/60000 (35%)]\tLoss: 0.503259\n",
      "Training Epoch: 1 [21312/60000 (36%)]\tLoss: 0.766207\n",
      "Training Epoch: 1 [21344/60000 (36%)]\tLoss: 0.489911\n",
      "Training Epoch: 1 [21376/60000 (36%)]\tLoss: 0.642875\n",
      "Training Epoch: 1 [21408/60000 (36%)]\tLoss: 0.570681\n",
      "Training Epoch: 1 [21440/60000 (36%)]\tLoss: 0.713253\n",
      "Training Epoch: 1 [21472/60000 (36%)]\tLoss: 1.082942\n",
      "Training Epoch: 1 [21504/60000 (36%)]\tLoss: 0.723633\n",
      "Training Epoch: 1 [21536/60000 (36%)]\tLoss: 0.593417\n",
      "Training Epoch: 1 [21568/60000 (36%)]\tLoss: 0.803529\n",
      "Training Epoch: 1 [21600/60000 (36%)]\tLoss: 1.085825\n",
      "Training Epoch: 1 [21632/60000 (36%)]\tLoss: 0.549470\n",
      "Training Epoch: 1 [21664/60000 (36%)]\tLoss: 0.566248\n",
      "Training Epoch: 1 [21696/60000 (36%)]\tLoss: 0.687208\n",
      "Training Epoch: 1 [21728/60000 (36%)]\tLoss: 0.686282\n",
      "Training Epoch: 1 [21760/60000 (36%)]\tLoss: 0.767055\n",
      "Training Epoch: 1 [21792/60000 (36%)]\tLoss: 0.483134\n",
      "Training Epoch: 1 [21824/60000 (36%)]\tLoss: 0.872274\n",
      "Training Epoch: 1 [21856/60000 (36%)]\tLoss: 0.532244\n",
      "Training Epoch: 1 [21888/60000 (36%)]\tLoss: 0.639274\n",
      "Training Epoch: 1 [21920/60000 (37%)]\tLoss: 0.737182\n",
      "Training Epoch: 1 [21952/60000 (37%)]\tLoss: 0.676970\n",
      "Training Epoch: 1 [21984/60000 (37%)]\tLoss: 0.681038\n",
      "Training Epoch: 1 [22016/60000 (37%)]\tLoss: 0.888013\n",
      "Training Epoch: 1 [22048/60000 (37%)]\tLoss: 1.044975\n",
      "Training Epoch: 1 [22080/60000 (37%)]\tLoss: 0.821874\n",
      "Training Epoch: 1 [22112/60000 (37%)]\tLoss: 0.524254\n",
      "Training Epoch: 1 [22144/60000 (37%)]\tLoss: 0.915211\n",
      "Training Epoch: 1 [22176/60000 (37%)]\tLoss: 0.655211\n",
      "Training Epoch: 1 [22208/60000 (37%)]\tLoss: 0.922964\n",
      "Training Epoch: 1 [22240/60000 (37%)]\tLoss: 0.441397\n",
      "Training Epoch: 1 [22272/60000 (37%)]\tLoss: 0.816551\n",
      "Training Epoch: 1 [22304/60000 (37%)]\tLoss: 0.917767\n",
      "Training Epoch: 1 [22336/60000 (37%)]\tLoss: 0.825882\n",
      "Training Epoch: 1 [22368/60000 (37%)]\tLoss: 0.783846\n",
      "Training Epoch: 1 [22400/60000 (37%)]\tLoss: 0.555430\n",
      "Training Epoch: 1 [22432/60000 (37%)]\tLoss: 0.726651\n",
      "Training Epoch: 1 [22464/60000 (37%)]\tLoss: 1.286690\n",
      "Training Epoch: 1 [22496/60000 (37%)]\tLoss: 0.801820\n",
      "Training Epoch: 1 [22528/60000 (38%)]\tLoss: 0.568293\n",
      "Training Epoch: 1 [22560/60000 (38%)]\tLoss: 0.759345\n",
      "Training Epoch: 1 [22592/60000 (38%)]\tLoss: 0.765421\n",
      "Training Epoch: 1 [22624/60000 (38%)]\tLoss: 0.533954\n",
      "Training Epoch: 1 [22656/60000 (38%)]\tLoss: 0.484933\n",
      "Training Epoch: 1 [22688/60000 (38%)]\tLoss: 0.669136\n",
      "Training Epoch: 1 [22720/60000 (38%)]\tLoss: 0.459479\n",
      "Training Epoch: 1 [22752/60000 (38%)]\tLoss: 0.667857\n",
      "Training Epoch: 1 [22784/60000 (38%)]\tLoss: 0.672660\n",
      "Training Epoch: 1 [22816/60000 (38%)]\tLoss: 0.697784\n",
      "Training Epoch: 1 [22848/60000 (38%)]\tLoss: 0.632411\n",
      "Training Epoch: 1 [22880/60000 (38%)]\tLoss: 0.562003\n",
      "Training Epoch: 1 [22912/60000 (38%)]\tLoss: 0.772770\n",
      "Training Epoch: 1 [22944/60000 (38%)]\tLoss: 0.529375\n",
      "Training Epoch: 1 [22976/60000 (38%)]\tLoss: 0.981818\n",
      "Training Epoch: 1 [23008/60000 (38%)]\tLoss: 0.903181\n",
      "Training Epoch: 1 [23040/60000 (38%)]\tLoss: 0.787266\n",
      "Training Epoch: 1 [23072/60000 (38%)]\tLoss: 0.822125\n",
      "Training Epoch: 1 [23104/60000 (39%)]\tLoss: 0.334189\n",
      "Training Epoch: 1 [23136/60000 (39%)]\tLoss: 0.501663\n",
      "Training Epoch: 1 [23168/60000 (39%)]\tLoss: 0.548479\n",
      "Training Epoch: 1 [23200/60000 (39%)]\tLoss: 0.720828\n",
      "Training Epoch: 1 [23232/60000 (39%)]\tLoss: 0.648104\n",
      "Training Epoch: 1 [23264/60000 (39%)]\tLoss: 0.751286\n",
      "Training Epoch: 1 [23296/60000 (39%)]\tLoss: 0.742468\n",
      "Training Epoch: 1 [23328/60000 (39%)]\tLoss: 0.292840\n",
      "Training Epoch: 1 [23360/60000 (39%)]\tLoss: 0.694005\n",
      "Training Epoch: 1 [23392/60000 (39%)]\tLoss: 1.078706\n",
      "Training Epoch: 1 [23424/60000 (39%)]\tLoss: 0.435281\n",
      "Training Epoch: 1 [23456/60000 (39%)]\tLoss: 0.752381\n",
      "Training Epoch: 1 [23488/60000 (39%)]\tLoss: 0.354001\n",
      "Training Epoch: 1 [23520/60000 (39%)]\tLoss: 0.518347\n",
      "Training Epoch: 1 [23552/60000 (39%)]\tLoss: 1.507620\n",
      "Training Epoch: 1 [23584/60000 (39%)]\tLoss: 0.615430\n",
      "Training Epoch: 1 [23616/60000 (39%)]\tLoss: 0.492584\n",
      "Training Epoch: 1 [23648/60000 (39%)]\tLoss: 1.050676\n",
      "Training Epoch: 1 [23680/60000 (39%)]\tLoss: 0.677182\n",
      "Training Epoch: 1 [23712/60000 (40%)]\tLoss: 0.538286\n",
      "Training Epoch: 1 [23744/60000 (40%)]\tLoss: 0.939268\n",
      "Training Epoch: 1 [23776/60000 (40%)]\tLoss: 0.565675\n",
      "Training Epoch: 1 [23808/60000 (40%)]\tLoss: 0.956481\n",
      "Training Epoch: 1 [23840/60000 (40%)]\tLoss: 0.683621\n",
      "Training Epoch: 1 [23872/60000 (40%)]\tLoss: 0.720634\n",
      "Training Epoch: 1 [23904/60000 (40%)]\tLoss: 0.819713\n",
      "Training Epoch: 1 [23936/60000 (40%)]\tLoss: 0.490907\n",
      "Training Epoch: 1 [23968/60000 (40%)]\tLoss: 0.776685\n",
      "Training Epoch: 1 [24000/60000 (40%)]\tLoss: 0.995135\n",
      "Training Epoch: 1 [24032/60000 (40%)]\tLoss: 0.317253\n",
      "Training Epoch: 1 [24064/60000 (40%)]\tLoss: 0.468477\n",
      "Training Epoch: 1 [24096/60000 (40%)]\tLoss: 0.759126\n",
      "Training Epoch: 1 [24128/60000 (40%)]\tLoss: 0.753862\n",
      "Training Epoch: 1 [24160/60000 (40%)]\tLoss: 0.385597\n",
      "Training Epoch: 1 [24192/60000 (40%)]\tLoss: 1.104998\n",
      "Training Epoch: 1 [24224/60000 (40%)]\tLoss: 0.742796\n",
      "Training Epoch: 1 [24256/60000 (40%)]\tLoss: 0.717107\n",
      "Training Epoch: 1 [24288/60000 (40%)]\tLoss: 1.022534\n",
      "Training Epoch: 1 [24320/60000 (41%)]\tLoss: 0.933778\n",
      "Training Epoch: 1 [24352/60000 (41%)]\tLoss: 0.772500\n",
      "Training Epoch: 1 [24384/60000 (41%)]\tLoss: 0.828630\n",
      "Training Epoch: 1 [24416/60000 (41%)]\tLoss: 0.658226\n",
      "Training Epoch: 1 [24448/60000 (41%)]\tLoss: 1.223643\n",
      "Training Epoch: 1 [24480/60000 (41%)]\tLoss: 0.949807\n",
      "Training Epoch: 1 [24512/60000 (41%)]\tLoss: 0.560892\n",
      "Training Epoch: 1 [24544/60000 (41%)]\tLoss: 0.511716\n",
      "Training Epoch: 1 [24576/60000 (41%)]\tLoss: 0.542137\n",
      "Training Epoch: 1 [24608/60000 (41%)]\tLoss: 1.104414\n",
      "Training Epoch: 1 [24640/60000 (41%)]\tLoss: 0.703472\n",
      "Training Epoch: 1 [24672/60000 (41%)]\tLoss: 0.725267\n",
      "Training Epoch: 1 [24704/60000 (41%)]\tLoss: 0.511544\n",
      "Training Epoch: 1 [24736/60000 (41%)]\tLoss: 0.685332\n",
      "Training Epoch: 1 [24768/60000 (41%)]\tLoss: 0.589805\n",
      "Training Epoch: 1 [24800/60000 (41%)]\tLoss: 0.414727\n",
      "Training Epoch: 1 [24832/60000 (41%)]\tLoss: 0.676452\n",
      "Training Epoch: 1 [24864/60000 (41%)]\tLoss: 0.718663\n",
      "Training Epoch: 1 [24896/60000 (41%)]\tLoss: 0.728884\n",
      "Training Epoch: 1 [24928/60000 (42%)]\tLoss: 0.851494\n",
      "Training Epoch: 1 [24960/60000 (42%)]\tLoss: 0.682142\n",
      "Training Epoch: 1 [24992/60000 (42%)]\tLoss: 0.541453\n",
      "Training Epoch: 1 [25024/60000 (42%)]\tLoss: 0.780627\n",
      "Training Epoch: 1 [25056/60000 (42%)]\tLoss: 0.790867\n",
      "Training Epoch: 1 [25088/60000 (42%)]\tLoss: 0.696884\n",
      "Training Epoch: 1 [25120/60000 (42%)]\tLoss: 1.209681\n",
      "Training Epoch: 1 [25152/60000 (42%)]\tLoss: 0.792730\n",
      "Training Epoch: 1 [25184/60000 (42%)]\tLoss: 1.104198\n",
      "Training Epoch: 1 [25216/60000 (42%)]\tLoss: 0.579563\n",
      "Training Epoch: 1 [25248/60000 (42%)]\tLoss: 0.679925\n",
      "Training Epoch: 1 [25280/60000 (42%)]\tLoss: 0.426287\n",
      "Training Epoch: 1 [25312/60000 (42%)]\tLoss: 0.806638\n",
      "Training Epoch: 1 [25344/60000 (42%)]\tLoss: 0.503497\n",
      "Training Epoch: 1 [25376/60000 (42%)]\tLoss: 0.886474\n",
      "Training Epoch: 1 [25408/60000 (42%)]\tLoss: 0.596052\n",
      "Training Epoch: 1 [25440/60000 (42%)]\tLoss: 0.634935\n",
      "Training Epoch: 1 [25472/60000 (42%)]\tLoss: 0.757585\n",
      "Training Epoch: 1 [25504/60000 (43%)]\tLoss: 0.336548\n",
      "Training Epoch: 1 [25536/60000 (43%)]\tLoss: 0.487785\n",
      "Training Epoch: 1 [25568/60000 (43%)]\tLoss: 0.810145\n",
      "Training Epoch: 1 [25600/60000 (43%)]\tLoss: 0.655668\n",
      "Training Epoch: 1 [25632/60000 (43%)]\tLoss: 0.655357\n",
      "Training Epoch: 1 [25664/60000 (43%)]\tLoss: 0.324902\n",
      "Training Epoch: 1 [25696/60000 (43%)]\tLoss: 0.883553\n",
      "Training Epoch: 1 [25728/60000 (43%)]\tLoss: 1.047205\n",
      "Training Epoch: 1 [25760/60000 (43%)]\tLoss: 0.671772\n",
      "Training Epoch: 1 [25792/60000 (43%)]\tLoss: 0.830494\n",
      "Training Epoch: 1 [25824/60000 (43%)]\tLoss: 1.166632\n",
      "Training Epoch: 1 [25856/60000 (43%)]\tLoss: 0.661811\n",
      "Training Epoch: 1 [25888/60000 (43%)]\tLoss: 0.499041\n",
      "Training Epoch: 1 [25920/60000 (43%)]\tLoss: 0.870978\n",
      "Training Epoch: 1 [25952/60000 (43%)]\tLoss: 1.020818\n",
      "Training Epoch: 1 [25984/60000 (43%)]\tLoss: 0.543649\n",
      "Training Epoch: 1 [26016/60000 (43%)]\tLoss: 0.467937\n",
      "Training Epoch: 1 [26048/60000 (43%)]\tLoss: 0.411944\n",
      "Training Epoch: 1 [26080/60000 (43%)]\tLoss: 1.410355\n",
      "Training Epoch: 1 [26112/60000 (44%)]\tLoss: 0.568709\n",
      "Training Epoch: 1 [26144/60000 (44%)]\tLoss: 1.097009\n",
      "Training Epoch: 1 [26176/60000 (44%)]\tLoss: 0.533186\n",
      "Training Epoch: 1 [26208/60000 (44%)]\tLoss: 0.645290\n",
      "Training Epoch: 1 [26240/60000 (44%)]\tLoss: 0.524920\n",
      "Training Epoch: 1 [26272/60000 (44%)]\tLoss: 0.670013\n",
      "Training Epoch: 1 [26304/60000 (44%)]\tLoss: 0.815310\n",
      "Training Epoch: 1 [26336/60000 (44%)]\tLoss: 0.514859\n",
      "Training Epoch: 1 [26368/60000 (44%)]\tLoss: 0.992965\n",
      "Training Epoch: 1 [26400/60000 (44%)]\tLoss: 0.735884\n",
      "Training Epoch: 1 [26432/60000 (44%)]\tLoss: 0.783873\n",
      "Training Epoch: 1 [26464/60000 (44%)]\tLoss: 1.002663\n",
      "Training Epoch: 1 [26496/60000 (44%)]\tLoss: 0.477265\n",
      "Training Epoch: 1 [26528/60000 (44%)]\tLoss: 0.504806\n",
      "Training Epoch: 1 [26560/60000 (44%)]\tLoss: 0.630316\n",
      "Training Epoch: 1 [26592/60000 (44%)]\tLoss: 0.659732\n",
      "Training Epoch: 1 [26624/60000 (44%)]\tLoss: 0.349566\n",
      "Training Epoch: 1 [26656/60000 (44%)]\tLoss: 0.586655\n",
      "Training Epoch: 1 [26688/60000 (44%)]\tLoss: 0.740018\n",
      "Training Epoch: 1 [26720/60000 (45%)]\tLoss: 0.671176\n",
      "Training Epoch: 1 [26752/60000 (45%)]\tLoss: 0.626506\n",
      "Training Epoch: 1 [26784/60000 (45%)]\tLoss: 0.807586\n",
      "Training Epoch: 1 [26816/60000 (45%)]\tLoss: 0.445464\n",
      "Training Epoch: 1 [26848/60000 (45%)]\tLoss: 0.443939\n",
      "Training Epoch: 1 [26880/60000 (45%)]\tLoss: 0.534836\n",
      "Training Epoch: 1 [26912/60000 (45%)]\tLoss: 1.055548\n",
      "Training Epoch: 1 [26944/60000 (45%)]\tLoss: 0.571173\n",
      "Training Epoch: 1 [26976/60000 (45%)]\tLoss: 0.647653\n",
      "Training Epoch: 1 [27008/60000 (45%)]\tLoss: 0.496070\n",
      "Training Epoch: 1 [27040/60000 (45%)]\tLoss: 0.538421\n",
      "Training Epoch: 1 [27072/60000 (45%)]\tLoss: 0.714745\n",
      "Training Epoch: 1 [27104/60000 (45%)]\tLoss: 0.685807\n",
      "Training Epoch: 1 [27136/60000 (45%)]\tLoss: 0.570257\n",
      "Training Epoch: 1 [27168/60000 (45%)]\tLoss: 0.665175\n",
      "Training Epoch: 1 [27200/60000 (45%)]\tLoss: 0.592581\n",
      "Training Epoch: 1 [27232/60000 (45%)]\tLoss: 0.778185\n",
      "Training Epoch: 1 [27264/60000 (45%)]\tLoss: 0.599901\n",
      "Training Epoch: 1 [27296/60000 (45%)]\tLoss: 0.395126\n",
      "Training Epoch: 1 [27328/60000 (46%)]\tLoss: 0.733641\n",
      "Training Epoch: 1 [27360/60000 (46%)]\tLoss: 0.628554\n",
      "Training Epoch: 1 [27392/60000 (46%)]\tLoss: 0.794219\n",
      "Training Epoch: 1 [27424/60000 (46%)]\tLoss: 0.823458\n",
      "Training Epoch: 1 [27456/60000 (46%)]\tLoss: 0.504697\n",
      "Training Epoch: 1 [27488/60000 (46%)]\tLoss: 0.914735\n",
      "Training Epoch: 1 [27520/60000 (46%)]\tLoss: 0.811104\n",
      "Training Epoch: 1 [27552/60000 (46%)]\tLoss: 0.489604\n",
      "Training Epoch: 1 [27584/60000 (46%)]\tLoss: 0.915859\n",
      "Training Epoch: 1 [27616/60000 (46%)]\tLoss: 0.782807\n",
      "Training Epoch: 1 [27648/60000 (46%)]\tLoss: 0.585608\n",
      "Training Epoch: 1 [27680/60000 (46%)]\tLoss: 0.343784\n",
      "Training Epoch: 1 [27712/60000 (46%)]\tLoss: 0.437599\n",
      "Training Epoch: 1 [27744/60000 (46%)]\tLoss: 0.631902\n",
      "Training Epoch: 1 [27776/60000 (46%)]\tLoss: 0.639776\n",
      "Training Epoch: 1 [27808/60000 (46%)]\tLoss: 0.438504\n",
      "Training Epoch: 1 [27840/60000 (46%)]\tLoss: 0.762825\n",
      "Training Epoch: 1 [27872/60000 (46%)]\tLoss: 0.795513\n",
      "Training Epoch: 1 [27904/60000 (47%)]\tLoss: 0.774655\n",
      "Training Epoch: 1 [27936/60000 (47%)]\tLoss: 0.536401\n",
      "Training Epoch: 1 [27968/60000 (47%)]\tLoss: 0.670812\n",
      "Training Epoch: 1 [28000/60000 (47%)]\tLoss: 0.589899\n",
      "Training Epoch: 1 [28032/60000 (47%)]\tLoss: 0.631114\n",
      "Training Epoch: 1 [28064/60000 (47%)]\tLoss: 0.733105\n",
      "Training Epoch: 1 [28096/60000 (47%)]\tLoss: 0.516434\n",
      "Training Epoch: 1 [28128/60000 (47%)]\tLoss: 0.661421\n",
      "Training Epoch: 1 [28160/60000 (47%)]\tLoss: 0.547698\n",
      "Training Epoch: 1 [28192/60000 (47%)]\tLoss: 0.613785\n",
      "Training Epoch: 1 [28224/60000 (47%)]\tLoss: 0.590812\n",
      "Training Epoch: 1 [28256/60000 (47%)]\tLoss: 0.743390\n",
      "Training Epoch: 1 [28288/60000 (47%)]\tLoss: 0.456458\n",
      "Training Epoch: 1 [28320/60000 (47%)]\tLoss: 0.417758\n",
      "Training Epoch: 1 [28352/60000 (47%)]\tLoss: 0.591111\n",
      "Training Epoch: 1 [28384/60000 (47%)]\tLoss: 1.391718\n",
      "Training Epoch: 1 [28416/60000 (47%)]\tLoss: 0.765359\n",
      "Training Epoch: 1 [28448/60000 (47%)]\tLoss: 0.836322\n",
      "Training Epoch: 1 [28480/60000 (47%)]\tLoss: 0.737969\n",
      "Training Epoch: 1 [28512/60000 (48%)]\tLoss: 0.740851\n",
      "Training Epoch: 1 [28544/60000 (48%)]\tLoss: 0.597761\n",
      "Training Epoch: 1 [28576/60000 (48%)]\tLoss: 0.416822\n",
      "Training Epoch: 1 [28608/60000 (48%)]\tLoss: 0.717481\n",
      "Training Epoch: 1 [28640/60000 (48%)]\tLoss: 0.546891\n",
      "Training Epoch: 1 [28672/60000 (48%)]\tLoss: 0.885954\n",
      "Training Epoch: 1 [28704/60000 (48%)]\tLoss: 0.543654\n",
      "Training Epoch: 1 [28736/60000 (48%)]\tLoss: 0.810413\n",
      "Training Epoch: 1 [28768/60000 (48%)]\tLoss: 0.728694\n",
      "Training Epoch: 1 [28800/60000 (48%)]\tLoss: 0.585562\n",
      "Training Epoch: 1 [28832/60000 (48%)]\tLoss: 0.497735\n",
      "Training Epoch: 1 [28864/60000 (48%)]\tLoss: 0.593037\n",
      "Training Epoch: 1 [28896/60000 (48%)]\tLoss: 0.928495\n",
      "Training Epoch: 1 [28928/60000 (48%)]\tLoss: 0.584162\n",
      "Training Epoch: 1 [28960/60000 (48%)]\tLoss: 1.290138\n",
      "Training Epoch: 1 [28992/60000 (48%)]\tLoss: 0.854424\n",
      "Training Epoch: 1 [29024/60000 (48%)]\tLoss: 0.730103\n",
      "Training Epoch: 1 [29056/60000 (48%)]\tLoss: 0.658305\n",
      "Training Epoch: 1 [29088/60000 (48%)]\tLoss: 0.746851\n",
      "Training Epoch: 1 [29120/60000 (49%)]\tLoss: 0.879946\n",
      "Training Epoch: 1 [29152/60000 (49%)]\tLoss: 0.877678\n",
      "Training Epoch: 1 [29184/60000 (49%)]\tLoss: 0.508521\n",
      "Training Epoch: 1 [29216/60000 (49%)]\tLoss: 0.736649\n",
      "Training Epoch: 1 [29248/60000 (49%)]\tLoss: 0.736565\n",
      "Training Epoch: 1 [29280/60000 (49%)]\tLoss: 0.738172\n",
      "Training Epoch: 1 [29312/60000 (49%)]\tLoss: 1.244640\n",
      "Training Epoch: 1 [29344/60000 (49%)]\tLoss: 0.648030\n",
      "Training Epoch: 1 [29376/60000 (49%)]\tLoss: 1.019946\n",
      "Training Epoch: 1 [29408/60000 (49%)]\tLoss: 0.486667\n",
      "Training Epoch: 1 [29440/60000 (49%)]\tLoss: 0.692684\n",
      "Training Epoch: 1 [29472/60000 (49%)]\tLoss: 0.402711\n",
      "Training Epoch: 1 [29504/60000 (49%)]\tLoss: 0.466150\n",
      "Training Epoch: 1 [29536/60000 (49%)]\tLoss: 0.681128\n",
      "Training Epoch: 1 [29568/60000 (49%)]\tLoss: 0.721229\n",
      "Training Epoch: 1 [29600/60000 (49%)]\tLoss: 0.681901\n",
      "Training Epoch: 1 [29632/60000 (49%)]\tLoss: 0.431609\n",
      "Training Epoch: 1 [29664/60000 (49%)]\tLoss: 0.859770\n",
      "Training Epoch: 1 [29696/60000 (49%)]\tLoss: 0.642953\n",
      "Training Epoch: 1 [29728/60000 (50%)]\tLoss: 0.711272\n",
      "Training Epoch: 1 [29760/60000 (50%)]\tLoss: 0.635514\n",
      "Training Epoch: 1 [29792/60000 (50%)]\tLoss: 0.475826\n",
      "Training Epoch: 1 [29824/60000 (50%)]\tLoss: 0.525474\n",
      "Training Epoch: 1 [29856/60000 (50%)]\tLoss: 0.851874\n",
      "Training Epoch: 1 [29888/60000 (50%)]\tLoss: 0.646573\n",
      "Training Epoch: 1 [29920/60000 (50%)]\tLoss: 0.542546\n",
      "Training Epoch: 1 [29952/60000 (50%)]\tLoss: 0.974017\n",
      "Training Epoch: 1 [29984/60000 (50%)]\tLoss: 1.284859\n",
      "Training Epoch: 1 [30016/60000 (50%)]\tLoss: 0.332450\n",
      "Training Epoch: 1 [30048/60000 (50%)]\tLoss: 0.479976\n",
      "Training Epoch: 1 [30080/60000 (50%)]\tLoss: 1.912297\n",
      "Training Epoch: 1 [30112/60000 (50%)]\tLoss: 0.494377\n",
      "Training Epoch: 1 [30144/60000 (50%)]\tLoss: 0.748992\n",
      "Training Epoch: 1 [30176/60000 (50%)]\tLoss: 0.835558\n",
      "Training Epoch: 1 [30208/60000 (50%)]\tLoss: 0.521414\n",
      "Training Epoch: 1 [30240/60000 (50%)]\tLoss: 0.561618\n",
      "Training Epoch: 1 [30272/60000 (50%)]\tLoss: 0.702370\n",
      "Training Epoch: 1 [30304/60000 (51%)]\tLoss: 0.418386\n",
      "Training Epoch: 1 [30336/60000 (51%)]\tLoss: 0.475450\n",
      "Training Epoch: 1 [30368/60000 (51%)]\tLoss: 0.415505\n",
      "Training Epoch: 1 [30400/60000 (51%)]\tLoss: 0.464163\n",
      "Training Epoch: 1 [30432/60000 (51%)]\tLoss: 0.449299\n",
      "Training Epoch: 1 [30464/60000 (51%)]\tLoss: 0.617296\n",
      "Training Epoch: 1 [30496/60000 (51%)]\tLoss: 0.684573\n",
      "Training Epoch: 1 [30528/60000 (51%)]\tLoss: 0.806583\n",
      "Training Epoch: 1 [30560/60000 (51%)]\tLoss: 0.600590\n",
      "Training Epoch: 1 [30592/60000 (51%)]\tLoss: 0.654856\n",
      "Training Epoch: 1 [30624/60000 (51%)]\tLoss: 0.637449\n",
      "Training Epoch: 1 [30656/60000 (51%)]\tLoss: 0.535695\n",
      "Training Epoch: 1 [30688/60000 (51%)]\tLoss: 0.591754\n",
      "Training Epoch: 1 [30720/60000 (51%)]\tLoss: 0.796857\n",
      "Training Epoch: 1 [30752/60000 (51%)]\tLoss: 1.163366\n",
      "Training Epoch: 1 [30784/60000 (51%)]\tLoss: 0.486728\n",
      "Training Epoch: 1 [30816/60000 (51%)]\tLoss: 0.692348\n",
      "Training Epoch: 1 [30848/60000 (51%)]\tLoss: 0.514211\n",
      "Training Epoch: 1 [30880/60000 (51%)]\tLoss: 0.925941\n",
      "Training Epoch: 1 [30912/60000 (52%)]\tLoss: 0.930087\n",
      "Training Epoch: 1 [30944/60000 (52%)]\tLoss: 0.517494\n",
      "Training Epoch: 1 [30976/60000 (52%)]\tLoss: 0.726738\n",
      "Training Epoch: 1 [31008/60000 (52%)]\tLoss: 0.642171\n",
      "Training Epoch: 1 [31040/60000 (52%)]\tLoss: 0.560442\n",
      "Training Epoch: 1 [31072/60000 (52%)]\tLoss: 0.476312\n",
      "Training Epoch: 1 [31104/60000 (52%)]\tLoss: 0.609938\n",
      "Training Epoch: 1 [31136/60000 (52%)]\tLoss: 0.794634\n",
      "Training Epoch: 1 [31168/60000 (52%)]\tLoss: 0.361627\n",
      "Training Epoch: 1 [31200/60000 (52%)]\tLoss: 0.899610\n",
      "Training Epoch: 1 [31232/60000 (52%)]\tLoss: 0.741762\n",
      "Training Epoch: 1 [31264/60000 (52%)]\tLoss: 0.848948\n",
      "Training Epoch: 1 [31296/60000 (52%)]\tLoss: 1.648617\n",
      "Training Epoch: 1 [31328/60000 (52%)]\tLoss: 0.861013\n",
      "Training Epoch: 1 [31360/60000 (52%)]\tLoss: 0.769202\n",
      "Training Epoch: 1 [31392/60000 (52%)]\tLoss: 0.673353\n",
      "Training Epoch: 1 [31424/60000 (52%)]\tLoss: 0.748529\n",
      "Training Epoch: 1 [31456/60000 (52%)]\tLoss: 0.715331\n",
      "Training Epoch: 1 [31488/60000 (52%)]\tLoss: 0.432951\n",
      "Training Epoch: 1 [31520/60000 (53%)]\tLoss: 0.854822\n",
      "Training Epoch: 1 [31552/60000 (53%)]\tLoss: 0.781499\n",
      "Training Epoch: 1 [31584/60000 (53%)]\tLoss: 0.544927\n",
      "Training Epoch: 1 [31616/60000 (53%)]\tLoss: 1.014538\n",
      "Training Epoch: 1 [31648/60000 (53%)]\tLoss: 0.441314\n",
      "Training Epoch: 1 [31680/60000 (53%)]\tLoss: 0.853636\n",
      "Training Epoch: 1 [31712/60000 (53%)]\tLoss: 0.496441\n",
      "Training Epoch: 1 [31744/60000 (53%)]\tLoss: 0.456982\n",
      "Training Epoch: 1 [31776/60000 (53%)]\tLoss: 0.648980\n",
      "Training Epoch: 1 [31808/60000 (53%)]\tLoss: 0.530323\n",
      "Training Epoch: 1 [31840/60000 (53%)]\tLoss: 0.741974\n",
      "Training Epoch: 1 [31872/60000 (53%)]\tLoss: 0.564814\n",
      "Training Epoch: 1 [31904/60000 (53%)]\tLoss: 0.618777\n",
      "Training Epoch: 1 [31936/60000 (53%)]\tLoss: 0.647298\n",
      "Training Epoch: 1 [31968/60000 (53%)]\tLoss: 0.338514\n",
      "Training Epoch: 1 [32000/60000 (53%)]\tLoss: 0.791844\n",
      "Training Epoch: 1 [32032/60000 (53%)]\tLoss: 0.817763\n",
      "Training Epoch: 1 [32064/60000 (53%)]\tLoss: 1.009517\n",
      "Training Epoch: 1 [32096/60000 (53%)]\tLoss: 0.683876\n",
      "Training Epoch: 1 [32128/60000 (54%)]\tLoss: 0.516662\n",
      "Training Epoch: 1 [32160/60000 (54%)]\tLoss: 0.555933\n",
      "Training Epoch: 1 [32192/60000 (54%)]\tLoss: 1.391902\n",
      "Training Epoch: 1 [32224/60000 (54%)]\tLoss: 0.793239\n",
      "Training Epoch: 1 [32256/60000 (54%)]\tLoss: 0.446312\n",
      "Training Epoch: 1 [32288/60000 (54%)]\tLoss: 0.391951\n",
      "Training Epoch: 1 [32320/60000 (54%)]\tLoss: 0.222519\n",
      "Training Epoch: 1 [32352/60000 (54%)]\tLoss: 0.591057\n",
      "Training Epoch: 1 [32384/60000 (54%)]\tLoss: 0.417612\n",
      "Training Epoch: 1 [32416/60000 (54%)]\tLoss: 0.331163\n",
      "Training Epoch: 1 [32448/60000 (54%)]\tLoss: 0.510412\n",
      "Training Epoch: 1 [32480/60000 (54%)]\tLoss: 0.376412\n",
      "Training Epoch: 1 [32512/60000 (54%)]\tLoss: 0.853355\n",
      "Training Epoch: 1 [32544/60000 (54%)]\tLoss: 0.551785\n",
      "Training Epoch: 1 [32576/60000 (54%)]\tLoss: 1.206784\n",
      "Training Epoch: 1 [32608/60000 (54%)]\tLoss: 0.574237\n",
      "Training Epoch: 1 [32640/60000 (54%)]\tLoss: 1.060687\n",
      "Training Epoch: 1 [32672/60000 (54%)]\tLoss: 0.551316\n",
      "Training Epoch: 1 [32704/60000 (55%)]\tLoss: 0.328847\n",
      "Training Epoch: 1 [32736/60000 (55%)]\tLoss: 0.501735\n",
      "Training Epoch: 1 [32768/60000 (55%)]\tLoss: 0.500322\n",
      "Training Epoch: 1 [32800/60000 (55%)]\tLoss: 0.408766\n",
      "Training Epoch: 1 [32832/60000 (55%)]\tLoss: 0.595148\n",
      "Training Epoch: 1 [32864/60000 (55%)]\tLoss: 1.142112\n",
      "Training Epoch: 1 [32896/60000 (55%)]\tLoss: 0.753901\n",
      "Training Epoch: 1 [32928/60000 (55%)]\tLoss: 1.108968\n",
      "Training Epoch: 1 [32960/60000 (55%)]\tLoss: 0.768857\n",
      "Training Epoch: 1 [32992/60000 (55%)]\tLoss: 0.479652\n",
      "Training Epoch: 1 [33024/60000 (55%)]\tLoss: 0.643016\n",
      "Training Epoch: 1 [33056/60000 (55%)]\tLoss: 0.768243\n",
      "Training Epoch: 1 [33088/60000 (55%)]\tLoss: 1.429696\n",
      "Training Epoch: 1 [33120/60000 (55%)]\tLoss: 0.575474\n",
      "Training Epoch: 1 [33152/60000 (55%)]\tLoss: 0.571418\n",
      "Training Epoch: 1 [33184/60000 (55%)]\tLoss: 0.762985\n",
      "Training Epoch: 1 [33216/60000 (55%)]\tLoss: 0.501697\n",
      "Training Epoch: 1 [33248/60000 (55%)]\tLoss: 0.851665\n",
      "Training Epoch: 1 [33280/60000 (55%)]\tLoss: 0.857641\n",
      "Training Epoch: 1 [33312/60000 (56%)]\tLoss: 0.736898\n",
      "Training Epoch: 1 [33344/60000 (56%)]\tLoss: 0.475534\n",
      "Training Epoch: 1 [33376/60000 (56%)]\tLoss: 1.055399\n",
      "Training Epoch: 1 [33408/60000 (56%)]\tLoss: 0.431912\n",
      "Training Epoch: 1 [33440/60000 (56%)]\tLoss: 0.614137\n",
      "Training Epoch: 1 [33472/60000 (56%)]\tLoss: 0.742887\n",
      "Training Epoch: 1 [33504/60000 (56%)]\tLoss: 0.746461\n",
      "Training Epoch: 1 [33536/60000 (56%)]\tLoss: 0.433715\n",
      "Training Epoch: 1 [33568/60000 (56%)]\tLoss: 0.938638\n",
      "Training Epoch: 1 [33600/60000 (56%)]\tLoss: 0.903647\n",
      "Training Epoch: 1 [33632/60000 (56%)]\tLoss: 0.594950\n",
      "Training Epoch: 1 [33664/60000 (56%)]\tLoss: 0.529471\n",
      "Training Epoch: 1 [33696/60000 (56%)]\tLoss: 0.999191\n",
      "Training Epoch: 1 [33728/60000 (56%)]\tLoss: 0.714009\n",
      "Training Epoch: 1 [33760/60000 (56%)]\tLoss: 0.536194\n",
      "Training Epoch: 1 [33792/60000 (56%)]\tLoss: 0.695820\n",
      "Training Epoch: 1 [33824/60000 (56%)]\tLoss: 0.571359\n",
      "Training Epoch: 1 [33856/60000 (56%)]\tLoss: 0.459846\n",
      "Training Epoch: 1 [33888/60000 (56%)]\tLoss: 0.651882\n",
      "Training Epoch: 1 [33920/60000 (57%)]\tLoss: 0.493223\n",
      "Training Epoch: 1 [33952/60000 (57%)]\tLoss: 0.800265\n",
      "Training Epoch: 1 [33984/60000 (57%)]\tLoss: 0.423856\n",
      "Training Epoch: 1 [34016/60000 (57%)]\tLoss: 0.728934\n",
      "Training Epoch: 1 [34048/60000 (57%)]\tLoss: 0.370430\n",
      "Training Epoch: 1 [34080/60000 (57%)]\tLoss: 0.700945\n",
      "Training Epoch: 1 [34112/60000 (57%)]\tLoss: 0.774360\n",
      "Training Epoch: 1 [34144/60000 (57%)]\tLoss: 0.929816\n",
      "Training Epoch: 1 [34176/60000 (57%)]\tLoss: 1.063596\n",
      "Training Epoch: 1 [34208/60000 (57%)]\tLoss: 0.340519\n",
      "Training Epoch: 1 [34240/60000 (57%)]\tLoss: 0.766910\n",
      "Training Epoch: 1 [34272/60000 (57%)]\tLoss: 0.367249\n",
      "Training Epoch: 1 [34304/60000 (57%)]\tLoss: 0.808787\n",
      "Training Epoch: 1 [34336/60000 (57%)]\tLoss: 0.604591\n",
      "Training Epoch: 1 [34368/60000 (57%)]\tLoss: 0.682372\n",
      "Training Epoch: 1 [34400/60000 (57%)]\tLoss: 0.738748\n",
      "Training Epoch: 1 [34432/60000 (57%)]\tLoss: 0.795253\n",
      "Training Epoch: 1 [34464/60000 (57%)]\tLoss: 0.577290\n",
      "Training Epoch: 1 [34496/60000 (57%)]\tLoss: 0.414754\n",
      "Training Epoch: 1 [34528/60000 (58%)]\tLoss: 0.483342\n",
      "Training Epoch: 1 [34560/60000 (58%)]\tLoss: 1.065888\n",
      "Training Epoch: 1 [34592/60000 (58%)]\tLoss: 0.659762\n",
      "Training Epoch: 1 [34624/60000 (58%)]\tLoss: 0.961580\n",
      "Training Epoch: 1 [34656/60000 (58%)]\tLoss: 0.551347\n",
      "Training Epoch: 1 [34688/60000 (58%)]\tLoss: 1.366398\n",
      "Training Epoch: 1 [34720/60000 (58%)]\tLoss: 0.453992\n",
      "Training Epoch: 1 [34752/60000 (58%)]\tLoss: 0.820130\n",
      "Training Epoch: 1 [34784/60000 (58%)]\tLoss: 0.601666\n",
      "Training Epoch: 1 [34816/60000 (58%)]\tLoss: 0.615525\n",
      "Training Epoch: 1 [34848/60000 (58%)]\tLoss: 0.532670\n",
      "Training Epoch: 1 [34880/60000 (58%)]\tLoss: 0.650965\n",
      "Training Epoch: 1 [34912/60000 (58%)]\tLoss: 0.331557\n",
      "Training Epoch: 1 [34944/60000 (58%)]\tLoss: 0.658096\n",
      "Training Epoch: 1 [34976/60000 (58%)]\tLoss: 0.794009\n",
      "Training Epoch: 1 [35008/60000 (58%)]\tLoss: 0.692916\n",
      "Training Epoch: 1 [35040/60000 (58%)]\tLoss: 0.475421\n",
      "Training Epoch: 1 [35072/60000 (58%)]\tLoss: 0.921224\n",
      "Training Epoch: 1 [35104/60000 (59%)]\tLoss: 0.534847\n",
      "Training Epoch: 1 [35136/60000 (59%)]\tLoss: 0.555847\n",
      "Training Epoch: 1 [35168/60000 (59%)]\tLoss: 0.664285\n",
      "Training Epoch: 1 [35200/60000 (59%)]\tLoss: 0.482329\n",
      "Training Epoch: 1 [35232/60000 (59%)]\tLoss: 0.698302\n",
      "Training Epoch: 1 [35264/60000 (59%)]\tLoss: 0.772683\n",
      "Training Epoch: 1 [35296/60000 (59%)]\tLoss: 0.428815\n",
      "Training Epoch: 1 [35328/60000 (59%)]\tLoss: 0.522719\n",
      "Training Epoch: 1 [35360/60000 (59%)]\tLoss: 0.595761\n",
      "Training Epoch: 1 [35392/60000 (59%)]\tLoss: 0.596873\n",
      "Training Epoch: 1 [35424/60000 (59%)]\tLoss: 0.451836\n",
      "Training Epoch: 1 [35456/60000 (59%)]\tLoss: 0.980842\n",
      "Training Epoch: 1 [35488/60000 (59%)]\tLoss: 0.979193\n",
      "Training Epoch: 1 [35520/60000 (59%)]\tLoss: 0.443569\n",
      "Training Epoch: 1 [35552/60000 (59%)]\tLoss: 0.661443\n",
      "Training Epoch: 1 [35584/60000 (59%)]\tLoss: 1.073683\n",
      "Training Epoch: 1 [35616/60000 (59%)]\tLoss: 0.463932\n",
      "Training Epoch: 1 [35648/60000 (59%)]\tLoss: 0.511826\n",
      "Training Epoch: 1 [35680/60000 (59%)]\tLoss: 0.786746\n",
      "Training Epoch: 1 [35712/60000 (60%)]\tLoss: 0.759251\n",
      "Training Epoch: 1 [35744/60000 (60%)]\tLoss: 0.402378\n",
      "Training Epoch: 1 [35776/60000 (60%)]\tLoss: 0.437361\n",
      "Training Epoch: 1 [35808/60000 (60%)]\tLoss: 0.638528\n",
      "Training Epoch: 1 [35840/60000 (60%)]\tLoss: 0.909637\n",
      "Training Epoch: 1 [35872/60000 (60%)]\tLoss: 0.984712\n",
      "Training Epoch: 1 [35904/60000 (60%)]\tLoss: 0.377472\n",
      "Training Epoch: 1 [35936/60000 (60%)]\tLoss: 0.717675\n",
      "Training Epoch: 1 [35968/60000 (60%)]\tLoss: 0.588317\n",
      "Training Epoch: 1 [36000/60000 (60%)]\tLoss: 0.794089\n",
      "Training Epoch: 1 [36032/60000 (60%)]\tLoss: 1.060551\n",
      "Training Epoch: 1 [36064/60000 (60%)]\tLoss: 0.739197\n",
      "Training Epoch: 1 [36096/60000 (60%)]\tLoss: 0.711310\n",
      "Training Epoch: 1 [36128/60000 (60%)]\tLoss: 0.873711\n",
      "Training Epoch: 1 [36160/60000 (60%)]\tLoss: 0.847136\n",
      "Training Epoch: 1 [36192/60000 (60%)]\tLoss: 0.972273\n",
      "Training Epoch: 1 [36224/60000 (60%)]\tLoss: 0.451555\n",
      "Training Epoch: 1 [36256/60000 (60%)]\tLoss: 0.617496\n",
      "Training Epoch: 1 [36288/60000 (60%)]\tLoss: 0.526343\n",
      "Training Epoch: 1 [36320/60000 (61%)]\tLoss: 0.867378\n",
      "Training Epoch: 1 [36352/60000 (61%)]\tLoss: 0.867095\n",
      "Training Epoch: 1 [36384/60000 (61%)]\tLoss: 0.715916\n",
      "Training Epoch: 1 [36416/60000 (61%)]\tLoss: 0.553228\n",
      "Training Epoch: 1 [36448/60000 (61%)]\tLoss: 0.549195\n",
      "Training Epoch: 1 [36480/60000 (61%)]\tLoss: 1.060590\n",
      "Training Epoch: 1 [36512/60000 (61%)]\tLoss: 0.519472\n",
      "Training Epoch: 1 [36544/60000 (61%)]\tLoss: 0.787933\n",
      "Training Epoch: 1 [36576/60000 (61%)]\tLoss: 0.713319\n",
      "Training Epoch: 1 [36608/60000 (61%)]\tLoss: 0.643676\n",
      "Training Epoch: 1 [36640/60000 (61%)]\tLoss: 0.697397\n",
      "Training Epoch: 1 [36672/60000 (61%)]\tLoss: 1.117031\n",
      "Training Epoch: 1 [36704/60000 (61%)]\tLoss: 0.731196\n",
      "Training Epoch: 1 [36736/60000 (61%)]\tLoss: 0.630668\n",
      "Training Epoch: 1 [36768/60000 (61%)]\tLoss: 0.366378\n",
      "Training Epoch: 1 [36800/60000 (61%)]\tLoss: 0.507205\n",
      "Training Epoch: 1 [36832/60000 (61%)]\tLoss: 0.481547\n",
      "Training Epoch: 1 [36864/60000 (61%)]\tLoss: 0.270171\n",
      "Training Epoch: 1 [36896/60000 (61%)]\tLoss: 0.442836\n",
      "Training Epoch: 1 [36928/60000 (62%)]\tLoss: 0.855925\n",
      "Training Epoch: 1 [36960/60000 (62%)]\tLoss: 0.486682\n",
      "Training Epoch: 1 [36992/60000 (62%)]\tLoss: 0.683171\n",
      "Training Epoch: 1 [37024/60000 (62%)]\tLoss: 0.485083\n",
      "Training Epoch: 1 [37056/60000 (62%)]\tLoss: 0.563247\n",
      "Training Epoch: 1 [37088/60000 (62%)]\tLoss: 0.413985\n",
      "Training Epoch: 1 [37120/60000 (62%)]\tLoss: 0.440650\n",
      "Training Epoch: 1 [37152/60000 (62%)]\tLoss: 0.540431\n",
      "Training Epoch: 1 [37184/60000 (62%)]\tLoss: 0.595455\n",
      "Training Epoch: 1 [37216/60000 (62%)]\tLoss: 0.656771\n",
      "Training Epoch: 1 [37248/60000 (62%)]\tLoss: 0.643826\n",
      "Training Epoch: 1 [37280/60000 (62%)]\tLoss: 0.783008\n",
      "Training Epoch: 1 [37312/60000 (62%)]\tLoss: 0.471221\n",
      "Training Epoch: 1 [37344/60000 (62%)]\tLoss: 0.444001\n",
      "Training Epoch: 1 [37376/60000 (62%)]\tLoss: 1.253234\n",
      "Training Epoch: 1 [37408/60000 (62%)]\tLoss: 0.780107\n",
      "Training Epoch: 1 [37440/60000 (62%)]\tLoss: 0.630497\n",
      "Training Epoch: 1 [37472/60000 (62%)]\tLoss: 1.020856\n",
      "Training Epoch: 1 [37504/60000 (63%)]\tLoss: 0.625726\n",
      "Training Epoch: 1 [37536/60000 (63%)]\tLoss: 0.631582\n",
      "Training Epoch: 1 [37568/60000 (63%)]\tLoss: 0.375138\n",
      "Training Epoch: 1 [37600/60000 (63%)]\tLoss: 0.583550\n",
      "Training Epoch: 1 [37632/60000 (63%)]\tLoss: 0.577917\n",
      "Training Epoch: 1 [37664/60000 (63%)]\tLoss: 0.396803\n",
      "Training Epoch: 1 [37696/60000 (63%)]\tLoss: 0.805029\n",
      "Training Epoch: 1 [37728/60000 (63%)]\tLoss: 0.525069\n",
      "Training Epoch: 1 [37760/60000 (63%)]\tLoss: 0.936884\n",
      "Training Epoch: 1 [37792/60000 (63%)]\tLoss: 0.808642\n",
      "Training Epoch: 1 [37824/60000 (63%)]\tLoss: 0.726603\n",
      "Training Epoch: 1 [37856/60000 (63%)]\tLoss: 0.577209\n",
      "Training Epoch: 1 [37888/60000 (63%)]\tLoss: 0.612273\n",
      "Training Epoch: 1 [37920/60000 (63%)]\tLoss: 0.643741\n",
      "Training Epoch: 1 [37952/60000 (63%)]\tLoss: 0.543184\n",
      "Training Epoch: 1 [37984/60000 (63%)]\tLoss: 0.554432\n",
      "Training Epoch: 1 [38016/60000 (63%)]\tLoss: 0.810075\n",
      "Training Epoch: 1 [38048/60000 (63%)]\tLoss: 0.387839\n",
      "Training Epoch: 1 [38080/60000 (63%)]\tLoss: 0.502602\n",
      "Training Epoch: 1 [38112/60000 (64%)]\tLoss: 0.747296\n",
      "Training Epoch: 1 [38144/60000 (64%)]\tLoss: 0.772387\n",
      "Training Epoch: 1 [38176/60000 (64%)]\tLoss: 1.164031\n",
      "Training Epoch: 1 [38208/60000 (64%)]\tLoss: 1.051831\n",
      "Training Epoch: 1 [38240/60000 (64%)]\tLoss: 0.700610\n",
      "Training Epoch: 1 [38272/60000 (64%)]\tLoss: 0.505606\n",
      "Training Epoch: 1 [38304/60000 (64%)]\tLoss: 0.633631\n",
      "Training Epoch: 1 [38336/60000 (64%)]\tLoss: 0.702129\n",
      "Training Epoch: 1 [38368/60000 (64%)]\tLoss: 0.511960\n",
      "Training Epoch: 1 [38400/60000 (64%)]\tLoss: 0.624598\n",
      "Training Epoch: 1 [38432/60000 (64%)]\tLoss: 0.849608\n",
      "Training Epoch: 1 [38464/60000 (64%)]\tLoss: 0.553889\n",
      "Training Epoch: 1 [38496/60000 (64%)]\tLoss: 1.044174\n",
      "Training Epoch: 1 [38528/60000 (64%)]\tLoss: 0.389998\n",
      "Training Epoch: 1 [38560/60000 (64%)]\tLoss: 0.452462\n",
      "Training Epoch: 1 [38592/60000 (64%)]\tLoss: 0.813873\n",
      "Training Epoch: 1 [38624/60000 (64%)]\tLoss: 0.605784\n",
      "Training Epoch: 1 [38656/60000 (64%)]\tLoss: 0.646675\n",
      "Training Epoch: 1 [38688/60000 (64%)]\tLoss: 0.614908\n",
      "Training Epoch: 1 [38720/60000 (65%)]\tLoss: 0.700915\n",
      "Training Epoch: 1 [38752/60000 (65%)]\tLoss: 0.914963\n",
      "Training Epoch: 1 [38784/60000 (65%)]\tLoss: 0.646781\n",
      "Training Epoch: 1 [38816/60000 (65%)]\tLoss: 0.581073\n",
      "Training Epoch: 1 [38848/60000 (65%)]\tLoss: 0.437360\n",
      "Training Epoch: 1 [38880/60000 (65%)]\tLoss: 0.347069\n",
      "Training Epoch: 1 [38912/60000 (65%)]\tLoss: 0.758627\n",
      "Training Epoch: 1 [38944/60000 (65%)]\tLoss: 1.063294\n",
      "Training Epoch: 1 [38976/60000 (65%)]\tLoss: 0.667040\n",
      "Training Epoch: 1 [39008/60000 (65%)]\tLoss: 0.570513\n",
      "Training Epoch: 1 [39040/60000 (65%)]\tLoss: 0.942443\n",
      "Training Epoch: 1 [39072/60000 (65%)]\tLoss: 0.915455\n",
      "Training Epoch: 1 [39104/60000 (65%)]\tLoss: 0.446681\n",
      "Training Epoch: 1 [39136/60000 (65%)]\tLoss: 0.607693\n",
      "Training Epoch: 1 [39168/60000 (65%)]\tLoss: 0.693620\n",
      "Training Epoch: 1 [39200/60000 (65%)]\tLoss: 0.389755\n",
      "Training Epoch: 1 [39232/60000 (65%)]\tLoss: 0.542728\n",
      "Training Epoch: 1 [39264/60000 (65%)]\tLoss: 0.528787\n",
      "Training Epoch: 1 [39296/60000 (65%)]\tLoss: 0.266673\n",
      "Training Epoch: 1 [39328/60000 (66%)]\tLoss: 0.957262\n",
      "Training Epoch: 1 [39360/60000 (66%)]\tLoss: 0.442346\n",
      "Training Epoch: 1 [39392/60000 (66%)]\tLoss: 0.821924\n",
      "Training Epoch: 1 [39424/60000 (66%)]\tLoss: 0.825026\n",
      "Training Epoch: 1 [39456/60000 (66%)]\tLoss: 0.522415\n",
      "Training Epoch: 1 [39488/60000 (66%)]\tLoss: 0.279226\n",
      "Training Epoch: 1 [39520/60000 (66%)]\tLoss: 0.554072\n",
      "Training Epoch: 1 [39552/60000 (66%)]\tLoss: 0.570842\n",
      "Training Epoch: 1 [39584/60000 (66%)]\tLoss: 0.655666\n",
      "Training Epoch: 1 [39616/60000 (66%)]\tLoss: 0.453223\n",
      "Training Epoch: 1 [39648/60000 (66%)]\tLoss: 0.737781\n",
      "Training Epoch: 1 [39680/60000 (66%)]\tLoss: 0.353645\n",
      "Training Epoch: 1 [39712/60000 (66%)]\tLoss: 0.283718\n",
      "Training Epoch: 1 [39744/60000 (66%)]\tLoss: 0.714296\n",
      "Training Epoch: 1 [39776/60000 (66%)]\tLoss: 0.660711\n",
      "Training Epoch: 1 [39808/60000 (66%)]\tLoss: 0.855934\n",
      "Training Epoch: 1 [39840/60000 (66%)]\tLoss: 0.733866\n",
      "Training Epoch: 1 [39872/60000 (66%)]\tLoss: 0.459785\n",
      "Training Epoch: 1 [39904/60000 (67%)]\tLoss: 0.770354\n",
      "Training Epoch: 1 [39936/60000 (67%)]\tLoss: 0.556309\n",
      "Training Epoch: 1 [39968/60000 (67%)]\tLoss: 0.475171\n",
      "Training Epoch: 1 [40000/60000 (67%)]\tLoss: 0.639285\n",
      "Training Epoch: 1 [40032/60000 (67%)]\tLoss: 0.705423\n",
      "Training Epoch: 1 [40064/60000 (67%)]\tLoss: 0.385762\n",
      "Training Epoch: 1 [40096/60000 (67%)]\tLoss: 0.537493\n",
      "Training Epoch: 1 [40128/60000 (67%)]\tLoss: 1.225489\n",
      "Training Epoch: 1 [40160/60000 (67%)]\tLoss: 1.238797\n",
      "Training Epoch: 1 [40192/60000 (67%)]\tLoss: 0.844909\n",
      "Training Epoch: 1 [40224/60000 (67%)]\tLoss: 0.544309\n",
      "Training Epoch: 1 [40256/60000 (67%)]\tLoss: 0.810570\n",
      "Training Epoch: 1 [40288/60000 (67%)]\tLoss: 1.397145\n",
      "Training Epoch: 1 [40320/60000 (67%)]\tLoss: 0.623105\n",
      "Training Epoch: 1 [40352/60000 (67%)]\tLoss: 0.293072\n",
      "Training Epoch: 1 [40384/60000 (67%)]\tLoss: 0.506129\n",
      "Training Epoch: 1 [40416/60000 (67%)]\tLoss: 0.663703\n",
      "Training Epoch: 1 [40448/60000 (67%)]\tLoss: 0.513570\n",
      "Training Epoch: 1 [40480/60000 (67%)]\tLoss: 0.711679\n",
      "Training Epoch: 1 [40512/60000 (68%)]\tLoss: 0.519381\n",
      "Training Epoch: 1 [40544/60000 (68%)]\tLoss: 0.616285\n",
      "Training Epoch: 1 [40576/60000 (68%)]\tLoss: 1.260919\n",
      "Training Epoch: 1 [40608/60000 (68%)]\tLoss: 0.652895\n",
      "Training Epoch: 1 [40640/60000 (68%)]\tLoss: 0.479339\n",
      "Training Epoch: 1 [40672/60000 (68%)]\tLoss: 0.749106\n",
      "Training Epoch: 1 [40704/60000 (68%)]\tLoss: 1.011056\n",
      "Training Epoch: 1 [40736/60000 (68%)]\tLoss: 0.985728\n",
      "Training Epoch: 1 [40768/60000 (68%)]\tLoss: 0.561689\n",
      "Training Epoch: 1 [40800/60000 (68%)]\tLoss: 0.794631\n",
      "Training Epoch: 1 [40832/60000 (68%)]\tLoss: 0.601007\n",
      "Training Epoch: 1 [40864/60000 (68%)]\tLoss: 0.552694\n",
      "Training Epoch: 1 [40896/60000 (68%)]\tLoss: 0.654218\n",
      "Training Epoch: 1 [40928/60000 (68%)]\tLoss: 0.543142\n",
      "Training Epoch: 1 [40960/60000 (68%)]\tLoss: 0.423787\n",
      "Training Epoch: 1 [40992/60000 (68%)]\tLoss: 0.427495\n",
      "Training Epoch: 1 [41024/60000 (68%)]\tLoss: 0.642784\n",
      "Training Epoch: 1 [41056/60000 (68%)]\tLoss: 0.624055\n",
      "Training Epoch: 1 [41088/60000 (68%)]\tLoss: 0.413494\n",
      "Training Epoch: 1 [41120/60000 (69%)]\tLoss: 1.066554\n",
      "Training Epoch: 1 [41152/60000 (69%)]\tLoss: 0.692386\n",
      "Training Epoch: 1 [41184/60000 (69%)]\tLoss: 0.327367\n",
      "Training Epoch: 1 [41216/60000 (69%)]\tLoss: 1.015407\n",
      "Training Epoch: 1 [41248/60000 (69%)]\tLoss: 1.168085\n",
      "Training Epoch: 1 [41280/60000 (69%)]\tLoss: 0.742556\n",
      "Training Epoch: 1 [41312/60000 (69%)]\tLoss: 0.791656\n",
      "Training Epoch: 1 [41344/60000 (69%)]\tLoss: 0.697967\n",
      "Training Epoch: 1 [41376/60000 (69%)]\tLoss: 0.563780\n",
      "Training Epoch: 1 [41408/60000 (69%)]\tLoss: 0.680654\n",
      "Training Epoch: 1 [41440/60000 (69%)]\tLoss: 0.821382\n",
      "Training Epoch: 1 [41472/60000 (69%)]\tLoss: 0.588252\n",
      "Training Epoch: 1 [41504/60000 (69%)]\tLoss: 0.633202\n",
      "Training Epoch: 1 [41536/60000 (69%)]\tLoss: 0.533694\n",
      "Training Epoch: 1 [41568/60000 (69%)]\tLoss: 0.369305\n",
      "Training Epoch: 1 [41600/60000 (69%)]\tLoss: 0.634297\n",
      "Training Epoch: 1 [41632/60000 (69%)]\tLoss: 0.634210\n",
      "Training Epoch: 1 [41664/60000 (69%)]\tLoss: 0.576835\n",
      "Training Epoch: 1 [41696/60000 (69%)]\tLoss: 0.558333\n",
      "Training Epoch: 1 [41728/60000 (70%)]\tLoss: 0.561657\n",
      "Training Epoch: 1 [41760/60000 (70%)]\tLoss: 0.503499\n",
      "Training Epoch: 1 [41792/60000 (70%)]\tLoss: 0.596179\n",
      "Training Epoch: 1 [41824/60000 (70%)]\tLoss: 0.409556\n",
      "Training Epoch: 1 [41856/60000 (70%)]\tLoss: 0.616668\n",
      "Training Epoch: 1 [41888/60000 (70%)]\tLoss: 0.426966\n",
      "Training Epoch: 1 [41920/60000 (70%)]\tLoss: 0.578763\n",
      "Training Epoch: 1 [41952/60000 (70%)]\tLoss: 0.510212\n",
      "Training Epoch: 1 [41984/60000 (70%)]\tLoss: 0.601388\n",
      "Training Epoch: 1 [42016/60000 (70%)]\tLoss: 0.357176\n",
      "Training Epoch: 1 [42048/60000 (70%)]\tLoss: 0.344209\n",
      "Training Epoch: 1 [42080/60000 (70%)]\tLoss: 0.558335\n",
      "Training Epoch: 1 [42112/60000 (70%)]\tLoss: 0.491336\n",
      "Training Epoch: 1 [42144/60000 (70%)]\tLoss: 0.571767\n",
      "Training Epoch: 1 [42176/60000 (70%)]\tLoss: 0.339613\n",
      "Training Epoch: 1 [42208/60000 (70%)]\tLoss: 0.275815\n",
      "Training Epoch: 1 [42240/60000 (70%)]\tLoss: 0.536931\n",
      "Training Epoch: 1 [42272/60000 (70%)]\tLoss: 0.643541\n",
      "Training Epoch: 1 [42304/60000 (71%)]\tLoss: 0.810484\n",
      "Training Epoch: 1 [42336/60000 (71%)]\tLoss: 0.584581\n",
      "Training Epoch: 1 [42368/60000 (71%)]\tLoss: 0.689611\n",
      "Training Epoch: 1 [42400/60000 (71%)]\tLoss: 0.648922\n",
      "Training Epoch: 1 [42432/60000 (71%)]\tLoss: 0.713951\n",
      "Training Epoch: 1 [42464/60000 (71%)]\tLoss: 0.470079\n",
      "Training Epoch: 1 [42496/60000 (71%)]\tLoss: 0.648644\n",
      "Training Epoch: 1 [42528/60000 (71%)]\tLoss: 0.374173\n",
      "Training Epoch: 1 [42560/60000 (71%)]\tLoss: 0.597294\n",
      "Training Epoch: 1 [42592/60000 (71%)]\tLoss: 0.558953\n",
      "Training Epoch: 1 [42624/60000 (71%)]\tLoss: 0.481342\n",
      "Training Epoch: 1 [42656/60000 (71%)]\tLoss: 0.518969\n",
      "Training Epoch: 1 [42688/60000 (71%)]\tLoss: 0.341619\n",
      "Training Epoch: 1 [42720/60000 (71%)]\tLoss: 0.377239\n",
      "Training Epoch: 1 [42752/60000 (71%)]\tLoss: 0.559293\n",
      "Training Epoch: 1 [42784/60000 (71%)]\tLoss: 0.466023\n",
      "Training Epoch: 1 [42816/60000 (71%)]\tLoss: 0.617860\n",
      "Training Epoch: 1 [42848/60000 (71%)]\tLoss: 0.514495\n",
      "Training Epoch: 1 [42880/60000 (71%)]\tLoss: 0.410883\n",
      "Training Epoch: 1 [42912/60000 (72%)]\tLoss: 0.428000\n",
      "Training Epoch: 1 [42944/60000 (72%)]\tLoss: 0.436952\n",
      "Training Epoch: 1 [42976/60000 (72%)]\tLoss: 0.482838\n",
      "Training Epoch: 1 [43008/60000 (72%)]\tLoss: 0.626191\n",
      "Training Epoch: 1 [43040/60000 (72%)]\tLoss: 1.095040\n",
      "Training Epoch: 1 [43072/60000 (72%)]\tLoss: 0.482485\n",
      "Training Epoch: 1 [43104/60000 (72%)]\tLoss: 0.545648\n",
      "Training Epoch: 1 [43136/60000 (72%)]\tLoss: 0.527898\n",
      "Training Epoch: 1 [43168/60000 (72%)]\tLoss: 0.386693\n",
      "Training Epoch: 1 [43200/60000 (72%)]\tLoss: 0.156631\n",
      "Training Epoch: 1 [43232/60000 (72%)]\tLoss: 0.836643\n",
      "Training Epoch: 1 [43264/60000 (72%)]\tLoss: 0.818065\n",
      "Training Epoch: 1 [43296/60000 (72%)]\tLoss: 0.298046\n",
      "Training Epoch: 1 [43328/60000 (72%)]\tLoss: 0.357595\n",
      "Training Epoch: 1 [43360/60000 (72%)]\tLoss: 0.486098\n",
      "Training Epoch: 1 [43392/60000 (72%)]\tLoss: 1.115649\n",
      "Training Epoch: 1 [43424/60000 (72%)]\tLoss: 0.325475\n",
      "Training Epoch: 1 [43456/60000 (72%)]\tLoss: 0.733991\n",
      "Training Epoch: 1 [43488/60000 (72%)]\tLoss: 0.404159\n",
      "Training Epoch: 1 [43520/60000 (73%)]\tLoss: 0.679830\n",
      "Training Epoch: 1 [43552/60000 (73%)]\tLoss: 0.315150\n",
      "Training Epoch: 1 [43584/60000 (73%)]\tLoss: 0.756878\n",
      "Training Epoch: 1 [43616/60000 (73%)]\tLoss: 0.694123\n",
      "Training Epoch: 1 [43648/60000 (73%)]\tLoss: 1.173110\n",
      "Training Epoch: 1 [43680/60000 (73%)]\tLoss: 1.087826\n",
      "Training Epoch: 1 [43712/60000 (73%)]\tLoss: 0.709619\n",
      "Training Epoch: 1 [43744/60000 (73%)]\tLoss: 0.365276\n",
      "Training Epoch: 1 [43776/60000 (73%)]\tLoss: 0.879967\n",
      "Training Epoch: 1 [43808/60000 (73%)]\tLoss: 0.557123\n",
      "Training Epoch: 1 [43840/60000 (73%)]\tLoss: 0.756674\n",
      "Training Epoch: 1 [43872/60000 (73%)]\tLoss: 0.824625\n",
      "Training Epoch: 1 [43904/60000 (73%)]\tLoss: 0.955277\n",
      "Training Epoch: 1 [43936/60000 (73%)]\tLoss: 0.637585\n",
      "Training Epoch: 1 [43968/60000 (73%)]\tLoss: 0.882676\n",
      "Training Epoch: 1 [44000/60000 (73%)]\tLoss: 0.851878\n",
      "Training Epoch: 1 [44032/60000 (73%)]\tLoss: 0.607257\n",
      "Training Epoch: 1 [44064/60000 (73%)]\tLoss: 1.170643\n",
      "Training Epoch: 1 [44096/60000 (73%)]\tLoss: 0.805250\n",
      "Training Epoch: 1 [44128/60000 (74%)]\tLoss: 0.548547\n",
      "Training Epoch: 1 [44160/60000 (74%)]\tLoss: 0.647632\n",
      "Training Epoch: 1 [44192/60000 (74%)]\tLoss: 0.746982\n",
      "Training Epoch: 1 [44224/60000 (74%)]\tLoss: 0.415600\n",
      "Training Epoch: 1 [44256/60000 (74%)]\tLoss: 0.684148\n",
      "Training Epoch: 1 [44288/60000 (74%)]\tLoss: 0.476643\n",
      "Training Epoch: 1 [44320/60000 (74%)]\tLoss: 0.954023\n",
      "Training Epoch: 1 [44352/60000 (74%)]\tLoss: 0.534994\n",
      "Training Epoch: 1 [44384/60000 (74%)]\tLoss: 0.346663\n",
      "Training Epoch: 1 [44416/60000 (74%)]\tLoss: 0.491580\n",
      "Training Epoch: 1 [44448/60000 (74%)]\tLoss: 0.625599\n",
      "Training Epoch: 1 [44480/60000 (74%)]\tLoss: 0.715809\n",
      "Training Epoch: 1 [44512/60000 (74%)]\tLoss: 0.358220\n",
      "Training Epoch: 1 [44544/60000 (74%)]\tLoss: 0.465147\n",
      "Training Epoch: 1 [44576/60000 (74%)]\tLoss: 0.813398\n",
      "Training Epoch: 1 [44608/60000 (74%)]\tLoss: 0.574709\n",
      "Training Epoch: 1 [44640/60000 (74%)]\tLoss: 0.476008\n",
      "Training Epoch: 1 [44672/60000 (74%)]\tLoss: 0.587607\n",
      "Training Epoch: 1 [44704/60000 (75%)]\tLoss: 0.745607\n",
      "Training Epoch: 1 [44736/60000 (75%)]\tLoss: 0.690127\n",
      "Training Epoch: 1 [44768/60000 (75%)]\tLoss: 0.348629\n",
      "Training Epoch: 1 [44800/60000 (75%)]\tLoss: 0.564127\n",
      "Training Epoch: 1 [44832/60000 (75%)]\tLoss: 0.646891\n",
      "Training Epoch: 1 [44864/60000 (75%)]\tLoss: 0.679549\n",
      "Training Epoch: 1 [44896/60000 (75%)]\tLoss: 0.474970\n",
      "Training Epoch: 1 [44928/60000 (75%)]\tLoss: 0.416907\n",
      "Training Epoch: 1 [44960/60000 (75%)]\tLoss: 0.485679\n",
      "Training Epoch: 1 [44992/60000 (75%)]\tLoss: 0.482135\n",
      "Training Epoch: 1 [45024/60000 (75%)]\tLoss: 0.610500\n",
      "Training Epoch: 1 [45056/60000 (75%)]\tLoss: 0.454107\n",
      "Training Epoch: 1 [45088/60000 (75%)]\tLoss: 0.298015\n",
      "Training Epoch: 1 [45120/60000 (75%)]\tLoss: 0.552420\n",
      "Training Epoch: 1 [45152/60000 (75%)]\tLoss: 0.352577\n",
      "Training Epoch: 1 [45184/60000 (75%)]\tLoss: 0.490335\n",
      "Training Epoch: 1 [45216/60000 (75%)]\tLoss: 0.564859\n",
      "Training Epoch: 1 [45248/60000 (75%)]\tLoss: 0.470885\n",
      "Training Epoch: 1 [45280/60000 (75%)]\tLoss: 0.381408\n",
      "Training Epoch: 1 [45312/60000 (76%)]\tLoss: 0.356819\n",
      "Training Epoch: 1 [45344/60000 (76%)]\tLoss: 0.452529\n",
      "Training Epoch: 1 [45376/60000 (76%)]\tLoss: 0.454840\n",
      "Training Epoch: 1 [45408/60000 (76%)]\tLoss: 1.082636\n",
      "Training Epoch: 1 [45440/60000 (76%)]\tLoss: 0.488745\n",
      "Training Epoch: 1 [45472/60000 (76%)]\tLoss: 0.517584\n",
      "Training Epoch: 1 [45504/60000 (76%)]\tLoss: 0.385555\n",
      "Training Epoch: 1 [45536/60000 (76%)]\tLoss: 0.375272\n",
      "Training Epoch: 1 [45568/60000 (76%)]\tLoss: 0.977122\n",
      "Training Epoch: 1 [45600/60000 (76%)]\tLoss: 0.537684\n",
      "Training Epoch: 1 [45632/60000 (76%)]\tLoss: 0.443931\n",
      "Training Epoch: 1 [45664/60000 (76%)]\tLoss: 0.480155\n",
      "Training Epoch: 1 [45696/60000 (76%)]\tLoss: 0.443748\n",
      "Training Epoch: 1 [45728/60000 (76%)]\tLoss: 0.419347\n",
      "Training Epoch: 1 [45760/60000 (76%)]\tLoss: 0.584860\n",
      "Training Epoch: 1 [45792/60000 (76%)]\tLoss: 0.514773\n",
      "Training Epoch: 1 [45824/60000 (76%)]\tLoss: 1.101591\n",
      "Training Epoch: 1 [45856/60000 (76%)]\tLoss: 0.313450\n",
      "Training Epoch: 1 [45888/60000 (76%)]\tLoss: 0.661717\n",
      "Training Epoch: 1 [45920/60000 (77%)]\tLoss: 0.592339\n",
      "Training Epoch: 1 [45952/60000 (77%)]\tLoss: 0.375478\n",
      "Training Epoch: 1 [45984/60000 (77%)]\tLoss: 0.625136\n",
      "Training Epoch: 1 [46016/60000 (77%)]\tLoss: 0.672908\n",
      "Training Epoch: 1 [46048/60000 (77%)]\tLoss: 0.331648\n",
      "Training Epoch: 1 [46080/60000 (77%)]\tLoss: 0.561953\n",
      "Training Epoch: 1 [46112/60000 (77%)]\tLoss: 0.520080\n",
      "Training Epoch: 1 [46144/60000 (77%)]\tLoss: 0.480408\n",
      "Training Epoch: 1 [46176/60000 (77%)]\tLoss: 0.830126\n",
      "Training Epoch: 1 [46208/60000 (77%)]\tLoss: 0.741604\n",
      "Training Epoch: 1 [46240/60000 (77%)]\tLoss: 0.545695\n",
      "Training Epoch: 1 [46272/60000 (77%)]\tLoss: 0.392921\n",
      "Training Epoch: 1 [46304/60000 (77%)]\tLoss: 1.105583\n",
      "Training Epoch: 1 [46336/60000 (77%)]\tLoss: 0.517327\n",
      "Training Epoch: 1 [46368/60000 (77%)]\tLoss: 0.227426\n",
      "Training Epoch: 1 [46400/60000 (77%)]\tLoss: 0.603463\n",
      "Training Epoch: 1 [46432/60000 (77%)]\tLoss: 0.457018\n",
      "Training Epoch: 1 [46464/60000 (77%)]\tLoss: 0.635158\n",
      "Training Epoch: 1 [46496/60000 (77%)]\tLoss: 0.633085\n",
      "Training Epoch: 1 [46528/60000 (78%)]\tLoss: 0.813743\n",
      "Training Epoch: 1 [46560/60000 (78%)]\tLoss: 0.386486\n",
      "Training Epoch: 1 [46592/60000 (78%)]\tLoss: 0.536576\n",
      "Training Epoch: 1 [46624/60000 (78%)]\tLoss: 0.451150\n",
      "Training Epoch: 1 [46656/60000 (78%)]\tLoss: 1.207979\n",
      "Training Epoch: 1 [46688/60000 (78%)]\tLoss: 0.943945\n",
      "Training Epoch: 1 [46720/60000 (78%)]\tLoss: 0.557735\n",
      "Training Epoch: 1 [46752/60000 (78%)]\tLoss: 0.505882\n",
      "Training Epoch: 1 [46784/60000 (78%)]\tLoss: 0.449690\n",
      "Training Epoch: 1 [46816/60000 (78%)]\tLoss: 0.440787\n",
      "Training Epoch: 1 [46848/60000 (78%)]\tLoss: 0.441068\n",
      "Training Epoch: 1 [46880/60000 (78%)]\tLoss: 0.667793\n",
      "Training Epoch: 1 [46912/60000 (78%)]\tLoss: 0.366314\n",
      "Training Epoch: 1 [46944/60000 (78%)]\tLoss: 0.487349\n",
      "Training Epoch: 1 [46976/60000 (78%)]\tLoss: 0.328749\n",
      "Training Epoch: 1 [47008/60000 (78%)]\tLoss: 0.316307\n",
      "Training Epoch: 1 [47040/60000 (78%)]\tLoss: 0.429023\n",
      "Training Epoch: 1 [47072/60000 (78%)]\tLoss: 0.374358\n",
      "Training Epoch: 1 [47104/60000 (79%)]\tLoss: 0.811671\n",
      "Training Epoch: 1 [47136/60000 (79%)]\tLoss: 0.570965\n",
      "Training Epoch: 1 [47168/60000 (79%)]\tLoss: 0.482159\n",
      "Training Epoch: 1 [47200/60000 (79%)]\tLoss: 0.680697\n",
      "Training Epoch: 1 [47232/60000 (79%)]\tLoss: 0.438491\n",
      "Training Epoch: 1 [47264/60000 (79%)]\tLoss: 0.856410\n",
      "Training Epoch: 1 [47296/60000 (79%)]\tLoss: 0.332723\n",
      "Training Epoch: 1 [47328/60000 (79%)]\tLoss: 0.513822\n",
      "Training Epoch: 1 [47360/60000 (79%)]\tLoss: 0.722976\n",
      "Training Epoch: 1 [47392/60000 (79%)]\tLoss: 0.353900\n",
      "Training Epoch: 1 [47424/60000 (79%)]\tLoss: 0.609692\n",
      "Training Epoch: 1 [47456/60000 (79%)]\tLoss: 0.744095\n",
      "Training Epoch: 1 [47488/60000 (79%)]\tLoss: 0.609880\n",
      "Training Epoch: 1 [47520/60000 (79%)]\tLoss: 0.714865\n",
      "Training Epoch: 1 [47552/60000 (79%)]\tLoss: 0.588170\n",
      "Training Epoch: 1 [47584/60000 (79%)]\tLoss: 0.735503\n",
      "Training Epoch: 1 [47616/60000 (79%)]\tLoss: 0.498653\n",
      "Training Epoch: 1 [47648/60000 (79%)]\tLoss: 0.838077\n",
      "Training Epoch: 1 [47680/60000 (79%)]\tLoss: 0.348008\n",
      "Training Epoch: 1 [47712/60000 (80%)]\tLoss: 0.465020\n",
      "Training Epoch: 1 [47744/60000 (80%)]\tLoss: 0.773504\n",
      "Training Epoch: 1 [47776/60000 (80%)]\tLoss: 0.712850\n",
      "Training Epoch: 1 [47808/60000 (80%)]\tLoss: 0.324840\n",
      "Training Epoch: 1 [47840/60000 (80%)]\tLoss: 0.795713\n",
      "Training Epoch: 1 [47872/60000 (80%)]\tLoss: 0.674273\n",
      "Training Epoch: 1 [47904/60000 (80%)]\tLoss: 0.716452\n",
      "Training Epoch: 1 [47936/60000 (80%)]\tLoss: 0.590239\n",
      "Training Epoch: 1 [47968/60000 (80%)]\tLoss: 0.776853\n",
      "Training Epoch: 1 [48000/60000 (80%)]\tLoss: 0.516520\n",
      "Training Epoch: 1 [48032/60000 (80%)]\tLoss: 0.630329\n",
      "Training Epoch: 1 [48064/60000 (80%)]\tLoss: 0.409264\n",
      "Training Epoch: 1 [48096/60000 (80%)]\tLoss: 1.059284\n",
      "Training Epoch: 1 [48128/60000 (80%)]\tLoss: 0.547359\n",
      "Training Epoch: 1 [48160/60000 (80%)]\tLoss: 0.491021\n",
      "Training Epoch: 1 [48192/60000 (80%)]\tLoss: 0.834184\n",
      "Training Epoch: 1 [48224/60000 (80%)]\tLoss: 0.374157\n",
      "Training Epoch: 1 [48256/60000 (80%)]\tLoss: 0.598435\n",
      "Training Epoch: 1 [48288/60000 (80%)]\tLoss: 0.621884\n",
      "Training Epoch: 1 [48320/60000 (81%)]\tLoss: 0.414050\n",
      "Training Epoch: 1 [48352/60000 (81%)]\tLoss: 0.413239\n",
      "Training Epoch: 1 [48384/60000 (81%)]\tLoss: 0.879316\n",
      "Training Epoch: 1 [48416/60000 (81%)]\tLoss: 0.400705\n",
      "Training Epoch: 1 [48448/60000 (81%)]\tLoss: 0.648395\n",
      "Training Epoch: 1 [48480/60000 (81%)]\tLoss: 0.315418\n",
      "Training Epoch: 1 [48512/60000 (81%)]\tLoss: 0.678849\n",
      "Training Epoch: 1 [48544/60000 (81%)]\tLoss: 0.633385\n",
      "Training Epoch: 1 [48576/60000 (81%)]\tLoss: 0.515547\n",
      "Training Epoch: 1 [48608/60000 (81%)]\tLoss: 0.616494\n",
      "Training Epoch: 1 [48640/60000 (81%)]\tLoss: 0.201195\n",
      "Training Epoch: 1 [48672/60000 (81%)]\tLoss: 0.667038\n",
      "Training Epoch: 1 [48704/60000 (81%)]\tLoss: 0.208591\n",
      "Training Epoch: 1 [48736/60000 (81%)]\tLoss: 0.552714\n",
      "Training Epoch: 1 [48768/60000 (81%)]\tLoss: 0.656899\n",
      "Training Epoch: 1 [48800/60000 (81%)]\tLoss: 0.455232\n",
      "Training Epoch: 1 [48832/60000 (81%)]\tLoss: 0.495717\n",
      "Training Epoch: 1 [48864/60000 (81%)]\tLoss: 0.234227\n",
      "Training Epoch: 1 [48896/60000 (81%)]\tLoss: 0.337466\n",
      "Training Epoch: 1 [48928/60000 (82%)]\tLoss: 0.673330\n",
      "Training Epoch: 1 [48960/60000 (82%)]\tLoss: 0.425690\n",
      "Training Epoch: 1 [48992/60000 (82%)]\tLoss: 1.064020\n",
      "Training Epoch: 1 [49024/60000 (82%)]\tLoss: 0.440825\n",
      "Training Epoch: 1 [49056/60000 (82%)]\tLoss: 0.810406\n",
      "Training Epoch: 1 [49088/60000 (82%)]\tLoss: 0.364222\n",
      "Training Epoch: 1 [49120/60000 (82%)]\tLoss: 0.584874\n",
      "Training Epoch: 1 [49152/60000 (82%)]\tLoss: 0.195605\n",
      "Training Epoch: 1 [49184/60000 (82%)]\tLoss: 0.623549\n",
      "Training Epoch: 1 [49216/60000 (82%)]\tLoss: 0.679339\n",
      "Training Epoch: 1 [49248/60000 (82%)]\tLoss: 0.560976\n",
      "Training Epoch: 1 [49280/60000 (82%)]\tLoss: 0.199670\n",
      "Training Epoch: 1 [49312/60000 (82%)]\tLoss: 0.580647\n",
      "Training Epoch: 1 [49344/60000 (82%)]\tLoss: 0.506157\n",
      "Training Epoch: 1 [49376/60000 (82%)]\tLoss: 0.555966\n",
      "Training Epoch: 1 [49408/60000 (82%)]\tLoss: 0.415355\n",
      "Training Epoch: 1 [49440/60000 (82%)]\tLoss: 0.264618\n",
      "Training Epoch: 1 [49472/60000 (82%)]\tLoss: 0.911605\n",
      "Training Epoch: 1 [49504/60000 (83%)]\tLoss: 0.565000\n",
      "Training Epoch: 1 [49536/60000 (83%)]\tLoss: 0.432915\n",
      "Training Epoch: 1 [49568/60000 (83%)]\tLoss: 0.645278\n",
      "Training Epoch: 1 [49600/60000 (83%)]\tLoss: 0.269662\n",
      "Training Epoch: 1 [49632/60000 (83%)]\tLoss: 0.770504\n",
      "Training Epoch: 1 [49664/60000 (83%)]\tLoss: 0.764728\n",
      "Training Epoch: 1 [49696/60000 (83%)]\tLoss: 0.568030\n",
      "Training Epoch: 1 [49728/60000 (83%)]\tLoss: 1.575030\n",
      "Training Epoch: 1 [49760/60000 (83%)]\tLoss: 0.390136\n",
      "Training Epoch: 1 [49792/60000 (83%)]\tLoss: 0.711809\n",
      "Training Epoch: 1 [49824/60000 (83%)]\tLoss: 0.251292\n",
      "Training Epoch: 1 [49856/60000 (83%)]\tLoss: 0.601255\n",
      "Training Epoch: 1 [49888/60000 (83%)]\tLoss: 0.311852\n",
      "Training Epoch: 1 [49920/60000 (83%)]\tLoss: 0.406118\n",
      "Training Epoch: 1 [49952/60000 (83%)]\tLoss: 0.430183\n",
      "Training Epoch: 1 [49984/60000 (83%)]\tLoss: 0.733223\n",
      "Training Epoch: 1 [50016/60000 (83%)]\tLoss: 0.558776\n",
      "Training Epoch: 1 [50048/60000 (83%)]\tLoss: 0.624318\n",
      "Training Epoch: 1 [50080/60000 (83%)]\tLoss: 0.635916\n",
      "Training Epoch: 1 [50112/60000 (84%)]\tLoss: 0.378891\n",
      "Training Epoch: 1 [50144/60000 (84%)]\tLoss: 0.897957\n",
      "Training Epoch: 1 [50176/60000 (84%)]\tLoss: 0.801406\n",
      "Training Epoch: 1 [50208/60000 (84%)]\tLoss: 0.430330\n",
      "Training Epoch: 1 [50240/60000 (84%)]\tLoss: 0.373153\n",
      "Training Epoch: 1 [50272/60000 (84%)]\tLoss: 0.480415\n",
      "Training Epoch: 1 [50304/60000 (84%)]\tLoss: 0.899422\n",
      "Training Epoch: 1 [50336/60000 (84%)]\tLoss: 0.326253\n",
      "Training Epoch: 1 [50368/60000 (84%)]\tLoss: 0.987320\n",
      "Training Epoch: 1 [50400/60000 (84%)]\tLoss: 0.599908\n",
      "Training Epoch: 1 [50432/60000 (84%)]\tLoss: 0.546610\n",
      "Training Epoch: 1 [50464/60000 (84%)]\tLoss: 0.913984\n",
      "Training Epoch: 1 [50496/60000 (84%)]\tLoss: 0.514355\n",
      "Training Epoch: 1 [50528/60000 (84%)]\tLoss: 1.224737\n",
      "Training Epoch: 1 [50560/60000 (84%)]\tLoss: 0.480634\n",
      "Training Epoch: 1 [50592/60000 (84%)]\tLoss: 0.235277\n",
      "Training Epoch: 1 [50624/60000 (84%)]\tLoss: 0.929865\n",
      "Training Epoch: 1 [50656/60000 (84%)]\tLoss: 0.532626\n",
      "Training Epoch: 1 [50688/60000 (84%)]\tLoss: 0.257256\n",
      "Training Epoch: 1 [50720/60000 (85%)]\tLoss: 0.417318\n",
      "Training Epoch: 1 [50752/60000 (85%)]\tLoss: 0.381458\n",
      "Training Epoch: 1 [50784/60000 (85%)]\tLoss: 0.675673\n",
      "Training Epoch: 1 [50816/60000 (85%)]\tLoss: 0.263046\n",
      "Training Epoch: 1 [50848/60000 (85%)]\tLoss: 0.619577\n",
      "Training Epoch: 1 [50880/60000 (85%)]\tLoss: 0.292394\n",
      "Training Epoch: 1 [50912/60000 (85%)]\tLoss: 0.568444\n",
      "Training Epoch: 1 [50944/60000 (85%)]\tLoss: 0.597831\n",
      "Training Epoch: 1 [50976/60000 (85%)]\tLoss: 0.428139\n",
      "Training Epoch: 1 [51008/60000 (85%)]\tLoss: 0.240320\n",
      "Training Epoch: 1 [51040/60000 (85%)]\tLoss: 0.801016\n",
      "Training Epoch: 1 [51072/60000 (85%)]\tLoss: 0.487006\n",
      "Training Epoch: 1 [51104/60000 (85%)]\tLoss: 0.929509\n",
      "Training Epoch: 1 [51136/60000 (85%)]\tLoss: 0.405057\n",
      "Training Epoch: 1 [51168/60000 (85%)]\tLoss: 0.452892\n",
      "Training Epoch: 1 [51200/60000 (85%)]\tLoss: 0.597271\n",
      "Training Epoch: 1 [51232/60000 (85%)]\tLoss: 0.250293\n",
      "Training Epoch: 1 [51264/60000 (85%)]\tLoss: 0.617528\n",
      "Training Epoch: 1 [51296/60000 (85%)]\tLoss: 0.270054\n",
      "Training Epoch: 1 [51328/60000 (86%)]\tLoss: 0.484511\n",
      "Training Epoch: 1 [51360/60000 (86%)]\tLoss: 0.436553\n",
      "Training Epoch: 1 [51392/60000 (86%)]\tLoss: 0.554476\n",
      "Training Epoch: 1 [51424/60000 (86%)]\tLoss: 0.419643\n",
      "Training Epoch: 1 [51456/60000 (86%)]\tLoss: 0.648992\n",
      "Training Epoch: 1 [51488/60000 (86%)]\tLoss: 0.466756\n",
      "Training Epoch: 1 [51520/60000 (86%)]\tLoss: 0.919411\n",
      "Training Epoch: 1 [51552/60000 (86%)]\tLoss: 0.798513\n",
      "Training Epoch: 1 [51584/60000 (86%)]\tLoss: 0.419053\n",
      "Training Epoch: 1 [51616/60000 (86%)]\tLoss: 0.522628\n",
      "Training Epoch: 1 [51648/60000 (86%)]\tLoss: 0.444803\n",
      "Training Epoch: 1 [51680/60000 (86%)]\tLoss: 0.636864\n",
      "Training Epoch: 1 [51712/60000 (86%)]\tLoss: 0.565032\n",
      "Training Epoch: 1 [51744/60000 (86%)]\tLoss: 0.709138\n",
      "Training Epoch: 1 [51776/60000 (86%)]\tLoss: 0.423416\n",
      "Training Epoch: 1 [51808/60000 (86%)]\tLoss: 0.542373\n",
      "Training Epoch: 1 [51840/60000 (86%)]\tLoss: 0.444485\n",
      "Training Epoch: 1 [51872/60000 (86%)]\tLoss: 0.585131\n",
      "Training Epoch: 1 [51904/60000 (87%)]\tLoss: 0.892362\n",
      "Training Epoch: 1 [51936/60000 (87%)]\tLoss: 0.528971\n",
      "Training Epoch: 1 [51968/60000 (87%)]\tLoss: 0.978670\n",
      "Training Epoch: 1 [52000/60000 (87%)]\tLoss: 0.650179\n",
      "Training Epoch: 1 [52032/60000 (87%)]\tLoss: 0.479835\n",
      "Training Epoch: 1 [52064/60000 (87%)]\tLoss: 1.218279\n",
      "Training Epoch: 1 [52096/60000 (87%)]\tLoss: 0.414791\n",
      "Training Epoch: 1 [52128/60000 (87%)]\tLoss: 0.479811\n",
      "Training Epoch: 1 [52160/60000 (87%)]\tLoss: 0.695076\n",
      "Training Epoch: 1 [52192/60000 (87%)]\tLoss: 0.314943\n",
      "Training Epoch: 1 [52224/60000 (87%)]\tLoss: 0.769045\n",
      "Training Epoch: 1 [52256/60000 (87%)]\tLoss: 0.302644\n",
      "Training Epoch: 1 [52288/60000 (87%)]\tLoss: 0.645464\n",
      "Training Epoch: 1 [52320/60000 (87%)]\tLoss: 0.441191\n",
      "Training Epoch: 1 [52352/60000 (87%)]\tLoss: 0.589368\n",
      "Training Epoch: 1 [52384/60000 (87%)]\tLoss: 0.974667\n",
      "Training Epoch: 1 [52416/60000 (87%)]\tLoss: 0.565803\n",
      "Training Epoch: 1 [52448/60000 (87%)]\tLoss: 0.661284\n",
      "Training Epoch: 1 [52480/60000 (87%)]\tLoss: 0.601834\n",
      "Training Epoch: 1 [52512/60000 (88%)]\tLoss: 0.454879\n",
      "Training Epoch: 1 [52544/60000 (88%)]\tLoss: 0.609199\n",
      "Training Epoch: 1 [52576/60000 (88%)]\tLoss: 0.413330\n",
      "Training Epoch: 1 [52608/60000 (88%)]\tLoss: 0.326438\n",
      "Training Epoch: 1 [52640/60000 (88%)]\tLoss: 0.644423\n",
      "Training Epoch: 1 [52672/60000 (88%)]\tLoss: 0.856221\n",
      "Training Epoch: 1 [52704/60000 (88%)]\tLoss: 0.370232\n",
      "Training Epoch: 1 [52736/60000 (88%)]\tLoss: 0.622679\n",
      "Training Epoch: 1 [52768/60000 (88%)]\tLoss: 0.708024\n",
      "Training Epoch: 1 [52800/60000 (88%)]\tLoss: 0.335542\n",
      "Training Epoch: 1 [52832/60000 (88%)]\tLoss: 0.208509\n",
      "Training Epoch: 1 [52864/60000 (88%)]\tLoss: 0.897393\n",
      "Training Epoch: 1 [52896/60000 (88%)]\tLoss: 0.421076\n",
      "Training Epoch: 1 [52928/60000 (88%)]\tLoss: 0.378706\n",
      "Training Epoch: 1 [52960/60000 (88%)]\tLoss: 0.502091\n",
      "Training Epoch: 1 [52992/60000 (88%)]\tLoss: 0.392840\n",
      "Training Epoch: 1 [53024/60000 (88%)]\tLoss: 0.858129\n",
      "Training Epoch: 1 [53056/60000 (88%)]\tLoss: 0.447922\n",
      "Training Epoch: 1 [53088/60000 (88%)]\tLoss: 0.742312\n",
      "Training Epoch: 1 [53120/60000 (89%)]\tLoss: 0.187275\n",
      "Training Epoch: 1 [53152/60000 (89%)]\tLoss: 0.448329\n",
      "Training Epoch: 1 [53184/60000 (89%)]\tLoss: 0.751291\n",
      "Training Epoch: 1 [53216/60000 (89%)]\tLoss: 0.697065\n",
      "Training Epoch: 1 [53248/60000 (89%)]\tLoss: 0.747368\n",
      "Training Epoch: 1 [53280/60000 (89%)]\tLoss: 0.644411\n",
      "Training Epoch: 1 [53312/60000 (89%)]\tLoss: 0.592005\n",
      "Training Epoch: 1 [53344/60000 (89%)]\tLoss: 0.442569\n",
      "Training Epoch: 1 [53376/60000 (89%)]\tLoss: 0.473698\n",
      "Training Epoch: 1 [53408/60000 (89%)]\tLoss: 0.452975\n",
      "Training Epoch: 1 [53440/60000 (89%)]\tLoss: 0.651881\n",
      "Training Epoch: 1 [53472/60000 (89%)]\tLoss: 0.472636\n",
      "Training Epoch: 1 [53504/60000 (89%)]\tLoss: 0.906137\n",
      "Training Epoch: 1 [53536/60000 (89%)]\tLoss: 0.401796\n",
      "Training Epoch: 1 [53568/60000 (89%)]\tLoss: 0.704242\n",
      "Training Epoch: 1 [53600/60000 (89%)]\tLoss: 0.701978\n",
      "Training Epoch: 1 [53632/60000 (89%)]\tLoss: 0.666515\n",
      "Training Epoch: 1 [53664/60000 (89%)]\tLoss: 0.653587\n",
      "Training Epoch: 1 [53696/60000 (89%)]\tLoss: 0.264309\n",
      "Training Epoch: 1 [53728/60000 (90%)]\tLoss: 0.761505\n",
      "Training Epoch: 1 [53760/60000 (90%)]\tLoss: 0.679427\n",
      "Training Epoch: 1 [53792/60000 (90%)]\tLoss: 0.511563\n",
      "Training Epoch: 1 [53824/60000 (90%)]\tLoss: 0.837821\n",
      "Training Epoch: 1 [53856/60000 (90%)]\tLoss: 0.645997\n",
      "Training Epoch: 1 [53888/60000 (90%)]\tLoss: 0.458640\n",
      "Training Epoch: 1 [53920/60000 (90%)]\tLoss: 0.459681\n",
      "Training Epoch: 1 [53952/60000 (90%)]\tLoss: 0.653066\n",
      "Training Epoch: 1 [53984/60000 (90%)]\tLoss: 0.525628\n",
      "Training Epoch: 1 [54016/60000 (90%)]\tLoss: 0.398849\n",
      "Training Epoch: 1 [54048/60000 (90%)]\tLoss: 0.467858\n",
      "Training Epoch: 1 [54080/60000 (90%)]\tLoss: 0.556308\n",
      "Training Epoch: 1 [54112/60000 (90%)]\tLoss: 0.670233\n",
      "Training Epoch: 1 [54144/60000 (90%)]\tLoss: 0.827082\n",
      "Training Epoch: 1 [54176/60000 (90%)]\tLoss: 0.709481\n",
      "Training Epoch: 1 [54208/60000 (90%)]\tLoss: 0.373691\n",
      "Training Epoch: 1 [54240/60000 (90%)]\tLoss: 0.514660\n",
      "Training Epoch: 1 [54272/60000 (90%)]\tLoss: 0.411851\n",
      "Training Epoch: 1 [54304/60000 (91%)]\tLoss: 0.718914\n",
      "Training Epoch: 1 [54336/60000 (91%)]\tLoss: 0.598181\n",
      "Training Epoch: 1 [54368/60000 (91%)]\tLoss: 0.501523\n",
      "Training Epoch: 1 [54400/60000 (91%)]\tLoss: 0.863227\n",
      "Training Epoch: 1 [54432/60000 (91%)]\tLoss: 0.816413\n",
      "Training Epoch: 1 [54464/60000 (91%)]\tLoss: 0.670291\n",
      "Training Epoch: 1 [54496/60000 (91%)]\tLoss: 0.848388\n",
      "Training Epoch: 1 [54528/60000 (91%)]\tLoss: 0.866330\n",
      "Training Epoch: 1 [54560/60000 (91%)]\tLoss: 0.667171\n",
      "Training Epoch: 1 [54592/60000 (91%)]\tLoss: 0.549177\n",
      "Training Epoch: 1 [54624/60000 (91%)]\tLoss: 0.461777\n",
      "Training Epoch: 1 [54656/60000 (91%)]\tLoss: 0.725221\n",
      "Training Epoch: 1 [54688/60000 (91%)]\tLoss: 0.727727\n",
      "Training Epoch: 1 [54720/60000 (91%)]\tLoss: 0.962282\n",
      "Training Epoch: 1 [54752/60000 (91%)]\tLoss: 0.726481\n",
      "Training Epoch: 1 [54784/60000 (91%)]\tLoss: 0.883518\n",
      "Training Epoch: 1 [54816/60000 (91%)]\tLoss: 0.741210\n",
      "Training Epoch: 1 [54848/60000 (91%)]\tLoss: 0.518552\n",
      "Training Epoch: 1 [54880/60000 (91%)]\tLoss: 0.460775\n",
      "Training Epoch: 1 [54912/60000 (92%)]\tLoss: 0.358101\n",
      "Training Epoch: 1 [54944/60000 (92%)]\tLoss: 0.941598\n",
      "Training Epoch: 1 [54976/60000 (92%)]\tLoss: 0.453828\n",
      "Training Epoch: 1 [55008/60000 (92%)]\tLoss: 0.628607\n",
      "Training Epoch: 1 [55040/60000 (92%)]\tLoss: 0.260001\n",
      "Training Epoch: 1 [55072/60000 (92%)]\tLoss: 0.955634\n",
      "Training Epoch: 1 [55104/60000 (92%)]\tLoss: 0.528899\n",
      "Training Epoch: 1 [55136/60000 (92%)]\tLoss: 0.672388\n",
      "Training Epoch: 1 [55168/60000 (92%)]\tLoss: 0.625174\n",
      "Training Epoch: 1 [55200/60000 (92%)]\tLoss: 0.461663\n",
      "Training Epoch: 1 [55232/60000 (92%)]\tLoss: 0.604666\n",
      "Training Epoch: 1 [55264/60000 (92%)]\tLoss: 0.414991\n",
      "Training Epoch: 1 [55296/60000 (92%)]\tLoss: 0.633983\n",
      "Training Epoch: 1 [55328/60000 (92%)]\tLoss: 0.557369\n",
      "Training Epoch: 1 [55360/60000 (92%)]\tLoss: 0.216125\n",
      "Training Epoch: 1 [55392/60000 (92%)]\tLoss: 0.701541\n",
      "Training Epoch: 1 [55424/60000 (92%)]\tLoss: 0.803518\n",
      "Training Epoch: 1 [55456/60000 (92%)]\tLoss: 0.561361\n",
      "Training Epoch: 1 [55488/60000 (92%)]\tLoss: 0.255201\n",
      "Training Epoch: 1 [55520/60000 (93%)]\tLoss: 0.401458\n",
      "Training Epoch: 1 [55552/60000 (93%)]\tLoss: 0.586687\n",
      "Training Epoch: 1 [55584/60000 (93%)]\tLoss: 0.289514\n",
      "Training Epoch: 1 [55616/60000 (93%)]\tLoss: 0.629298\n",
      "Training Epoch: 1 [55648/60000 (93%)]\tLoss: 0.289359\n",
      "Training Epoch: 1 [55680/60000 (93%)]\tLoss: 0.450646\n",
      "Training Epoch: 1 [55712/60000 (93%)]\tLoss: 0.391707\n",
      "Training Epoch: 1 [55744/60000 (93%)]\tLoss: 0.477005\n",
      "Training Epoch: 1 [55776/60000 (93%)]\tLoss: 0.838567\n",
      "Training Epoch: 1 [55808/60000 (93%)]\tLoss: 0.407118\n",
      "Training Epoch: 1 [55840/60000 (93%)]\tLoss: 0.484444\n",
      "Training Epoch: 1 [55872/60000 (93%)]\tLoss: 0.339465\n",
      "Training Epoch: 1 [55904/60000 (93%)]\tLoss: 0.674770\n",
      "Training Epoch: 1 [55936/60000 (93%)]\tLoss: 0.573125\n",
      "Training Epoch: 1 [55968/60000 (93%)]\tLoss: 0.474769\n",
      "Training Epoch: 1 [56000/60000 (93%)]\tLoss: 0.565416\n",
      "Training Epoch: 1 [56032/60000 (93%)]\tLoss: 0.750897\n",
      "Training Epoch: 1 [56064/60000 (93%)]\tLoss: 0.709412\n",
      "Training Epoch: 1 [56096/60000 (93%)]\tLoss: 0.506074\n",
      "Training Epoch: 1 [56128/60000 (94%)]\tLoss: 0.591553\n",
      "Training Epoch: 1 [56160/60000 (94%)]\tLoss: 0.424962\n",
      "Training Epoch: 1 [56192/60000 (94%)]\tLoss: 0.915371\n",
      "Training Epoch: 1 [56224/60000 (94%)]\tLoss: 0.324329\n",
      "Training Epoch: 1 [56256/60000 (94%)]\tLoss: 0.445510\n",
      "Training Epoch: 1 [56288/60000 (94%)]\tLoss: 0.748343\n",
      "Training Epoch: 1 [56320/60000 (94%)]\tLoss: 0.310464\n",
      "Training Epoch: 1 [56352/60000 (94%)]\tLoss: 0.626617\n",
      "Training Epoch: 1 [56384/60000 (94%)]\tLoss: 0.352016\n",
      "Training Epoch: 1 [56416/60000 (94%)]\tLoss: 0.418067\n",
      "Training Epoch: 1 [56448/60000 (94%)]\tLoss: 0.899867\n",
      "Training Epoch: 1 [56480/60000 (94%)]\tLoss: 0.926220\n",
      "Training Epoch: 1 [56512/60000 (94%)]\tLoss: 0.371706\n",
      "Training Epoch: 1 [56544/60000 (94%)]\tLoss: 0.257318\n",
      "Training Epoch: 1 [56576/60000 (94%)]\tLoss: 0.780002\n",
      "Training Epoch: 1 [56608/60000 (94%)]\tLoss: 0.437050\n",
      "Training Epoch: 1 [56640/60000 (94%)]\tLoss: 0.329258\n",
      "Training Epoch: 1 [56672/60000 (94%)]\tLoss: 0.275929\n",
      "Training Epoch: 1 [56704/60000 (95%)]\tLoss: 0.366132\n",
      "Training Epoch: 1 [56736/60000 (95%)]\tLoss: 0.574546\n",
      "Training Epoch: 1 [56768/60000 (95%)]\tLoss: 0.310704\n",
      "Training Epoch: 1 [56800/60000 (95%)]\tLoss: 0.837020\n",
      "Training Epoch: 1 [56832/60000 (95%)]\tLoss: 0.284642\n",
      "Training Epoch: 1 [56864/60000 (95%)]\tLoss: 0.693688\n",
      "Training Epoch: 1 [56896/60000 (95%)]\tLoss: 0.273461\n",
      "Training Epoch: 1 [56928/60000 (95%)]\tLoss: 0.304729\n",
      "Training Epoch: 1 [56960/60000 (95%)]\tLoss: 0.651261\n",
      "Training Epoch: 1 [56992/60000 (95%)]\tLoss: 0.259326\n",
      "Training Epoch: 1 [57024/60000 (95%)]\tLoss: 0.482768\n",
      "Training Epoch: 1 [57056/60000 (95%)]\tLoss: 0.882267\n",
      "Training Epoch: 1 [57088/60000 (95%)]\tLoss: 0.248222\n",
      "Training Epoch: 1 [57120/60000 (95%)]\tLoss: 0.693198\n",
      "Training Epoch: 1 [57152/60000 (95%)]\tLoss: 0.315034\n",
      "Training Epoch: 1 [57184/60000 (95%)]\tLoss: 0.628082\n",
      "Training Epoch: 1 [57216/60000 (95%)]\tLoss: 0.737540\n",
      "Training Epoch: 1 [57248/60000 (95%)]\tLoss: 0.243404\n",
      "Training Epoch: 1 [57280/60000 (95%)]\tLoss: 0.353078\n",
      "Training Epoch: 1 [57312/60000 (96%)]\tLoss: 0.424029\n",
      "Training Epoch: 1 [57344/60000 (96%)]\tLoss: 0.489633\n",
      "Training Epoch: 1 [57376/60000 (96%)]\tLoss: 0.577562\n",
      "Training Epoch: 1 [57408/60000 (96%)]\tLoss: 0.467448\n",
      "Training Epoch: 1 [57440/60000 (96%)]\tLoss: 0.762791\n",
      "Training Epoch: 1 [57472/60000 (96%)]\tLoss: 0.492322\n",
      "Training Epoch: 1 [57504/60000 (96%)]\tLoss: 0.679586\n",
      "Training Epoch: 1 [57536/60000 (96%)]\tLoss: 0.372987\n",
      "Training Epoch: 1 [57568/60000 (96%)]\tLoss: 0.290703\n",
      "Training Epoch: 1 [57600/60000 (96%)]\tLoss: 0.600783\n",
      "Training Epoch: 1 [57632/60000 (96%)]\tLoss: 0.257726\n",
      "Training Epoch: 1 [57664/60000 (96%)]\tLoss: 0.865133\n",
      "Training Epoch: 1 [57696/60000 (96%)]\tLoss: 0.539660\n",
      "Training Epoch: 1 [57728/60000 (96%)]\tLoss: 0.575512\n",
      "Training Epoch: 1 [57760/60000 (96%)]\tLoss: 0.452223\n",
      "Training Epoch: 1 [57792/60000 (96%)]\tLoss: 0.538107\n",
      "Training Epoch: 1 [57824/60000 (96%)]\tLoss: 0.657647\n",
      "Training Epoch: 1 [57856/60000 (96%)]\tLoss: 0.472955\n",
      "Training Epoch: 1 [57888/60000 (96%)]\tLoss: 0.432464\n",
      "Training Epoch: 1 [57920/60000 (97%)]\tLoss: 0.785081\n",
      "Training Epoch: 1 [57952/60000 (97%)]\tLoss: 0.658420\n",
      "Training Epoch: 1 [57984/60000 (97%)]\tLoss: 0.586351\n",
      "Training Epoch: 1 [58016/60000 (97%)]\tLoss: 0.893937\n",
      "Training Epoch: 1 [58048/60000 (97%)]\tLoss: 0.583848\n",
      "Training Epoch: 1 [58080/60000 (97%)]\tLoss: 0.555609\n",
      "Training Epoch: 1 [58112/60000 (97%)]\tLoss: 0.724763\n",
      "Training Epoch: 1 [58144/60000 (97%)]\tLoss: 0.646448\n",
      "Training Epoch: 1 [58176/60000 (97%)]\tLoss: 0.425207\n",
      "Training Epoch: 1 [58208/60000 (97%)]\tLoss: 0.529620\n",
      "Training Epoch: 1 [58240/60000 (97%)]\tLoss: 0.479580\n",
      "Training Epoch: 1 [58272/60000 (97%)]\tLoss: 0.504717\n",
      "Training Epoch: 1 [58304/60000 (97%)]\tLoss: 0.539447\n",
      "Training Epoch: 1 [58336/60000 (97%)]\tLoss: 0.669252\n",
      "Training Epoch: 1 [58368/60000 (97%)]\tLoss: 0.841149\n",
      "Training Epoch: 1 [58400/60000 (97%)]\tLoss: 0.595055\n",
      "Training Epoch: 1 [58432/60000 (97%)]\tLoss: 0.431227\n",
      "Training Epoch: 1 [58464/60000 (97%)]\tLoss: 0.310877\n",
      "Training Epoch: 1 [58496/60000 (97%)]\tLoss: 0.523870\n",
      "Training Epoch: 1 [58528/60000 (98%)]\tLoss: 0.374014\n",
      "Training Epoch: 1 [58560/60000 (98%)]\tLoss: 0.594582\n",
      "Training Epoch: 1 [58592/60000 (98%)]\tLoss: 0.466862\n",
      "Training Epoch: 1 [58624/60000 (98%)]\tLoss: 0.637153\n",
      "Training Epoch: 1 [58656/60000 (98%)]\tLoss: 0.565788\n",
      "Training Epoch: 1 [58688/60000 (98%)]\tLoss: 0.673840\n",
      "Training Epoch: 1 [58720/60000 (98%)]\tLoss: 1.071053\n",
      "Training Epoch: 1 [58752/60000 (98%)]\tLoss: 0.322223\n",
      "Training Epoch: 1 [58784/60000 (98%)]\tLoss: 0.608902\n",
      "Training Epoch: 1 [58816/60000 (98%)]\tLoss: 0.446432\n",
      "Training Epoch: 1 [58848/60000 (98%)]\tLoss: 0.460533\n",
      "Training Epoch: 1 [58880/60000 (98%)]\tLoss: 0.629461\n",
      "Training Epoch: 1 [58912/60000 (98%)]\tLoss: 0.525434\n",
      "Training Epoch: 1 [58944/60000 (98%)]\tLoss: 0.453442\n",
      "Training Epoch: 1 [58976/60000 (98%)]\tLoss: 0.685584\n",
      "Training Epoch: 1 [59008/60000 (98%)]\tLoss: 0.480804\n",
      "Training Epoch: 1 [59040/60000 (98%)]\tLoss: 0.393501\n",
      "Training Epoch: 1 [59072/60000 (98%)]\tLoss: 0.621244\n",
      "Training Epoch: 1 [59104/60000 (99%)]\tLoss: 0.239457\n",
      "Training Epoch: 1 [59136/60000 (99%)]\tLoss: 0.572156\n",
      "Training Epoch: 1 [59168/60000 (99%)]\tLoss: 0.318689\n",
      "Training Epoch: 1 [59200/60000 (99%)]\tLoss: 0.370782\n",
      "Training Epoch: 1 [59232/60000 (99%)]\tLoss: 0.406562\n",
      "Training Epoch: 1 [59264/60000 (99%)]\tLoss: 0.448643\n",
      "Training Epoch: 1 [59296/60000 (99%)]\tLoss: 0.429222\n",
      "Training Epoch: 1 [59328/60000 (99%)]\tLoss: 0.387141\n",
      "Training Epoch: 1 [59360/60000 (99%)]\tLoss: 0.663151\n",
      "Training Epoch: 1 [59392/60000 (99%)]\tLoss: 0.810762\n",
      "Training Epoch: 1 [59424/60000 (99%)]\tLoss: 0.520872\n",
      "Training Epoch: 1 [59456/60000 (99%)]\tLoss: 0.375664\n",
      "Training Epoch: 1 [59488/60000 (99%)]\tLoss: 0.458353\n",
      "Training Epoch: 1 [59520/60000 (99%)]\tLoss: 0.841376\n",
      "Training Epoch: 1 [59552/60000 (99%)]\tLoss: 0.602776\n",
      "Training Epoch: 1 [59584/60000 (99%)]\tLoss: 0.660375\n",
      "Training Epoch: 1 [59616/60000 (99%)]\tLoss: 0.380151\n",
      "Training Epoch: 1 [59648/60000 (99%)]\tLoss: 0.274394\n",
      "Training Epoch: 1 [59680/60000 (99%)]\tLoss: 0.402426\n",
      "Training Epoch: 1 [59712/60000 (100%)]\tLoss: 0.316892\n",
      "Training Epoch: 1 [59744/60000 (100%)]\tLoss: 0.506890\n",
      "Training Epoch: 1 [59776/60000 (100%)]\tLoss: 0.369113\n",
      "Training Epoch: 1 [59808/60000 (100%)]\tLoss: 0.713665\n",
      "Training Epoch: 1 [59840/60000 (100%)]\tLoss: 1.068818\n",
      "Training Epoch: 1 [59872/60000 (100%)]\tLoss: 0.316190\n",
      "Training Epoch: 1 [59904/60000 (100%)]\tLoss: 0.476244\n",
      "Training Epoch: 1 [59936/60000 (100%)]\tLoss: 0.511093\n",
      "Training Epoch: 1 [59968/60000 (100%)]\tLoss: 0.358253\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9558/10000 (96%)\n",
      "\n",
      "Training Epoch: 2 [0/60000 (0%)]\tLoss: 0.812756\n",
      "Training Epoch: 2 [32/60000 (0%)]\tLoss: 0.366825\n",
      "Training Epoch: 2 [64/60000 (0%)]\tLoss: 0.608529\n",
      "Training Epoch: 2 [96/60000 (0%)]\tLoss: 0.474972\n",
      "Training Epoch: 2 [128/60000 (0%)]\tLoss: 0.894840\n",
      "Training Epoch: 2 [160/60000 (0%)]\tLoss: 0.613996\n",
      "Training Epoch: 2 [192/60000 (0%)]\tLoss: 0.118055\n",
      "Training Epoch: 2 [224/60000 (0%)]\tLoss: 0.471723\n",
      "Training Epoch: 2 [256/60000 (0%)]\tLoss: 0.660044\n",
      "Training Epoch: 2 [288/60000 (0%)]\tLoss: 0.497768\n",
      "Training Epoch: 2 [320/60000 (1%)]\tLoss: 0.634654\n",
      "Training Epoch: 2 [352/60000 (1%)]\tLoss: 0.466053\n",
      "Training Epoch: 2 [384/60000 (1%)]\tLoss: 0.975837\n",
      "Training Epoch: 2 [416/60000 (1%)]\tLoss: 1.055077\n",
      "Training Epoch: 2 [448/60000 (1%)]\tLoss: 0.499443\n",
      "Training Epoch: 2 [480/60000 (1%)]\tLoss: 0.854436\n",
      "Training Epoch: 2 [512/60000 (1%)]\tLoss: 0.254046\n",
      "Training Epoch: 2 [544/60000 (1%)]\tLoss: 0.659534\n",
      "Training Epoch: 2 [576/60000 (1%)]\tLoss: 0.318242\n",
      "Training Epoch: 2 [608/60000 (1%)]\tLoss: 0.717735\n",
      "Training Epoch: 2 [640/60000 (1%)]\tLoss: 0.383295\n",
      "Training Epoch: 2 [672/60000 (1%)]\tLoss: 0.424396\n",
      "Training Epoch: 2 [704/60000 (1%)]\tLoss: 0.479794\n",
      "Training Epoch: 2 [736/60000 (1%)]\tLoss: 0.440378\n",
      "Training Epoch: 2 [768/60000 (1%)]\tLoss: 0.538902\n",
      "Training Epoch: 2 [800/60000 (1%)]\tLoss: 0.233858\n",
      "Training Epoch: 2 [832/60000 (1%)]\tLoss: 0.369252\n",
      "Training Epoch: 2 [864/60000 (1%)]\tLoss: 0.360463\n",
      "Training Epoch: 2 [896/60000 (1%)]\tLoss: 1.203009\n",
      "Training Epoch: 2 [928/60000 (2%)]\tLoss: 0.621801\n",
      "Training Epoch: 2 [960/60000 (2%)]\tLoss: 0.868066\n",
      "Training Epoch: 2 [992/60000 (2%)]\tLoss: 0.179606\n",
      "Training Epoch: 2 [1024/60000 (2%)]\tLoss: 0.652222\n",
      "Training Epoch: 2 [1056/60000 (2%)]\tLoss: 0.289748\n",
      "Training Epoch: 2 [1088/60000 (2%)]\tLoss: 0.443716\n",
      "Training Epoch: 2 [1120/60000 (2%)]\tLoss: 0.940849\n",
      "Training Epoch: 2 [1152/60000 (2%)]\tLoss: 0.477737\n",
      "Training Epoch: 2 [1184/60000 (2%)]\tLoss: 0.560403\n",
      "Training Epoch: 2 [1216/60000 (2%)]\tLoss: 0.280573\n",
      "Training Epoch: 2 [1248/60000 (2%)]\tLoss: 0.555161\n",
      "Training Epoch: 2 [1280/60000 (2%)]\tLoss: 0.457238\n",
      "Training Epoch: 2 [1312/60000 (2%)]\tLoss: 0.532020\n",
      "Training Epoch: 2 [1344/60000 (2%)]\tLoss: 0.483921\n",
      "Training Epoch: 2 [1376/60000 (2%)]\tLoss: 0.518217\n",
      "Training Epoch: 2 [1408/60000 (2%)]\tLoss: 0.715744\n",
      "Training Epoch: 2 [1440/60000 (2%)]\tLoss: 0.207650\n",
      "Training Epoch: 2 [1472/60000 (2%)]\tLoss: 0.580878\n",
      "Training Epoch: 2 [1504/60000 (3%)]\tLoss: 0.419227\n",
      "Training Epoch: 2 [1536/60000 (3%)]\tLoss: 0.805161\n",
      "Training Epoch: 2 [1568/60000 (3%)]\tLoss: 0.652903\n",
      "Training Epoch: 2 [1600/60000 (3%)]\tLoss: 0.174396\n",
      "Training Epoch: 2 [1632/60000 (3%)]\tLoss: 0.365830\n",
      "Training Epoch: 2 [1664/60000 (3%)]\tLoss: 0.836761\n",
      "Training Epoch: 2 [1696/60000 (3%)]\tLoss: 0.656055\n",
      "Training Epoch: 2 [1728/60000 (3%)]\tLoss: 0.346975\n",
      "Training Epoch: 2 [1760/60000 (3%)]\tLoss: 0.838718\n",
      "Training Epoch: 2 [1792/60000 (3%)]\tLoss: 0.455538\n",
      "Training Epoch: 2 [1824/60000 (3%)]\tLoss: 0.576925\n",
      "Training Epoch: 2 [1856/60000 (3%)]\tLoss: 0.284567\n",
      "Training Epoch: 2 [1888/60000 (3%)]\tLoss: 0.715225\n",
      "Training Epoch: 2 [1920/60000 (3%)]\tLoss: 0.709006\n",
      "Training Epoch: 2 [1952/60000 (3%)]\tLoss: 0.609232\n",
      "Training Epoch: 2 [1984/60000 (3%)]\tLoss: 0.841173\n",
      "Training Epoch: 2 [2016/60000 (3%)]\tLoss: 0.318772\n",
      "Training Epoch: 2 [2048/60000 (3%)]\tLoss: 0.634579\n",
      "Training Epoch: 2 [2080/60000 (3%)]\tLoss: 1.427823\n",
      "Training Epoch: 2 [2112/60000 (4%)]\tLoss: 0.407748\n",
      "Training Epoch: 2 [2144/60000 (4%)]\tLoss: 0.793930\n",
      "Training Epoch: 2 [2176/60000 (4%)]\tLoss: 0.725661\n",
      "Training Epoch: 2 [2208/60000 (4%)]\tLoss: 0.489414\n",
      "Training Epoch: 2 [2240/60000 (4%)]\tLoss: 0.576717\n",
      "Training Epoch: 2 [2272/60000 (4%)]\tLoss: 0.277203\n",
      "Training Epoch: 2 [2304/60000 (4%)]\tLoss: 0.719989\n",
      "Training Epoch: 2 [2336/60000 (4%)]\tLoss: 0.246716\n",
      "Training Epoch: 2 [2368/60000 (4%)]\tLoss: 0.297873\n",
      "Training Epoch: 2 [2400/60000 (4%)]\tLoss: 0.353147\n",
      "Training Epoch: 2 [2432/60000 (4%)]\tLoss: 0.518617\n",
      "Training Epoch: 2 [2464/60000 (4%)]\tLoss: 0.901631\n",
      "Training Epoch: 2 [2496/60000 (4%)]\tLoss: 0.571833\n",
      "Training Epoch: 2 [2528/60000 (4%)]\tLoss: 0.329476\n",
      "Training Epoch: 2 [2560/60000 (4%)]\tLoss: 0.543092\n",
      "Training Epoch: 2 [2592/60000 (4%)]\tLoss: 0.345328\n",
      "Training Epoch: 2 [2624/60000 (4%)]\tLoss: 0.617981\n",
      "Training Epoch: 2 [2656/60000 (4%)]\tLoss: 0.654414\n",
      "Training Epoch: 2 [2688/60000 (4%)]\tLoss: 0.885630\n",
      "Training Epoch: 2 [2720/60000 (5%)]\tLoss: 0.718246\n",
      "Training Epoch: 2 [2752/60000 (5%)]\tLoss: 0.415688\n",
      "Training Epoch: 2 [2784/60000 (5%)]\tLoss: 0.701795\n",
      "Training Epoch: 2 [2816/60000 (5%)]\tLoss: 0.508021\n",
      "Training Epoch: 2 [2848/60000 (5%)]\tLoss: 0.696274\n",
      "Training Epoch: 2 [2880/60000 (5%)]\tLoss: 0.976290\n",
      "Training Epoch: 2 [2912/60000 (5%)]\tLoss: 0.702307\n",
      "Training Epoch: 2 [2944/60000 (5%)]\tLoss: 0.536039\n",
      "Training Epoch: 2 [2976/60000 (5%)]\tLoss: 0.598914\n",
      "Training Epoch: 2 [3008/60000 (5%)]\tLoss: 0.340071\n",
      "Training Epoch: 2 [3040/60000 (5%)]\tLoss: 0.756370\n",
      "Training Epoch: 2 [3072/60000 (5%)]\tLoss: 0.318471\n",
      "Training Epoch: 2 [3104/60000 (5%)]\tLoss: 0.512326\n",
      "Training Epoch: 2 [3136/60000 (5%)]\tLoss: 0.227691\n",
      "Training Epoch: 2 [3168/60000 (5%)]\tLoss: 0.782218\n",
      "Training Epoch: 2 [3200/60000 (5%)]\tLoss: 0.502348\n",
      "Training Epoch: 2 [3232/60000 (5%)]\tLoss: 0.404944\n",
      "Training Epoch: 2 [3264/60000 (5%)]\tLoss: 0.499658\n",
      "Training Epoch: 2 [3296/60000 (5%)]\tLoss: 0.683354\n",
      "Training Epoch: 2 [3328/60000 (6%)]\tLoss: 0.629715\n",
      "Training Epoch: 2 [3360/60000 (6%)]\tLoss: 0.653234\n",
      "Training Epoch: 2 [3392/60000 (6%)]\tLoss: 0.382611\n",
      "Training Epoch: 2 [3424/60000 (6%)]\tLoss: 0.858450\n",
      "Training Epoch: 2 [3456/60000 (6%)]\tLoss: 0.526966\n",
      "Training Epoch: 2 [3488/60000 (6%)]\tLoss: 0.349377\n",
      "Training Epoch: 2 [3520/60000 (6%)]\tLoss: 0.326475\n",
      "Training Epoch: 2 [3552/60000 (6%)]\tLoss: 0.338733\n",
      "Training Epoch: 2 [3584/60000 (6%)]\tLoss: 0.488719\n",
      "Training Epoch: 2 [3616/60000 (6%)]\tLoss: 0.411866\n",
      "Training Epoch: 2 [3648/60000 (6%)]\tLoss: 0.710276\n",
      "Training Epoch: 2 [3680/60000 (6%)]\tLoss: 0.404208\n",
      "Training Epoch: 2 [3712/60000 (6%)]\tLoss: 0.575250\n",
      "Training Epoch: 2 [3744/60000 (6%)]\tLoss: 0.637048\n",
      "Training Epoch: 2 [3776/60000 (6%)]\tLoss: 0.669245\n",
      "Training Epoch: 2 [3808/60000 (6%)]\tLoss: 0.770727\n",
      "Training Epoch: 2 [3840/60000 (6%)]\tLoss: 0.500388\n",
      "Training Epoch: 2 [3872/60000 (6%)]\tLoss: 0.253646\n",
      "Training Epoch: 2 [3904/60000 (7%)]\tLoss: 0.659481\n",
      "Training Epoch: 2 [3936/60000 (7%)]\tLoss: 0.609179\n",
      "Training Epoch: 2 [3968/60000 (7%)]\tLoss: 0.529206\n",
      "Training Epoch: 2 [4000/60000 (7%)]\tLoss: 0.436830\n",
      "Training Epoch: 2 [4032/60000 (7%)]\tLoss: 0.477701\n",
      "Training Epoch: 2 [4064/60000 (7%)]\tLoss: 0.584055\n",
      "Training Epoch: 2 [4096/60000 (7%)]\tLoss: 0.364182\n",
      "Training Epoch: 2 [4128/60000 (7%)]\tLoss: 0.548778\n",
      "Training Epoch: 2 [4160/60000 (7%)]\tLoss: 0.439562\n",
      "Training Epoch: 2 [4192/60000 (7%)]\tLoss: 0.339395\n",
      "Training Epoch: 2 [4224/60000 (7%)]\tLoss: 0.741335\n",
      "Training Epoch: 2 [4256/60000 (7%)]\tLoss: 0.243324\n",
      "Training Epoch: 2 [4288/60000 (7%)]\tLoss: 0.512804\n",
      "Training Epoch: 2 [4320/60000 (7%)]\tLoss: 0.443159\n",
      "Training Epoch: 2 [4352/60000 (7%)]\tLoss: 0.685457\n",
      "Training Epoch: 2 [4384/60000 (7%)]\tLoss: 0.489283\n",
      "Training Epoch: 2 [4416/60000 (7%)]\tLoss: 0.562858\n",
      "Training Epoch: 2 [4448/60000 (7%)]\tLoss: 0.615803\n",
      "Training Epoch: 2 [4480/60000 (7%)]\tLoss: 0.687466\n",
      "Training Epoch: 2 [4512/60000 (8%)]\tLoss: 0.298199\n",
      "Training Epoch: 2 [4544/60000 (8%)]\tLoss: 0.624927\n",
      "Training Epoch: 2 [4576/60000 (8%)]\tLoss: 0.428523\n",
      "Training Epoch: 2 [4608/60000 (8%)]\tLoss: 0.712156\n",
      "Training Epoch: 2 [4640/60000 (8%)]\tLoss: 0.251780\n",
      "Training Epoch: 2 [4672/60000 (8%)]\tLoss: 0.953425\n",
      "Training Epoch: 2 [4704/60000 (8%)]\tLoss: 0.306289\n",
      "Training Epoch: 2 [4736/60000 (8%)]\tLoss: 0.363463\n",
      "Training Epoch: 2 [4768/60000 (8%)]\tLoss: 0.348461\n",
      "Training Epoch: 2 [4800/60000 (8%)]\tLoss: 1.135520\n",
      "Training Epoch: 2 [4832/60000 (8%)]\tLoss: 0.369260\n",
      "Training Epoch: 2 [4864/60000 (8%)]\tLoss: 0.384978\n",
      "Training Epoch: 2 [4896/60000 (8%)]\tLoss: 0.637864\n",
      "Training Epoch: 2 [4928/60000 (8%)]\tLoss: 0.352418\n",
      "Training Epoch: 2 [4960/60000 (8%)]\tLoss: 0.478199\n",
      "Training Epoch: 2 [4992/60000 (8%)]\tLoss: 0.350580\n",
      "Training Epoch: 2 [5024/60000 (8%)]\tLoss: 0.682759\n",
      "Training Epoch: 2 [5056/60000 (8%)]\tLoss: 0.605233\n",
      "Training Epoch: 2 [5088/60000 (8%)]\tLoss: 0.853611\n",
      "Training Epoch: 2 [5120/60000 (9%)]\tLoss: 0.173059\n",
      "Training Epoch: 2 [5152/60000 (9%)]\tLoss: 0.353133\n",
      "Training Epoch: 2 [5184/60000 (9%)]\tLoss: 0.613127\n",
      "Training Epoch: 2 [5216/60000 (9%)]\tLoss: 0.896779\n",
      "Training Epoch: 2 [5248/60000 (9%)]\tLoss: 0.468934\n",
      "Training Epoch: 2 [5280/60000 (9%)]\tLoss: 0.423839\n",
      "Training Epoch: 2 [5312/60000 (9%)]\tLoss: 0.400702\n",
      "Training Epoch: 2 [5344/60000 (9%)]\tLoss: 0.424086\n",
      "Training Epoch: 2 [5376/60000 (9%)]\tLoss: 0.538194\n",
      "Training Epoch: 2 [5408/60000 (9%)]\tLoss: 0.371440\n",
      "Training Epoch: 2 [5440/60000 (9%)]\tLoss: 0.601462\n",
      "Training Epoch: 2 [5472/60000 (9%)]\tLoss: 0.379640\n",
      "Training Epoch: 2 [5504/60000 (9%)]\tLoss: 0.597469\n",
      "Training Epoch: 2 [5536/60000 (9%)]\tLoss: 0.506036\n",
      "Training Epoch: 2 [5568/60000 (9%)]\tLoss: 0.870243\n",
      "Training Epoch: 2 [5600/60000 (9%)]\tLoss: 0.465474\n",
      "Training Epoch: 2 [5632/60000 (9%)]\tLoss: 0.337982\n",
      "Training Epoch: 2 [5664/60000 (9%)]\tLoss: 0.505478\n",
      "Training Epoch: 2 [5696/60000 (9%)]\tLoss: 0.377119\n",
      "Training Epoch: 2 [5728/60000 (10%)]\tLoss: 0.725235\n",
      "Training Epoch: 2 [5760/60000 (10%)]\tLoss: 0.719779\n",
      "Training Epoch: 2 [5792/60000 (10%)]\tLoss: 0.606813\n",
      "Training Epoch: 2 [5824/60000 (10%)]\tLoss: 0.973679\n",
      "Training Epoch: 2 [5856/60000 (10%)]\tLoss: 0.378201\n",
      "Training Epoch: 2 [5888/60000 (10%)]\tLoss: 0.445185\n",
      "Training Epoch: 2 [5920/60000 (10%)]\tLoss: 0.579468\n",
      "Training Epoch: 2 [5952/60000 (10%)]\tLoss: 0.514108\n",
      "Training Epoch: 2 [5984/60000 (10%)]\tLoss: 0.197653\n",
      "Training Epoch: 2 [6016/60000 (10%)]\tLoss: 0.043820\n",
      "Training Epoch: 2 [6048/60000 (10%)]\tLoss: 0.307722\n",
      "Training Epoch: 2 [6080/60000 (10%)]\tLoss: 0.500712\n",
      "Training Epoch: 2 [6112/60000 (10%)]\tLoss: 0.942010\n",
      "Training Epoch: 2 [6144/60000 (10%)]\tLoss: 0.513184\n",
      "Training Epoch: 2 [6176/60000 (10%)]\tLoss: 0.603224\n",
      "Training Epoch: 2 [6208/60000 (10%)]\tLoss: 0.417077\n",
      "Training Epoch: 2 [6240/60000 (10%)]\tLoss: 0.575830\n",
      "Training Epoch: 2 [6272/60000 (10%)]\tLoss: 0.965848\n",
      "Training Epoch: 2 [6304/60000 (11%)]\tLoss: 0.759077\n",
      "Training Epoch: 2 [6336/60000 (11%)]\tLoss: 0.510326\n",
      "Training Epoch: 2 [6368/60000 (11%)]\tLoss: 0.315614\n",
      "Training Epoch: 2 [6400/60000 (11%)]\tLoss: 0.512134\n",
      "Training Epoch: 2 [6432/60000 (11%)]\tLoss: 0.480205\n",
      "Training Epoch: 2 [6464/60000 (11%)]\tLoss: 0.846650\n",
      "Training Epoch: 2 [6496/60000 (11%)]\tLoss: 0.203904\n",
      "Training Epoch: 2 [6528/60000 (11%)]\tLoss: 0.798979\n",
      "Training Epoch: 2 [6560/60000 (11%)]\tLoss: 0.691729\n",
      "Training Epoch: 2 [6592/60000 (11%)]\tLoss: 0.464570\n",
      "Training Epoch: 2 [6624/60000 (11%)]\tLoss: 0.327351\n",
      "Training Epoch: 2 [6656/60000 (11%)]\tLoss: 0.235156\n",
      "Training Epoch: 2 [6688/60000 (11%)]\tLoss: 0.474546\n",
      "Training Epoch: 2 [6720/60000 (11%)]\tLoss: 0.519898\n",
      "Training Epoch: 2 [6752/60000 (11%)]\tLoss: 0.466910\n",
      "Training Epoch: 2 [6784/60000 (11%)]\tLoss: 0.684555\n",
      "Training Epoch: 2 [6816/60000 (11%)]\tLoss: 0.412060\n",
      "Training Epoch: 2 [6848/60000 (11%)]\tLoss: 0.857286\n",
      "Training Epoch: 2 [6880/60000 (11%)]\tLoss: 0.355367\n",
      "Training Epoch: 2 [6912/60000 (12%)]\tLoss: 0.386084\n",
      "Training Epoch: 2 [6944/60000 (12%)]\tLoss: 0.673620\n",
      "Training Epoch: 2 [6976/60000 (12%)]\tLoss: 0.164770\n",
      "Training Epoch: 2 [7008/60000 (12%)]\tLoss: 0.487459\n",
      "Training Epoch: 2 [7040/60000 (12%)]\tLoss: 0.333477\n",
      "Training Epoch: 2 [7072/60000 (12%)]\tLoss: 0.943383\n",
      "Training Epoch: 2 [7104/60000 (12%)]\tLoss: 0.348736\n",
      "Training Epoch: 2 [7136/60000 (12%)]\tLoss: 0.583545\n",
      "Training Epoch: 2 [7168/60000 (12%)]\tLoss: 0.225634\n",
      "Training Epoch: 2 [7200/60000 (12%)]\tLoss: 0.192684\n",
      "Training Epoch: 2 [7232/60000 (12%)]\tLoss: 0.604896\n",
      "Training Epoch: 2 [7264/60000 (12%)]\tLoss: 0.421022\n",
      "Training Epoch: 2 [7296/60000 (12%)]\tLoss: 0.764599\n",
      "Training Epoch: 2 [7328/60000 (12%)]\tLoss: 0.335256\n",
      "Training Epoch: 2 [7360/60000 (12%)]\tLoss: 1.048434\n",
      "Training Epoch: 2 [7392/60000 (12%)]\tLoss: 0.640245\n",
      "Training Epoch: 2 [7424/60000 (12%)]\tLoss: 0.545213\n",
      "Training Epoch: 2 [7456/60000 (12%)]\tLoss: 0.143923\n",
      "Training Epoch: 2 [7488/60000 (12%)]\tLoss: 0.449810\n",
      "Training Epoch: 2 [7520/60000 (13%)]\tLoss: 0.772456\n",
      "Training Epoch: 2 [7552/60000 (13%)]\tLoss: 0.380716\n",
      "Training Epoch: 2 [7584/60000 (13%)]\tLoss: 0.253782\n",
      "Training Epoch: 2 [7616/60000 (13%)]\tLoss: 0.499265\n",
      "Training Epoch: 2 [7648/60000 (13%)]\tLoss: 0.389339\n",
      "Training Epoch: 2 [7680/60000 (13%)]\tLoss: 0.560273\n",
      "Training Epoch: 2 [7712/60000 (13%)]\tLoss: 0.325054\n",
      "Training Epoch: 2 [7744/60000 (13%)]\tLoss: 0.372971\n",
      "Training Epoch: 2 [7776/60000 (13%)]\tLoss: 0.281722\n",
      "Training Epoch: 2 [7808/60000 (13%)]\tLoss: 0.347048\n",
      "Training Epoch: 2 [7840/60000 (13%)]\tLoss: 0.362167\n",
      "Training Epoch: 2 [7872/60000 (13%)]\tLoss: 0.552420\n",
      "Training Epoch: 2 [7904/60000 (13%)]\tLoss: 0.334542\n",
      "Training Epoch: 2 [7936/60000 (13%)]\tLoss: 0.633644\n",
      "Training Epoch: 2 [7968/60000 (13%)]\tLoss: 0.522085\n",
      "Training Epoch: 2 [8000/60000 (13%)]\tLoss: 0.550906\n",
      "Training Epoch: 2 [8032/60000 (13%)]\tLoss: 0.456904\n",
      "Training Epoch: 2 [8064/60000 (13%)]\tLoss: 0.303988\n",
      "Training Epoch: 2 [8096/60000 (13%)]\tLoss: 0.591861\n",
      "Training Epoch: 2 [8128/60000 (14%)]\tLoss: 0.306271\n",
      "Training Epoch: 2 [8160/60000 (14%)]\tLoss: 0.669820\n",
      "Training Epoch: 2 [8192/60000 (14%)]\tLoss: 0.481932\n",
      "Training Epoch: 2 [8224/60000 (14%)]\tLoss: 0.243831\n",
      "Training Epoch: 2 [8256/60000 (14%)]\tLoss: 0.456819\n",
      "Training Epoch: 2 [8288/60000 (14%)]\tLoss: 0.505378\n",
      "Training Epoch: 2 [8320/60000 (14%)]\tLoss: 0.770597\n",
      "Training Epoch: 2 [8352/60000 (14%)]\tLoss: 0.704661\n",
      "Training Epoch: 2 [8384/60000 (14%)]\tLoss: 0.501019\n",
      "Training Epoch: 2 [8416/60000 (14%)]\tLoss: 0.556452\n",
      "Training Epoch: 2 [8448/60000 (14%)]\tLoss: 0.716007\n",
      "Training Epoch: 2 [8480/60000 (14%)]\tLoss: 0.674251\n",
      "Training Epoch: 2 [8512/60000 (14%)]\tLoss: 0.451089\n",
      "Training Epoch: 2 [8544/60000 (14%)]\tLoss: 0.538283\n",
      "Training Epoch: 2 [8576/60000 (14%)]\tLoss: 0.477728\n",
      "Training Epoch: 2 [8608/60000 (14%)]\tLoss: 1.086064\n",
      "Training Epoch: 2 [8640/60000 (14%)]\tLoss: 0.479277\n",
      "Training Epoch: 2 [8672/60000 (14%)]\tLoss: 0.583852\n",
      "Training Epoch: 2 [8704/60000 (15%)]\tLoss: 0.680365\n",
      "Training Epoch: 2 [8736/60000 (15%)]\tLoss: 0.505773\n",
      "Training Epoch: 2 [8768/60000 (15%)]\tLoss: 0.418476\n",
      "Training Epoch: 2 [8800/60000 (15%)]\tLoss: 0.480768\n",
      "Training Epoch: 2 [8832/60000 (15%)]\tLoss: 0.445441\n",
      "Training Epoch: 2 [8864/60000 (15%)]\tLoss: 0.373466\n",
      "Training Epoch: 2 [8896/60000 (15%)]\tLoss: 0.053090\n",
      "Training Epoch: 2 [8928/60000 (15%)]\tLoss: 0.222263\n",
      "Training Epoch: 2 [8960/60000 (15%)]\tLoss: 0.297182\n",
      "Training Epoch: 2 [8992/60000 (15%)]\tLoss: 0.884987\n",
      "Training Epoch: 2 [9024/60000 (15%)]\tLoss: 0.223429\n",
      "Training Epoch: 2 [9056/60000 (15%)]\tLoss: 0.295293\n",
      "Training Epoch: 2 [9088/60000 (15%)]\tLoss: 0.625247\n",
      "Training Epoch: 2 [9120/60000 (15%)]\tLoss: 0.494224\n",
      "Training Epoch: 2 [9152/60000 (15%)]\tLoss: 0.776603\n",
      "Training Epoch: 2 [9184/60000 (15%)]\tLoss: 0.650262\n",
      "Training Epoch: 2 [9216/60000 (15%)]\tLoss: 0.424634\n",
      "Training Epoch: 2 [9248/60000 (15%)]\tLoss: 0.306907\n",
      "Training Epoch: 2 [9280/60000 (15%)]\tLoss: 0.551025\n",
      "Training Epoch: 2 [9312/60000 (16%)]\tLoss: 0.333356\n",
      "Training Epoch: 2 [9344/60000 (16%)]\tLoss: 0.242839\n",
      "Training Epoch: 2 [9376/60000 (16%)]\tLoss: 0.831111\n",
      "Training Epoch: 2 [9408/60000 (16%)]\tLoss: 0.311934\n",
      "Training Epoch: 2 [9440/60000 (16%)]\tLoss: 0.601125\n",
      "Training Epoch: 2 [9472/60000 (16%)]\tLoss: 0.801348\n",
      "Training Epoch: 2 [9504/60000 (16%)]\tLoss: 0.340642\n",
      "Training Epoch: 2 [9536/60000 (16%)]\tLoss: 0.361059\n",
      "Training Epoch: 2 [9568/60000 (16%)]\tLoss: 0.444198\n",
      "Training Epoch: 2 [9600/60000 (16%)]\tLoss: 0.684797\n",
      "Training Epoch: 2 [9632/60000 (16%)]\tLoss: 0.527857\n",
      "Training Epoch: 2 [9664/60000 (16%)]\tLoss: 0.362146\n",
      "Training Epoch: 2 [9696/60000 (16%)]\tLoss: 0.634708\n",
      "Training Epoch: 2 [9728/60000 (16%)]\tLoss: 0.452557\n",
      "Training Epoch: 2 [9760/60000 (16%)]\tLoss: 0.715132\n",
      "Training Epoch: 2 [9792/60000 (16%)]\tLoss: 0.572085\n",
      "Training Epoch: 2 [9824/60000 (16%)]\tLoss: 0.322824\n",
      "Training Epoch: 2 [9856/60000 (16%)]\tLoss: 0.679346\n",
      "Training Epoch: 2 [9888/60000 (16%)]\tLoss: 0.356143\n",
      "Training Epoch: 2 [9920/60000 (17%)]\tLoss: 0.449256\n",
      "Training Epoch: 2 [9952/60000 (17%)]\tLoss: 0.731078\n",
      "Training Epoch: 2 [9984/60000 (17%)]\tLoss: 0.559455\n",
      "Training Epoch: 2 [10016/60000 (17%)]\tLoss: 0.267145\n",
      "Training Epoch: 2 [10048/60000 (17%)]\tLoss: 0.396038\n",
      "Training Epoch: 2 [10080/60000 (17%)]\tLoss: 0.720937\n",
      "Training Epoch: 2 [10112/60000 (17%)]\tLoss: 0.483794\n",
      "Training Epoch: 2 [10144/60000 (17%)]\tLoss: 0.279987\n",
      "Training Epoch: 2 [10176/60000 (17%)]\tLoss: 0.739066\n",
      "Training Epoch: 2 [10208/60000 (17%)]\tLoss: 0.884100\n",
      "Training Epoch: 2 [10240/60000 (17%)]\tLoss: 0.927324\n",
      "Training Epoch: 2 [10272/60000 (17%)]\tLoss: 0.541484\n",
      "Training Epoch: 2 [10304/60000 (17%)]\tLoss: 0.953565\n",
      "Training Epoch: 2 [10336/60000 (17%)]\tLoss: 0.575888\n",
      "Training Epoch: 2 [10368/60000 (17%)]\tLoss: 0.399356\n",
      "Training Epoch: 2 [10400/60000 (17%)]\tLoss: 0.416453\n",
      "Training Epoch: 2 [10432/60000 (17%)]\tLoss: 0.875793\n",
      "Training Epoch: 2 [10464/60000 (17%)]\tLoss: 0.675014\n",
      "Training Epoch: 2 [10496/60000 (17%)]\tLoss: 0.763104\n",
      "Training Epoch: 2 [10528/60000 (18%)]\tLoss: 0.530409\n",
      "Training Epoch: 2 [10560/60000 (18%)]\tLoss: 0.393084\n",
      "Training Epoch: 2 [10592/60000 (18%)]\tLoss: 0.499648\n",
      "Training Epoch: 2 [10624/60000 (18%)]\tLoss: 0.404699\n",
      "Training Epoch: 2 [10656/60000 (18%)]\tLoss: 0.581345\n",
      "Training Epoch: 2 [10688/60000 (18%)]\tLoss: 0.253553\n",
      "Training Epoch: 2 [10720/60000 (18%)]\tLoss: 0.642337\n",
      "Training Epoch: 2 [10752/60000 (18%)]\tLoss: 0.344152\n",
      "Training Epoch: 2 [10784/60000 (18%)]\tLoss: 0.233416\n",
      "Training Epoch: 2 [10816/60000 (18%)]\tLoss: 0.144509\n",
      "Training Epoch: 2 [10848/60000 (18%)]\tLoss: 0.268973\n",
      "Training Epoch: 2 [10880/60000 (18%)]\tLoss: 0.693854\n",
      "Training Epoch: 2 [10912/60000 (18%)]\tLoss: 0.435384\n",
      "Training Epoch: 2 [10944/60000 (18%)]\tLoss: 0.740100\n",
      "Training Epoch: 2 [10976/60000 (18%)]\tLoss: 0.622498\n",
      "Training Epoch: 2 [11008/60000 (18%)]\tLoss: 0.449928\n",
      "Training Epoch: 2 [11040/60000 (18%)]\tLoss: 0.978523\n",
      "Training Epoch: 2 [11072/60000 (18%)]\tLoss: 0.379989\n",
      "Training Epoch: 2 [11104/60000 (19%)]\tLoss: 0.246008\n",
      "Training Epoch: 2 [11136/60000 (19%)]\tLoss: 0.280427\n",
      "Training Epoch: 2 [11168/60000 (19%)]\tLoss: 0.874215\n",
      "Training Epoch: 2 [11200/60000 (19%)]\tLoss: 0.248970\n",
      "Training Epoch: 2 [11232/60000 (19%)]\tLoss: 0.233093\n",
      "Training Epoch: 2 [11264/60000 (19%)]\tLoss: 0.332348\n",
      "Training Epoch: 2 [11296/60000 (19%)]\tLoss: 0.429286\n",
      "Training Epoch: 2 [11328/60000 (19%)]\tLoss: 0.359396\n",
      "Training Epoch: 2 [11360/60000 (19%)]\tLoss: 0.386070\n",
      "Training Epoch: 2 [11392/60000 (19%)]\tLoss: 0.334775\n",
      "Training Epoch: 2 [11424/60000 (19%)]\tLoss: 0.662411\n",
      "Training Epoch: 2 [11456/60000 (19%)]\tLoss: 0.451578\n",
      "Training Epoch: 2 [11488/60000 (19%)]\tLoss: 0.461209\n",
      "Training Epoch: 2 [11520/60000 (19%)]\tLoss: 0.763913\n",
      "Training Epoch: 2 [11552/60000 (19%)]\tLoss: 0.393684\n",
      "Training Epoch: 2 [11584/60000 (19%)]\tLoss: 0.621178\n",
      "Training Epoch: 2 [11616/60000 (19%)]\tLoss: 0.942430\n",
      "Training Epoch: 2 [11648/60000 (19%)]\tLoss: 0.326994\n",
      "Training Epoch: 2 [11680/60000 (19%)]\tLoss: 0.725059\n",
      "Training Epoch: 2 [11712/60000 (20%)]\tLoss: 0.673257\n",
      "Training Epoch: 2 [11744/60000 (20%)]\tLoss: 0.516871\n",
      "Training Epoch: 2 [11776/60000 (20%)]\tLoss: 0.675677\n",
      "Training Epoch: 2 [11808/60000 (20%)]\tLoss: 0.688740\n",
      "Training Epoch: 2 [11840/60000 (20%)]\tLoss: 0.564121\n",
      "Training Epoch: 2 [11872/60000 (20%)]\tLoss: 0.374447\n",
      "Training Epoch: 2 [11904/60000 (20%)]\tLoss: 0.498690\n",
      "Training Epoch: 2 [11936/60000 (20%)]\tLoss: 0.485193\n",
      "Training Epoch: 2 [11968/60000 (20%)]\tLoss: 0.136512\n",
      "Training Epoch: 2 [12000/60000 (20%)]\tLoss: 0.320237\n",
      "Training Epoch: 2 [12032/60000 (20%)]\tLoss: 0.392911\n",
      "Training Epoch: 2 [12064/60000 (20%)]\tLoss: 0.441837\n",
      "Training Epoch: 2 [12096/60000 (20%)]\tLoss: 0.709493\n",
      "Training Epoch: 2 [12128/60000 (20%)]\tLoss: 0.502605\n",
      "Training Epoch: 2 [12160/60000 (20%)]\tLoss: 0.368636\n",
      "Training Epoch: 2 [12192/60000 (20%)]\tLoss: 0.375071\n",
      "Training Epoch: 2 [12224/60000 (20%)]\tLoss: 0.384127\n",
      "Training Epoch: 2 [12256/60000 (20%)]\tLoss: 0.723707\n",
      "Training Epoch: 2 [12288/60000 (20%)]\tLoss: 0.365859\n",
      "Training Epoch: 2 [12320/60000 (21%)]\tLoss: 0.595965\n",
      "Training Epoch: 2 [12352/60000 (21%)]\tLoss: 0.560173\n",
      "Training Epoch: 2 [12384/60000 (21%)]\tLoss: 0.469880\n",
      "Training Epoch: 2 [12416/60000 (21%)]\tLoss: 0.641455\n",
      "Training Epoch: 2 [12448/60000 (21%)]\tLoss: 0.397541\n",
      "Training Epoch: 2 [12480/60000 (21%)]\tLoss: 0.465148\n",
      "Training Epoch: 2 [12512/60000 (21%)]\tLoss: 0.433900\n",
      "Training Epoch: 2 [12544/60000 (21%)]\tLoss: 0.510434\n",
      "Training Epoch: 2 [12576/60000 (21%)]\tLoss: 0.447655\n",
      "Training Epoch: 2 [12608/60000 (21%)]\tLoss: 0.994886\n",
      "Training Epoch: 2 [12640/60000 (21%)]\tLoss: 0.349893\n",
      "Training Epoch: 2 [12672/60000 (21%)]\tLoss: 0.328654\n",
      "Training Epoch: 2 [12704/60000 (21%)]\tLoss: 0.316721\n",
      "Training Epoch: 2 [12736/60000 (21%)]\tLoss: 0.747939\n",
      "Training Epoch: 2 [12768/60000 (21%)]\tLoss: 1.012292\n",
      "Training Epoch: 2 [12800/60000 (21%)]\tLoss: 0.506331\n",
      "Training Epoch: 2 [12832/60000 (21%)]\tLoss: 0.671156\n",
      "Training Epoch: 2 [12864/60000 (21%)]\tLoss: 0.537230\n",
      "Training Epoch: 2 [12896/60000 (21%)]\tLoss: 0.510495\n",
      "Training Epoch: 2 [12928/60000 (22%)]\tLoss: 0.381252\n",
      "Training Epoch: 2 [12960/60000 (22%)]\tLoss: 0.451720\n",
      "Training Epoch: 2 [12992/60000 (22%)]\tLoss: 0.509696\n",
      "Training Epoch: 2 [13024/60000 (22%)]\tLoss: 0.518941\n",
      "Training Epoch: 2 [13056/60000 (22%)]\tLoss: 0.476905\n",
      "Training Epoch: 2 [13088/60000 (22%)]\tLoss: 0.378392\n",
      "Training Epoch: 2 [13120/60000 (22%)]\tLoss: 0.471376\n",
      "Training Epoch: 2 [13152/60000 (22%)]\tLoss: 0.426022\n",
      "Training Epoch: 2 [13184/60000 (22%)]\tLoss: 0.448351\n",
      "Training Epoch: 2 [13216/60000 (22%)]\tLoss: 0.374407\n",
      "Training Epoch: 2 [13248/60000 (22%)]\tLoss: 1.031466\n",
      "Training Epoch: 2 [13280/60000 (22%)]\tLoss: 0.512920\n",
      "Training Epoch: 2 [13312/60000 (22%)]\tLoss: 0.463400\n",
      "Training Epoch: 2 [13344/60000 (22%)]\tLoss: 0.683496\n",
      "Training Epoch: 2 [13376/60000 (22%)]\tLoss: 0.373136\n",
      "Training Epoch: 2 [13408/60000 (22%)]\tLoss: 0.876221\n",
      "Training Epoch: 2 [13440/60000 (22%)]\tLoss: 0.435986\n",
      "Training Epoch: 2 [13472/60000 (22%)]\tLoss: 0.455525\n",
      "Training Epoch: 2 [13504/60000 (23%)]\tLoss: 0.737038\n",
      "Training Epoch: 2 [13536/60000 (23%)]\tLoss: 0.579539\n",
      "Training Epoch: 2 [13568/60000 (23%)]\tLoss: 0.530674\n",
      "Training Epoch: 2 [13600/60000 (23%)]\tLoss: 0.551024\n",
      "Training Epoch: 2 [13632/60000 (23%)]\tLoss: 0.558641\n",
      "Training Epoch: 2 [13664/60000 (23%)]\tLoss: 0.473347\n",
      "Training Epoch: 2 [13696/60000 (23%)]\tLoss: 0.364274\n",
      "Training Epoch: 2 [13728/60000 (23%)]\tLoss: 0.469628\n",
      "Training Epoch: 2 [13760/60000 (23%)]\tLoss: 0.898514\n",
      "Training Epoch: 2 [13792/60000 (23%)]\tLoss: 0.425166\n",
      "Training Epoch: 2 [13824/60000 (23%)]\tLoss: 0.371116\n",
      "Training Epoch: 2 [13856/60000 (23%)]\tLoss: 0.637222\n",
      "Training Epoch: 2 [13888/60000 (23%)]\tLoss: 0.560082\n",
      "Training Epoch: 2 [13920/60000 (23%)]\tLoss: 0.688145\n",
      "Training Epoch: 2 [13952/60000 (23%)]\tLoss: 0.671981\n",
      "Training Epoch: 2 [13984/60000 (23%)]\tLoss: 0.608883\n",
      "Training Epoch: 2 [14016/60000 (23%)]\tLoss: 0.505533\n",
      "Training Epoch: 2 [14048/60000 (23%)]\tLoss: 0.588548\n",
      "Training Epoch: 2 [14080/60000 (23%)]\tLoss: 0.602041\n",
      "Training Epoch: 2 [14112/60000 (24%)]\tLoss: 0.593190\n",
      "Training Epoch: 2 [14144/60000 (24%)]\tLoss: 0.526652\n",
      "Training Epoch: 2 [14176/60000 (24%)]\tLoss: 0.759457\n",
      "Training Epoch: 2 [14208/60000 (24%)]\tLoss: 0.581154\n",
      "Training Epoch: 2 [14240/60000 (24%)]\tLoss: 0.306567\n",
      "Training Epoch: 2 [14272/60000 (24%)]\tLoss: 0.666876\n",
      "Training Epoch: 2 [14304/60000 (24%)]\tLoss: 0.353951\n",
      "Training Epoch: 2 [14336/60000 (24%)]\tLoss: 0.367869\n",
      "Training Epoch: 2 [14368/60000 (24%)]\tLoss: 0.387150\n",
      "Training Epoch: 2 [14400/60000 (24%)]\tLoss: 0.838738\n",
      "Training Epoch: 2 [14432/60000 (24%)]\tLoss: 0.559332\n",
      "Training Epoch: 2 [14464/60000 (24%)]\tLoss: 0.527946\n",
      "Training Epoch: 2 [14496/60000 (24%)]\tLoss: 0.403927\n",
      "Training Epoch: 2 [14528/60000 (24%)]\tLoss: 0.364905\n",
      "Training Epoch: 2 [14560/60000 (24%)]\tLoss: 0.284770\n",
      "Training Epoch: 2 [14592/60000 (24%)]\tLoss: 0.439398\n",
      "Training Epoch: 2 [14624/60000 (24%)]\tLoss: 0.627260\n",
      "Training Epoch: 2 [14656/60000 (24%)]\tLoss: 0.359463\n",
      "Training Epoch: 2 [14688/60000 (24%)]\tLoss: 0.360890\n",
      "Training Epoch: 2 [14720/60000 (25%)]\tLoss: 0.556370\n",
      "Training Epoch: 2 [14752/60000 (25%)]\tLoss: 0.914377\n",
      "Training Epoch: 2 [14784/60000 (25%)]\tLoss: 0.387446\n",
      "Training Epoch: 2 [14816/60000 (25%)]\tLoss: 0.879831\n",
      "Training Epoch: 2 [14848/60000 (25%)]\tLoss: 0.499608\n",
      "Training Epoch: 2 [14880/60000 (25%)]\tLoss: 0.502879\n",
      "Training Epoch: 2 [14912/60000 (25%)]\tLoss: 0.966953\n",
      "Training Epoch: 2 [14944/60000 (25%)]\tLoss: 0.679539\n",
      "Training Epoch: 2 [14976/60000 (25%)]\tLoss: 0.678594\n",
      "Training Epoch: 2 [15008/60000 (25%)]\tLoss: 0.445560\n",
      "Training Epoch: 2 [15040/60000 (25%)]\tLoss: 0.307297\n",
      "Training Epoch: 2 [15072/60000 (25%)]\tLoss: 0.277712\n",
      "Training Epoch: 2 [15104/60000 (25%)]\tLoss: 0.449822\n",
      "Training Epoch: 2 [15136/60000 (25%)]\tLoss: 0.299143\n",
      "Training Epoch: 2 [15168/60000 (25%)]\tLoss: 0.632476\n",
      "Training Epoch: 2 [15200/60000 (25%)]\tLoss: 0.556858\n",
      "Training Epoch: 2 [15232/60000 (25%)]\tLoss: 0.606934\n",
      "Training Epoch: 2 [15264/60000 (25%)]\tLoss: 0.646322\n",
      "Training Epoch: 2 [15296/60000 (25%)]\tLoss: 0.399044\n",
      "Training Epoch: 2 [15328/60000 (26%)]\tLoss: 0.612082\n",
      "Training Epoch: 2 [15360/60000 (26%)]\tLoss: 0.414773\n",
      "Training Epoch: 2 [15392/60000 (26%)]\tLoss: 0.557373\n",
      "Training Epoch: 2 [15424/60000 (26%)]\tLoss: 0.449852\n",
      "Training Epoch: 2 [15456/60000 (26%)]\tLoss: 0.255089\n",
      "Training Epoch: 2 [15488/60000 (26%)]\tLoss: 0.410680\n",
      "Training Epoch: 2 [15520/60000 (26%)]\tLoss: 0.641985\n",
      "Training Epoch: 2 [15552/60000 (26%)]\tLoss: 0.189309\n",
      "Training Epoch: 2 [15584/60000 (26%)]\tLoss: 0.525540\n",
      "Training Epoch: 2 [15616/60000 (26%)]\tLoss: 0.382099\n",
      "Training Epoch: 2 [15648/60000 (26%)]\tLoss: 0.659105\n",
      "Training Epoch: 2 [15680/60000 (26%)]\tLoss: 0.830660\n",
      "Training Epoch: 2 [15712/60000 (26%)]\tLoss: 0.401734\n",
      "Training Epoch: 2 [15744/60000 (26%)]\tLoss: 0.165292\n",
      "Training Epoch: 2 [15776/60000 (26%)]\tLoss: 0.706074\n",
      "Training Epoch: 2 [15808/60000 (26%)]\tLoss: 0.426135\n",
      "Training Epoch: 2 [15840/60000 (26%)]\tLoss: 0.658867\n",
      "Training Epoch: 2 [15872/60000 (26%)]\tLoss: 0.334999\n",
      "Training Epoch: 2 [15904/60000 (27%)]\tLoss: 0.649372\n",
      "Training Epoch: 2 [15936/60000 (27%)]\tLoss: 0.675240\n",
      "Training Epoch: 2 [15968/60000 (27%)]\tLoss: 0.417086\n",
      "Training Epoch: 2 [16000/60000 (27%)]\tLoss: 0.234081\n",
      "Training Epoch: 2 [16032/60000 (27%)]\tLoss: 0.589281\n",
      "Training Epoch: 2 [16064/60000 (27%)]\tLoss: 0.831796\n",
      "Training Epoch: 2 [16096/60000 (27%)]\tLoss: 0.509381\n",
      "Training Epoch: 2 [16128/60000 (27%)]\tLoss: 0.258563\n",
      "Training Epoch: 2 [16160/60000 (27%)]\tLoss: 0.426748\n",
      "Training Epoch: 2 [16192/60000 (27%)]\tLoss: 0.721810\n",
      "Training Epoch: 2 [16224/60000 (27%)]\tLoss: 0.410503\n",
      "Training Epoch: 2 [16256/60000 (27%)]\tLoss: 0.478952\n",
      "Training Epoch: 2 [16288/60000 (27%)]\tLoss: 0.547166\n",
      "Training Epoch: 2 [16320/60000 (27%)]\tLoss: 0.319888\n",
      "Training Epoch: 2 [16352/60000 (27%)]\tLoss: 0.625693\n",
      "Training Epoch: 2 [16384/60000 (27%)]\tLoss: 0.371988\n",
      "Training Epoch: 2 [16416/60000 (27%)]\tLoss: 0.461853\n",
      "Training Epoch: 2 [16448/60000 (27%)]\tLoss: 0.416167\n",
      "Training Epoch: 2 [16480/60000 (27%)]\tLoss: 0.365416\n",
      "Training Epoch: 2 [16512/60000 (28%)]\tLoss: 0.982814\n",
      "Training Epoch: 2 [16544/60000 (28%)]\tLoss: 0.552340\n",
      "Training Epoch: 2 [16576/60000 (28%)]\tLoss: 0.591831\n",
      "Training Epoch: 2 [16608/60000 (28%)]\tLoss: 0.179195\n",
      "Training Epoch: 2 [16640/60000 (28%)]\tLoss: 0.523811\n",
      "Training Epoch: 2 [16672/60000 (28%)]\tLoss: 0.652335\n",
      "Training Epoch: 2 [16704/60000 (28%)]\tLoss: 0.706821\n",
      "Training Epoch: 2 [16736/60000 (28%)]\tLoss: 1.192587\n",
      "Training Epoch: 2 [16768/60000 (28%)]\tLoss: 0.522782\n",
      "Training Epoch: 2 [16800/60000 (28%)]\tLoss: 0.269565\n",
      "Training Epoch: 2 [16832/60000 (28%)]\tLoss: 0.324295\n",
      "Training Epoch: 2 [16864/60000 (28%)]\tLoss: 0.728617\n",
      "Training Epoch: 2 [16896/60000 (28%)]\tLoss: 0.563221\n",
      "Training Epoch: 2 [16928/60000 (28%)]\tLoss: 0.333851\n",
      "Training Epoch: 2 [16960/60000 (28%)]\tLoss: 0.315787\n",
      "Training Epoch: 2 [16992/60000 (28%)]\tLoss: 0.717951\n",
      "Training Epoch: 2 [17024/60000 (28%)]\tLoss: 0.233367\n",
      "Training Epoch: 2 [17056/60000 (28%)]\tLoss: 0.498837\n",
      "Training Epoch: 2 [17088/60000 (28%)]\tLoss: 0.420559\n",
      "Training Epoch: 2 [17120/60000 (29%)]\tLoss: 0.468012\n",
      "Training Epoch: 2 [17152/60000 (29%)]\tLoss: 0.621671\n",
      "Training Epoch: 2 [17184/60000 (29%)]\tLoss: 0.743728\n",
      "Training Epoch: 2 [17216/60000 (29%)]\tLoss: 0.264876\n",
      "Training Epoch: 2 [17248/60000 (29%)]\tLoss: 0.392791\n",
      "Training Epoch: 2 [17280/60000 (29%)]\tLoss: 0.850333\n",
      "Training Epoch: 2 [17312/60000 (29%)]\tLoss: 0.465433\n",
      "Training Epoch: 2 [17344/60000 (29%)]\tLoss: 0.454650\n",
      "Training Epoch: 2 [17376/60000 (29%)]\tLoss: 0.370097\n",
      "Training Epoch: 2 [17408/60000 (29%)]\tLoss: 0.595578\n",
      "Training Epoch: 2 [17440/60000 (29%)]\tLoss: 0.252516\n",
      "Training Epoch: 2 [17472/60000 (29%)]\tLoss: 0.260764\n",
      "Training Epoch: 2 [17504/60000 (29%)]\tLoss: 0.267417\n",
      "Training Epoch: 2 [17536/60000 (29%)]\tLoss: 1.041755\n",
      "Training Epoch: 2 [17568/60000 (29%)]\tLoss: 0.453813\n",
      "Training Epoch: 2 [17600/60000 (29%)]\tLoss: 0.543344\n",
      "Training Epoch: 2 [17632/60000 (29%)]\tLoss: 0.790063\n",
      "Training Epoch: 2 [17664/60000 (29%)]\tLoss: 0.449857\n",
      "Training Epoch: 2 [17696/60000 (29%)]\tLoss: 0.370680\n",
      "Training Epoch: 2 [17728/60000 (30%)]\tLoss: 1.131466\n",
      "Training Epoch: 2 [17760/60000 (30%)]\tLoss: 0.577184\n",
      "Training Epoch: 2 [17792/60000 (30%)]\tLoss: 0.453681\n",
      "Training Epoch: 2 [17824/60000 (30%)]\tLoss: 0.457997\n",
      "Training Epoch: 2 [17856/60000 (30%)]\tLoss: 0.253523\n",
      "Training Epoch: 2 [17888/60000 (30%)]\tLoss: 0.534751\n",
      "Training Epoch: 2 [17920/60000 (30%)]\tLoss: 0.406225\n",
      "Training Epoch: 2 [17952/60000 (30%)]\tLoss: 0.381016\n",
      "Training Epoch: 2 [17984/60000 (30%)]\tLoss: 0.642390\n",
      "Training Epoch: 2 [18016/60000 (30%)]\tLoss: 0.294695\n",
      "Training Epoch: 2 [18048/60000 (30%)]\tLoss: 0.462017\n",
      "Training Epoch: 2 [18080/60000 (30%)]\tLoss: 0.367466\n",
      "Training Epoch: 2 [18112/60000 (30%)]\tLoss: 0.553441\n",
      "Training Epoch: 2 [18144/60000 (30%)]\tLoss: 0.684299\n",
      "Training Epoch: 2 [18176/60000 (30%)]\tLoss: 0.696790\n",
      "Training Epoch: 2 [18208/60000 (30%)]\tLoss: 0.247977\n",
      "Training Epoch: 2 [18240/60000 (30%)]\tLoss: 0.496919\n",
      "Training Epoch: 2 [18272/60000 (30%)]\tLoss: 0.511022\n",
      "Training Epoch: 2 [18304/60000 (31%)]\tLoss: 0.751248\n",
      "Training Epoch: 2 [18336/60000 (31%)]\tLoss: 0.458407\n",
      "Training Epoch: 2 [18368/60000 (31%)]\tLoss: 0.655439\n",
      "Training Epoch: 2 [18400/60000 (31%)]\tLoss: 0.394823\n",
      "Training Epoch: 2 [18432/60000 (31%)]\tLoss: 0.780503\n",
      "Training Epoch: 2 [18464/60000 (31%)]\tLoss: 0.433531\n",
      "Training Epoch: 2 [18496/60000 (31%)]\tLoss: 0.452719\n",
      "Training Epoch: 2 [18528/60000 (31%)]\tLoss: 0.689051\n",
      "Training Epoch: 2 [18560/60000 (31%)]\tLoss: 0.473750\n",
      "Training Epoch: 2 [18592/60000 (31%)]\tLoss: 0.332391\n",
      "Training Epoch: 2 [18624/60000 (31%)]\tLoss: 0.677346\n",
      "Training Epoch: 2 [18656/60000 (31%)]\tLoss: 0.434707\n",
      "Training Epoch: 2 [18688/60000 (31%)]\tLoss: 0.620947\n",
      "Training Epoch: 2 [18720/60000 (31%)]\tLoss: 0.542971\n",
      "Training Epoch: 2 [18752/60000 (31%)]\tLoss: 0.318745\n",
      "Training Epoch: 2 [18784/60000 (31%)]\tLoss: 0.264273\n",
      "Training Epoch: 2 [18816/60000 (31%)]\tLoss: 0.500199\n",
      "Training Epoch: 2 [18848/60000 (31%)]\tLoss: 0.639448\n",
      "Training Epoch: 2 [18880/60000 (31%)]\tLoss: 0.580812\n",
      "Training Epoch: 2 [18912/60000 (32%)]\tLoss: 0.881463\n",
      "Training Epoch: 2 [18944/60000 (32%)]\tLoss: 0.344249\n",
      "Training Epoch: 2 [18976/60000 (32%)]\tLoss: 0.482643\n",
      "Training Epoch: 2 [19008/60000 (32%)]\tLoss: 0.718258\n",
      "Training Epoch: 2 [19040/60000 (32%)]\tLoss: 0.682446\n",
      "Training Epoch: 2 [19072/60000 (32%)]\tLoss: 0.894249\n",
      "Training Epoch: 2 [19104/60000 (32%)]\tLoss: 0.432845\n",
      "Training Epoch: 2 [19136/60000 (32%)]\tLoss: 0.580411\n",
      "Training Epoch: 2 [19168/60000 (32%)]\tLoss: 0.340311\n",
      "Training Epoch: 2 [19200/60000 (32%)]\tLoss: 0.444518\n",
      "Training Epoch: 2 [19232/60000 (32%)]\tLoss: 0.599483\n",
      "Training Epoch: 2 [19264/60000 (32%)]\tLoss: 0.322139\n",
      "Training Epoch: 2 [19296/60000 (32%)]\tLoss: 0.453632\n",
      "Training Epoch: 2 [19328/60000 (32%)]\tLoss: 0.206817\n",
      "Training Epoch: 2 [19360/60000 (32%)]\tLoss: 0.581538\n",
      "Training Epoch: 2 [19392/60000 (32%)]\tLoss: 0.665832\n",
      "Training Epoch: 2 [19424/60000 (32%)]\tLoss: 0.676901\n",
      "Training Epoch: 2 [19456/60000 (32%)]\tLoss: 1.147293\n",
      "Training Epoch: 2 [19488/60000 (32%)]\tLoss: 0.220061\n",
      "Training Epoch: 2 [19520/60000 (33%)]\tLoss: 0.684755\n",
      "Training Epoch: 2 [19552/60000 (33%)]\tLoss: 0.458367\n",
      "Training Epoch: 2 [19584/60000 (33%)]\tLoss: 0.710825\n",
      "Training Epoch: 2 [19616/60000 (33%)]\tLoss: 0.245744\n",
      "Training Epoch: 2 [19648/60000 (33%)]\tLoss: 0.239257\n",
      "Training Epoch: 2 [19680/60000 (33%)]\tLoss: 0.512968\n",
      "Training Epoch: 2 [19712/60000 (33%)]\tLoss: 0.983127\n",
      "Training Epoch: 2 [19744/60000 (33%)]\tLoss: 0.352997\n",
      "Training Epoch: 2 [19776/60000 (33%)]\tLoss: 0.577585\n",
      "Training Epoch: 2 [19808/60000 (33%)]\tLoss: 0.585824\n",
      "Training Epoch: 2 [19840/60000 (33%)]\tLoss: 0.333612\n",
      "Training Epoch: 2 [19872/60000 (33%)]\tLoss: 0.134921\n",
      "Training Epoch: 2 [19904/60000 (33%)]\tLoss: 0.368259\n",
      "Training Epoch: 2 [19936/60000 (33%)]\tLoss: 0.314575\n",
      "Training Epoch: 2 [19968/60000 (33%)]\tLoss: 0.613145\n",
      "Training Epoch: 2 [20000/60000 (33%)]\tLoss: 0.667781\n",
      "Training Epoch: 2 [20032/60000 (33%)]\tLoss: 0.503546\n",
      "Training Epoch: 2 [20064/60000 (33%)]\tLoss: 1.014647\n",
      "Training Epoch: 2 [20096/60000 (33%)]\tLoss: 0.328174\n",
      "Training Epoch: 2 [20128/60000 (34%)]\tLoss: 0.234044\n",
      "Training Epoch: 2 [20160/60000 (34%)]\tLoss: 0.522396\n",
      "Training Epoch: 2 [20192/60000 (34%)]\tLoss: 0.592914\n",
      "Training Epoch: 2 [20224/60000 (34%)]\tLoss: 0.265551\n",
      "Training Epoch: 2 [20256/60000 (34%)]\tLoss: 0.352243\n",
      "Training Epoch: 2 [20288/60000 (34%)]\tLoss: 0.316831\n",
      "Training Epoch: 2 [20320/60000 (34%)]\tLoss: 0.432166\n",
      "Training Epoch: 2 [20352/60000 (34%)]\tLoss: 0.401786\n",
      "Training Epoch: 2 [20384/60000 (34%)]\tLoss: 0.080416\n",
      "Training Epoch: 2 [20416/60000 (34%)]\tLoss: 0.716420\n",
      "Training Epoch: 2 [20448/60000 (34%)]\tLoss: 0.362522\n",
      "Training Epoch: 2 [20480/60000 (34%)]\tLoss: 0.286106\n",
      "Training Epoch: 2 [20512/60000 (34%)]\tLoss: 0.554589\n",
      "Training Epoch: 2 [20544/60000 (34%)]\tLoss: 0.935770\n",
      "Training Epoch: 2 [20576/60000 (34%)]\tLoss: 0.688678\n",
      "Training Epoch: 2 [20608/60000 (34%)]\tLoss: 0.591100\n",
      "Training Epoch: 2 [20640/60000 (34%)]\tLoss: 0.463765\n",
      "Training Epoch: 2 [20672/60000 (34%)]\tLoss: 0.665424\n",
      "Training Epoch: 2 [20704/60000 (35%)]\tLoss: 0.341260\n",
      "Training Epoch: 2 [20736/60000 (35%)]\tLoss: 0.273115\n",
      "Training Epoch: 2 [20768/60000 (35%)]\tLoss: 0.687292\n",
      "Training Epoch: 2 [20800/60000 (35%)]\tLoss: 0.530189\n",
      "Training Epoch: 2 [20832/60000 (35%)]\tLoss: 0.281740\n",
      "Training Epoch: 2 [20864/60000 (35%)]\tLoss: 0.206885\n",
      "Training Epoch: 2 [20896/60000 (35%)]\tLoss: 0.245328\n",
      "Training Epoch: 2 [20928/60000 (35%)]\tLoss: 0.451358\n",
      "Training Epoch: 2 [20960/60000 (35%)]\tLoss: 0.629304\n",
      "Training Epoch: 2 [20992/60000 (35%)]\tLoss: 0.705498\n",
      "Training Epoch: 2 [21024/60000 (35%)]\tLoss: 0.397478\n",
      "Training Epoch: 2 [21056/60000 (35%)]\tLoss: 0.797943\n",
      "Training Epoch: 2 [21088/60000 (35%)]\tLoss: 0.427247\n",
      "Training Epoch: 2 [21120/60000 (35%)]\tLoss: 0.311238\n",
      "Training Epoch: 2 [21152/60000 (35%)]\tLoss: 0.426943\n",
      "Training Epoch: 2 [21184/60000 (35%)]\tLoss: 0.339792\n",
      "Training Epoch: 2 [21216/60000 (35%)]\tLoss: 0.264406\n",
      "Training Epoch: 2 [21248/60000 (35%)]\tLoss: 0.364884\n",
      "Training Epoch: 2 [21280/60000 (35%)]\tLoss: 0.309667\n",
      "Training Epoch: 2 [21312/60000 (36%)]\tLoss: 0.405872\n",
      "Training Epoch: 2 [21344/60000 (36%)]\tLoss: 0.511663\n",
      "Training Epoch: 2 [21376/60000 (36%)]\tLoss: 0.406232\n",
      "Training Epoch: 2 [21408/60000 (36%)]\tLoss: 0.244460\n",
      "Training Epoch: 2 [21440/60000 (36%)]\tLoss: 0.696179\n",
      "Training Epoch: 2 [21472/60000 (36%)]\tLoss: 0.334244\n",
      "Training Epoch: 2 [21504/60000 (36%)]\tLoss: 0.317877\n",
      "Training Epoch: 2 [21536/60000 (36%)]\tLoss: 0.339791\n",
      "Training Epoch: 2 [21568/60000 (36%)]\tLoss: 0.559084\n",
      "Training Epoch: 2 [21600/60000 (36%)]\tLoss: 0.524480\n",
      "Training Epoch: 2 [21632/60000 (36%)]\tLoss: 0.279125\n",
      "Training Epoch: 2 [21664/60000 (36%)]\tLoss: 0.361218\n",
      "Training Epoch: 2 [21696/60000 (36%)]\tLoss: 0.729569\n",
      "Training Epoch: 2 [21728/60000 (36%)]\tLoss: 0.462277\n",
      "Training Epoch: 2 [21760/60000 (36%)]\tLoss: 0.311181\n",
      "Training Epoch: 2 [21792/60000 (36%)]\tLoss: 0.399574\n",
      "Training Epoch: 2 [21824/60000 (36%)]\tLoss: 0.779335\n",
      "Training Epoch: 2 [21856/60000 (36%)]\tLoss: 0.220375\n",
      "Training Epoch: 2 [21888/60000 (36%)]\tLoss: 0.541901\n",
      "Training Epoch: 2 [21920/60000 (37%)]\tLoss: 0.583433\n",
      "Training Epoch: 2 [21952/60000 (37%)]\tLoss: 0.484171\n",
      "Training Epoch: 2 [21984/60000 (37%)]\tLoss: 0.337273\n",
      "Training Epoch: 2 [22016/60000 (37%)]\tLoss: 0.479692\n",
      "Training Epoch: 2 [22048/60000 (37%)]\tLoss: 0.358627\n",
      "Training Epoch: 2 [22080/60000 (37%)]\tLoss: 0.663311\n",
      "Training Epoch: 2 [22112/60000 (37%)]\tLoss: 0.579343\n",
      "Training Epoch: 2 [22144/60000 (37%)]\tLoss: 0.367798\n",
      "Training Epoch: 2 [22176/60000 (37%)]\tLoss: 0.475490\n",
      "Training Epoch: 2 [22208/60000 (37%)]\tLoss: 0.558748\n",
      "Training Epoch: 2 [22240/60000 (37%)]\tLoss: 0.551041\n",
      "Training Epoch: 2 [22272/60000 (37%)]\tLoss: 0.896135\n",
      "Training Epoch: 2 [22304/60000 (37%)]\tLoss: 0.365894\n",
      "Training Epoch: 2 [22336/60000 (37%)]\tLoss: 0.495935\n",
      "Training Epoch: 2 [22368/60000 (37%)]\tLoss: 0.432465\n",
      "Training Epoch: 2 [22400/60000 (37%)]\tLoss: 0.563651\n",
      "Training Epoch: 2 [22432/60000 (37%)]\tLoss: 0.624426\n",
      "Training Epoch: 2 [22464/60000 (37%)]\tLoss: 0.653638\n",
      "Training Epoch: 2 [22496/60000 (37%)]\tLoss: 0.111837\n",
      "Training Epoch: 2 [22528/60000 (38%)]\tLoss: 0.497093\n",
      "Training Epoch: 2 [22560/60000 (38%)]\tLoss: 0.325513\n",
      "Training Epoch: 2 [22592/60000 (38%)]\tLoss: 0.601981\n",
      "Training Epoch: 2 [22624/60000 (38%)]\tLoss: 0.420180\n",
      "Training Epoch: 2 [22656/60000 (38%)]\tLoss: 0.274508\n",
      "Training Epoch: 2 [22688/60000 (38%)]\tLoss: 0.573630\n",
      "Training Epoch: 2 [22720/60000 (38%)]\tLoss: 0.303992\n",
      "Training Epoch: 2 [22752/60000 (38%)]\tLoss: 0.555460\n",
      "Training Epoch: 2 [22784/60000 (38%)]\tLoss: 0.354486\n",
      "Training Epoch: 2 [22816/60000 (38%)]\tLoss: 0.427109\n",
      "Training Epoch: 2 [22848/60000 (38%)]\tLoss: 0.455238\n",
      "Training Epoch: 2 [22880/60000 (38%)]\tLoss: 0.415829\n",
      "Training Epoch: 2 [22912/60000 (38%)]\tLoss: 0.328493\n",
      "Training Epoch: 2 [22944/60000 (38%)]\tLoss: 0.510423\n",
      "Training Epoch: 2 [22976/60000 (38%)]\tLoss: 0.347857\n",
      "Training Epoch: 2 [23008/60000 (38%)]\tLoss: 0.731779\n",
      "Training Epoch: 2 [23040/60000 (38%)]\tLoss: 0.377863\n",
      "Training Epoch: 2 [23072/60000 (38%)]\tLoss: 0.772055\n",
      "Training Epoch: 2 [23104/60000 (39%)]\tLoss: 0.287061\n",
      "Training Epoch: 2 [23136/60000 (39%)]\tLoss: 0.490794\n",
      "Training Epoch: 2 [23168/60000 (39%)]\tLoss: 0.335617\n",
      "Training Epoch: 2 [23200/60000 (39%)]\tLoss: 0.712350\n",
      "Training Epoch: 2 [23232/60000 (39%)]\tLoss: 0.735581\n",
      "Training Epoch: 2 [23264/60000 (39%)]\tLoss: 0.181199\n",
      "Training Epoch: 2 [23296/60000 (39%)]\tLoss: 0.535175\n",
      "Training Epoch: 2 [23328/60000 (39%)]\tLoss: 0.151137\n",
      "Training Epoch: 2 [23360/60000 (39%)]\tLoss: 0.511736\n",
      "Training Epoch: 2 [23392/60000 (39%)]\tLoss: 0.252766\n",
      "Training Epoch: 2 [23424/60000 (39%)]\tLoss: 1.190532\n",
      "Training Epoch: 2 [23456/60000 (39%)]\tLoss: 0.590078\n",
      "Training Epoch: 2 [23488/60000 (39%)]\tLoss: 0.259700\n",
      "Training Epoch: 2 [23520/60000 (39%)]\tLoss: 0.681579\n",
      "Training Epoch: 2 [23552/60000 (39%)]\tLoss: 0.344250\n",
      "Training Epoch: 2 [23584/60000 (39%)]\tLoss: 0.301401\n",
      "Training Epoch: 2 [23616/60000 (39%)]\tLoss: 0.560725\n",
      "Training Epoch: 2 [23648/60000 (39%)]\tLoss: 0.307730\n",
      "Training Epoch: 2 [23680/60000 (39%)]\tLoss: 0.442143\n",
      "Training Epoch: 2 [23712/60000 (40%)]\tLoss: 0.384062\n",
      "Training Epoch: 2 [23744/60000 (40%)]\tLoss: 0.308217\n",
      "Training Epoch: 2 [23776/60000 (40%)]\tLoss: 0.231797\n",
      "Training Epoch: 2 [23808/60000 (40%)]\tLoss: 0.548531\n",
      "Training Epoch: 2 [23840/60000 (40%)]\tLoss: 0.617560\n",
      "Training Epoch: 2 [23872/60000 (40%)]\tLoss: 0.762413\n",
      "Training Epoch: 2 [23904/60000 (40%)]\tLoss: 0.638287\n",
      "Training Epoch: 2 [23936/60000 (40%)]\tLoss: 0.557097\n",
      "Training Epoch: 2 [23968/60000 (40%)]\tLoss: 0.360488\n",
      "Training Epoch: 2 [24000/60000 (40%)]\tLoss: 0.418486\n",
      "Training Epoch: 2 [24032/60000 (40%)]\tLoss: 0.183652\n",
      "Training Epoch: 2 [24064/60000 (40%)]\tLoss: 0.952781\n",
      "Training Epoch: 2 [24096/60000 (40%)]\tLoss: 0.263547\n",
      "Training Epoch: 2 [24128/60000 (40%)]\tLoss: 0.277487\n",
      "Training Epoch: 2 [24160/60000 (40%)]\tLoss: 0.918895\n",
      "Training Epoch: 2 [24192/60000 (40%)]\tLoss: 0.498171\n",
      "Training Epoch: 2 [24224/60000 (40%)]\tLoss: 0.270310\n",
      "Training Epoch: 2 [24256/60000 (40%)]\tLoss: 0.373503\n",
      "Training Epoch: 2 [24288/60000 (40%)]\tLoss: 0.509573\n",
      "Training Epoch: 2 [24320/60000 (41%)]\tLoss: 0.267195\n",
      "Training Epoch: 2 [24352/60000 (41%)]\tLoss: 0.200846\n",
      "Training Epoch: 2 [24384/60000 (41%)]\tLoss: 0.477585\n",
      "Training Epoch: 2 [24416/60000 (41%)]\tLoss: 0.194993\n",
      "Training Epoch: 2 [24448/60000 (41%)]\tLoss: 0.395144\n",
      "Training Epoch: 2 [24480/60000 (41%)]\tLoss: 0.346919\n",
      "Training Epoch: 2 [24512/60000 (41%)]\tLoss: 0.442980\n",
      "Training Epoch: 2 [24544/60000 (41%)]\tLoss: 0.524522\n",
      "Training Epoch: 2 [24576/60000 (41%)]\tLoss: 0.340568\n",
      "Training Epoch: 2 [24608/60000 (41%)]\tLoss: 0.362799\n",
      "Training Epoch: 2 [24640/60000 (41%)]\tLoss: 0.789123\n",
      "Training Epoch: 2 [24672/60000 (41%)]\tLoss: 0.184829\n",
      "Training Epoch: 2 [24704/60000 (41%)]\tLoss: 0.314670\n",
      "Training Epoch: 2 [24736/60000 (41%)]\tLoss: 0.270447\n",
      "Training Epoch: 2 [24768/60000 (41%)]\tLoss: 0.417943\n",
      "Training Epoch: 2 [24800/60000 (41%)]\tLoss: 0.119835\n",
      "Training Epoch: 2 [24832/60000 (41%)]\tLoss: 0.500330\n",
      "Training Epoch: 2 [24864/60000 (41%)]\tLoss: 0.393501\n",
      "Training Epoch: 2 [24896/60000 (41%)]\tLoss: 0.552168\n",
      "Training Epoch: 2 [24928/60000 (42%)]\tLoss: 0.441662\n",
      "Training Epoch: 2 [24960/60000 (42%)]\tLoss: 0.402924\n",
      "Training Epoch: 2 [24992/60000 (42%)]\tLoss: 0.361815\n",
      "Training Epoch: 2 [25024/60000 (42%)]\tLoss: 0.175046\n",
      "Training Epoch: 2 [25056/60000 (42%)]\tLoss: 0.340164\n",
      "Training Epoch: 2 [25088/60000 (42%)]\tLoss: 0.377878\n",
      "Training Epoch: 2 [25120/60000 (42%)]\tLoss: 0.663496\n",
      "Training Epoch: 2 [25152/60000 (42%)]\tLoss: 0.308189\n",
      "Training Epoch: 2 [25184/60000 (42%)]\tLoss: 0.380430\n",
      "Training Epoch: 2 [25216/60000 (42%)]\tLoss: 0.509224\n",
      "Training Epoch: 2 [25248/60000 (42%)]\tLoss: 0.532521\n",
      "Training Epoch: 2 [25280/60000 (42%)]\tLoss: 0.399231\n",
      "Training Epoch: 2 [25312/60000 (42%)]\tLoss: 0.400064\n",
      "Training Epoch: 2 [25344/60000 (42%)]\tLoss: 0.293732\n",
      "Training Epoch: 2 [25376/60000 (42%)]\tLoss: 0.466908\n",
      "Training Epoch: 2 [25408/60000 (42%)]\tLoss: 0.485002\n",
      "Training Epoch: 2 [25440/60000 (42%)]\tLoss: 0.317308\n",
      "Training Epoch: 2 [25472/60000 (42%)]\tLoss: 0.341655\n",
      "Training Epoch: 2 [25504/60000 (43%)]\tLoss: 0.329896\n",
      "Training Epoch: 2 [25536/60000 (43%)]\tLoss: 0.370086\n",
      "Training Epoch: 2 [25568/60000 (43%)]\tLoss: 0.380253\n",
      "Training Epoch: 2 [25600/60000 (43%)]\tLoss: 0.489248\n",
      "Training Epoch: 2 [25632/60000 (43%)]\tLoss: 0.704898\n",
      "Training Epoch: 2 [25664/60000 (43%)]\tLoss: 0.381606\n",
      "Training Epoch: 2 [25696/60000 (43%)]\tLoss: 0.233885\n",
      "Training Epoch: 2 [25728/60000 (43%)]\tLoss: 0.264995\n",
      "Training Epoch: 2 [25760/60000 (43%)]\tLoss: 0.406249\n",
      "Training Epoch: 2 [25792/60000 (43%)]\tLoss: 0.328871\n",
      "Training Epoch: 2 [25824/60000 (43%)]\tLoss: 0.423509\n",
      "Training Epoch: 2 [25856/60000 (43%)]\tLoss: 0.585583\n",
      "Training Epoch: 2 [25888/60000 (43%)]\tLoss: 0.559422\n",
      "Training Epoch: 2 [25920/60000 (43%)]\tLoss: 0.521053\n",
      "Training Epoch: 2 [25952/60000 (43%)]\tLoss: 0.305677\n",
      "Training Epoch: 2 [25984/60000 (43%)]\tLoss: 0.575043\n",
      "Training Epoch: 2 [26016/60000 (43%)]\tLoss: 0.313674\n",
      "Training Epoch: 2 [26048/60000 (43%)]\tLoss: 0.883164\n",
      "Training Epoch: 2 [26080/60000 (43%)]\tLoss: 0.250913\n",
      "Training Epoch: 2 [26112/60000 (44%)]\tLoss: 0.438011\n",
      "Training Epoch: 2 [26144/60000 (44%)]\tLoss: 0.402256\n",
      "Training Epoch: 2 [26176/60000 (44%)]\tLoss: 0.248464\n",
      "Training Epoch: 2 [26208/60000 (44%)]\tLoss: 0.135980\n",
      "Training Epoch: 2 [26240/60000 (44%)]\tLoss: 0.511026\n",
      "Training Epoch: 2 [26272/60000 (44%)]\tLoss: 0.296895\n",
      "Training Epoch: 2 [26304/60000 (44%)]\tLoss: 0.526064\n",
      "Training Epoch: 2 [26336/60000 (44%)]\tLoss: 0.224507\n",
      "Training Epoch: 2 [26368/60000 (44%)]\tLoss: 0.431399\n",
      "Training Epoch: 2 [26400/60000 (44%)]\tLoss: 0.216890\n",
      "Training Epoch: 2 [26432/60000 (44%)]\tLoss: 0.469861\n",
      "Training Epoch: 2 [26464/60000 (44%)]\tLoss: 0.288281\n",
      "Training Epoch: 2 [26496/60000 (44%)]\tLoss: 0.494220\n",
      "Training Epoch: 2 [26528/60000 (44%)]\tLoss: 0.556068\n",
      "Training Epoch: 2 [26560/60000 (44%)]\tLoss: 0.382113\n",
      "Training Epoch: 2 [26592/60000 (44%)]\tLoss: 0.289980\n",
      "Training Epoch: 2 [26624/60000 (44%)]\tLoss: 0.666869\n",
      "Training Epoch: 2 [26656/60000 (44%)]\tLoss: 0.416905\n",
      "Training Epoch: 2 [26688/60000 (44%)]\tLoss: 1.127983\n",
      "Training Epoch: 2 [26720/60000 (45%)]\tLoss: 0.227688\n",
      "Training Epoch: 2 [26752/60000 (45%)]\tLoss: 0.390550\n",
      "Training Epoch: 2 [26784/60000 (45%)]\tLoss: 0.426382\n",
      "Training Epoch: 2 [26816/60000 (45%)]\tLoss: 0.497568\n",
      "Training Epoch: 2 [26848/60000 (45%)]\tLoss: 0.340653\n",
      "Training Epoch: 2 [26880/60000 (45%)]\tLoss: 0.578669\n",
      "Training Epoch: 2 [26912/60000 (45%)]\tLoss: 0.133021\n",
      "Training Epoch: 2 [26944/60000 (45%)]\tLoss: 0.524255\n",
      "Training Epoch: 2 [26976/60000 (45%)]\tLoss: 0.518532\n",
      "Training Epoch: 2 [27008/60000 (45%)]\tLoss: 0.506620\n",
      "Training Epoch: 2 [27040/60000 (45%)]\tLoss: 1.138987\n",
      "Training Epoch: 2 [27072/60000 (45%)]\tLoss: 0.422783\n",
      "Training Epoch: 2 [27104/60000 (45%)]\tLoss: 0.447440\n",
      "Training Epoch: 2 [27136/60000 (45%)]\tLoss: 0.504156\n",
      "Training Epoch: 2 [27168/60000 (45%)]\tLoss: 0.491950\n",
      "Training Epoch: 2 [27200/60000 (45%)]\tLoss: 0.523624\n",
      "Training Epoch: 2 [27232/60000 (45%)]\tLoss: 0.823079\n",
      "Training Epoch: 2 [27264/60000 (45%)]\tLoss: 0.714618\n",
      "Training Epoch: 2 [27296/60000 (45%)]\tLoss: 0.581895\n",
      "Training Epoch: 2 [27328/60000 (46%)]\tLoss: 0.329819\n",
      "Training Epoch: 2 [27360/60000 (46%)]\tLoss: 0.606789\n",
      "Training Epoch: 2 [27392/60000 (46%)]\tLoss: 0.592659\n",
      "Training Epoch: 2 [27424/60000 (46%)]\tLoss: 0.211323\n",
      "Training Epoch: 2 [27456/60000 (46%)]\tLoss: 0.413900\n",
      "Training Epoch: 2 [27488/60000 (46%)]\tLoss: 0.530728\n",
      "Training Epoch: 2 [27520/60000 (46%)]\tLoss: 0.373055\n",
      "Training Epoch: 2 [27552/60000 (46%)]\tLoss: 0.456270\n",
      "Training Epoch: 2 [27584/60000 (46%)]\tLoss: 0.447108\n",
      "Training Epoch: 2 [27616/60000 (46%)]\tLoss: 0.557073\n",
      "Training Epoch: 2 [27648/60000 (46%)]\tLoss: 0.473610\n",
      "Training Epoch: 2 [27680/60000 (46%)]\tLoss: 0.408320\n",
      "Training Epoch: 2 [27712/60000 (46%)]\tLoss: 0.508664\n",
      "Training Epoch: 2 [27744/60000 (46%)]\tLoss: 0.348132\n",
      "Training Epoch: 2 [27776/60000 (46%)]\tLoss: 0.341807\n",
      "Training Epoch: 2 [27808/60000 (46%)]\tLoss: 0.361474\n",
      "Training Epoch: 2 [27840/60000 (46%)]\tLoss: 0.372166\n",
      "Training Epoch: 2 [27872/60000 (46%)]\tLoss: 0.154792\n",
      "Training Epoch: 2 [27904/60000 (47%)]\tLoss: 0.294956\n",
      "Training Epoch: 2 [27936/60000 (47%)]\tLoss: 0.437023\n",
      "Training Epoch: 2 [27968/60000 (47%)]\tLoss: 0.736520\n",
      "Training Epoch: 2 [28000/60000 (47%)]\tLoss: 0.340990\n",
      "Training Epoch: 2 [28032/60000 (47%)]\tLoss: 0.264789\n",
      "Training Epoch: 2 [28064/60000 (47%)]\tLoss: 0.324249\n",
      "Training Epoch: 2 [28096/60000 (47%)]\tLoss: 0.776440\n",
      "Training Epoch: 2 [28128/60000 (47%)]\tLoss: 0.393364\n",
      "Training Epoch: 2 [28160/60000 (47%)]\tLoss: 0.378264\n",
      "Training Epoch: 2 [28192/60000 (47%)]\tLoss: 0.654812\n",
      "Training Epoch: 2 [28224/60000 (47%)]\tLoss: 0.516319\n",
      "Training Epoch: 2 [28256/60000 (47%)]\tLoss: 0.953150\n",
      "Training Epoch: 2 [28288/60000 (47%)]\tLoss: 0.614295\n",
      "Training Epoch: 2 [28320/60000 (47%)]\tLoss: 0.144507\n",
      "Training Epoch: 2 [28352/60000 (47%)]\tLoss: 0.577153\n",
      "Training Epoch: 2 [28384/60000 (47%)]\tLoss: 0.370148\n",
      "Training Epoch: 2 [28416/60000 (47%)]\tLoss: 0.333758\n",
      "Training Epoch: 2 [28448/60000 (47%)]\tLoss: 0.770593\n",
      "Training Epoch: 2 [28480/60000 (47%)]\tLoss: 0.457140\n",
      "Training Epoch: 2 [28512/60000 (48%)]\tLoss: 0.199389\n",
      "Training Epoch: 2 [28544/60000 (48%)]\tLoss: 0.626007\n",
      "Training Epoch: 2 [28576/60000 (48%)]\tLoss: 0.336390\n",
      "Training Epoch: 2 [28608/60000 (48%)]\tLoss: 1.003568\n",
      "Training Epoch: 2 [28640/60000 (48%)]\tLoss: 0.197835\n",
      "Training Epoch: 2 [28672/60000 (48%)]\tLoss: 0.596611\n",
      "Training Epoch: 2 [28704/60000 (48%)]\tLoss: 0.306575\n",
      "Training Epoch: 2 [28736/60000 (48%)]\tLoss: 0.355544\n",
      "Training Epoch: 2 [28768/60000 (48%)]\tLoss: 0.644928\n",
      "Training Epoch: 2 [28800/60000 (48%)]\tLoss: 0.416036\n",
      "Training Epoch: 2 [28832/60000 (48%)]\tLoss: 0.549549\n",
      "Training Epoch: 2 [28864/60000 (48%)]\tLoss: 0.682556\n",
      "Training Epoch: 2 [28896/60000 (48%)]\tLoss: 0.084476\n",
      "Training Epoch: 2 [28928/60000 (48%)]\tLoss: 0.564873\n",
      "Training Epoch: 2 [28960/60000 (48%)]\tLoss: 0.360036\n",
      "Training Epoch: 2 [28992/60000 (48%)]\tLoss: 0.299011\n",
      "Training Epoch: 2 [29024/60000 (48%)]\tLoss: 0.452256\n",
      "Training Epoch: 2 [29056/60000 (48%)]\tLoss: 0.189585\n",
      "Training Epoch: 2 [29088/60000 (48%)]\tLoss: 0.378233\n",
      "Training Epoch: 2 [29120/60000 (49%)]\tLoss: 0.299717\n",
      "Training Epoch: 2 [29152/60000 (49%)]\tLoss: 0.587982\n",
      "Training Epoch: 2 [29184/60000 (49%)]\tLoss: 0.151199\n",
      "Training Epoch: 2 [29216/60000 (49%)]\tLoss: 0.383424\n",
      "Training Epoch: 2 [29248/60000 (49%)]\tLoss: 0.504631\n",
      "Training Epoch: 2 [29280/60000 (49%)]\tLoss: 0.397480\n",
      "Training Epoch: 2 [29312/60000 (49%)]\tLoss: 0.694398\n",
      "Training Epoch: 2 [29344/60000 (49%)]\tLoss: 0.233842\n",
      "Training Epoch: 2 [29376/60000 (49%)]\tLoss: 0.636369\n",
      "Training Epoch: 2 [29408/60000 (49%)]\tLoss: 0.273365\n",
      "Training Epoch: 2 [29440/60000 (49%)]\tLoss: 0.526397\n",
      "Training Epoch: 2 [29472/60000 (49%)]\tLoss: 1.050238\n",
      "Training Epoch: 2 [29504/60000 (49%)]\tLoss: 0.313939\n",
      "Training Epoch: 2 [29536/60000 (49%)]\tLoss: 0.472488\n",
      "Training Epoch: 2 [29568/60000 (49%)]\tLoss: 0.471105\n",
      "Training Epoch: 2 [29600/60000 (49%)]\tLoss: 0.473048\n",
      "Training Epoch: 2 [29632/60000 (49%)]\tLoss: 0.341499\n",
      "Training Epoch: 2 [29664/60000 (49%)]\tLoss: 0.223148\n",
      "Training Epoch: 2 [29696/60000 (49%)]\tLoss: 0.239157\n",
      "Training Epoch: 2 [29728/60000 (50%)]\tLoss: 0.533681\n",
      "Training Epoch: 2 [29760/60000 (50%)]\tLoss: 0.303573\n",
      "Training Epoch: 2 [29792/60000 (50%)]\tLoss: 0.507524\n",
      "Training Epoch: 2 [29824/60000 (50%)]\tLoss: 0.562038\n",
      "Training Epoch: 2 [29856/60000 (50%)]\tLoss: 0.385679\n",
      "Training Epoch: 2 [29888/60000 (50%)]\tLoss: 0.212963\n",
      "Training Epoch: 2 [29920/60000 (50%)]\tLoss: 0.755758\n",
      "Training Epoch: 2 [29952/60000 (50%)]\tLoss: 0.435442\n",
      "Training Epoch: 2 [29984/60000 (50%)]\tLoss: 0.295365\n",
      "Training Epoch: 2 [30016/60000 (50%)]\tLoss: 0.280115\n",
      "Training Epoch: 2 [30048/60000 (50%)]\tLoss: 0.415509\n",
      "Training Epoch: 2 [30080/60000 (50%)]\tLoss: 0.253347\n",
      "Training Epoch: 2 [30112/60000 (50%)]\tLoss: 0.511101\n",
      "Training Epoch: 2 [30144/60000 (50%)]\tLoss: 0.408636\n",
      "Training Epoch: 2 [30176/60000 (50%)]\tLoss: 0.177855\n",
      "Training Epoch: 2 [30208/60000 (50%)]\tLoss: 0.397317\n",
      "Training Epoch: 2 [30240/60000 (50%)]\tLoss: 0.215538\n",
      "Training Epoch: 2 [30272/60000 (50%)]\tLoss: 0.171806\n",
      "Training Epoch: 2 [30304/60000 (51%)]\tLoss: 0.813100\n",
      "Training Epoch: 2 [30336/60000 (51%)]\tLoss: 0.592060\n",
      "Training Epoch: 2 [30368/60000 (51%)]\tLoss: 0.780338\n",
      "Training Epoch: 2 [30400/60000 (51%)]\tLoss: 0.185398\n",
      "Training Epoch: 2 [30432/60000 (51%)]\tLoss: 0.559066\n",
      "Training Epoch: 2 [30464/60000 (51%)]\tLoss: 0.288842\n",
      "Training Epoch: 2 [30496/60000 (51%)]\tLoss: 0.333998\n",
      "Training Epoch: 2 [30528/60000 (51%)]\tLoss: 0.630365\n",
      "Training Epoch: 2 [30560/60000 (51%)]\tLoss: 0.851729\n",
      "Training Epoch: 2 [30592/60000 (51%)]\tLoss: 0.181626\n",
      "Training Epoch: 2 [30624/60000 (51%)]\tLoss: 0.280858\n",
      "Training Epoch: 2 [30656/60000 (51%)]\tLoss: 0.506661\n",
      "Training Epoch: 2 [30688/60000 (51%)]\tLoss: 0.157389\n",
      "Training Epoch: 2 [30720/60000 (51%)]\tLoss: 0.173713\n",
      "Training Epoch: 2 [30752/60000 (51%)]\tLoss: 0.240817\n",
      "Training Epoch: 2 [30784/60000 (51%)]\tLoss: 0.230589\n",
      "Training Epoch: 2 [30816/60000 (51%)]\tLoss: 0.510075\n",
      "Training Epoch: 2 [30848/60000 (51%)]\tLoss: 0.684460\n",
      "Training Epoch: 2 [30880/60000 (51%)]\tLoss: 0.608350\n",
      "Training Epoch: 2 [30912/60000 (52%)]\tLoss: 0.230345\n",
      "Training Epoch: 2 [30944/60000 (52%)]\tLoss: 0.486189\n",
      "Training Epoch: 2 [30976/60000 (52%)]\tLoss: 0.429736\n",
      "Training Epoch: 2 [31008/60000 (52%)]\tLoss: 0.701066\n",
      "Training Epoch: 2 [31040/60000 (52%)]\tLoss: 0.161691\n",
      "Training Epoch: 2 [31072/60000 (52%)]\tLoss: 0.253556\n",
      "Training Epoch: 2 [31104/60000 (52%)]\tLoss: 0.178523\n",
      "Training Epoch: 2 [31136/60000 (52%)]\tLoss: 0.307811\n",
      "Training Epoch: 2 [31168/60000 (52%)]\tLoss: 0.437594\n",
      "Training Epoch: 2 [31200/60000 (52%)]\tLoss: 0.269124\n",
      "Training Epoch: 2 [31232/60000 (52%)]\tLoss: 0.809439\n",
      "Training Epoch: 2 [31264/60000 (52%)]\tLoss: 0.957877\n",
      "Training Epoch: 2 [31296/60000 (52%)]\tLoss: 0.102281\n",
      "Training Epoch: 2 [31328/60000 (52%)]\tLoss: 0.535993\n",
      "Training Epoch: 2 [31360/60000 (52%)]\tLoss: 0.190657\n",
      "Training Epoch: 2 [31392/60000 (52%)]\tLoss: 0.148667\n",
      "Training Epoch: 2 [31424/60000 (52%)]\tLoss: 0.523767\n",
      "Training Epoch: 2 [31456/60000 (52%)]\tLoss: 0.114303\n",
      "Training Epoch: 2 [31488/60000 (52%)]\tLoss: 0.478862\n",
      "Training Epoch: 2 [31520/60000 (53%)]\tLoss: 0.355301\n",
      "Training Epoch: 2 [31552/60000 (53%)]\tLoss: 0.290692\n",
      "Training Epoch: 2 [31584/60000 (53%)]\tLoss: 0.723223\n",
      "Training Epoch: 2 [31616/60000 (53%)]\tLoss: 0.370486\n",
      "Training Epoch: 2 [31648/60000 (53%)]\tLoss: 0.450095\n",
      "Training Epoch: 2 [31680/60000 (53%)]\tLoss: 0.164362\n",
      "Training Epoch: 2 [31712/60000 (53%)]\tLoss: 0.282454\n",
      "Training Epoch: 2 [31744/60000 (53%)]\tLoss: 0.419336\n",
      "Training Epoch: 2 [31776/60000 (53%)]\tLoss: 0.378600\n",
      "Training Epoch: 2 [31808/60000 (53%)]\tLoss: 0.666460\n",
      "Training Epoch: 2 [31840/60000 (53%)]\tLoss: 0.556344\n",
      "Training Epoch: 2 [31872/60000 (53%)]\tLoss: 0.591816\n",
      "Training Epoch: 2 [31904/60000 (53%)]\tLoss: 0.456235\n",
      "Training Epoch: 2 [31936/60000 (53%)]\tLoss: 0.436248\n",
      "Training Epoch: 2 [31968/60000 (53%)]\tLoss: 0.482667\n",
      "Training Epoch: 2 [32000/60000 (53%)]\tLoss: 0.325189\n",
      "Training Epoch: 2 [32032/60000 (53%)]\tLoss: 0.221019\n",
      "Training Epoch: 2 [32064/60000 (53%)]\tLoss: 0.660935\n",
      "Training Epoch: 2 [32096/60000 (53%)]\tLoss: 0.453843\n",
      "Training Epoch: 2 [32128/60000 (54%)]\tLoss: 0.556118\n",
      "Training Epoch: 2 [32160/60000 (54%)]\tLoss: 0.791238\n",
      "Training Epoch: 2 [32192/60000 (54%)]\tLoss: 0.535024\n",
      "Training Epoch: 2 [32224/60000 (54%)]\tLoss: 0.338886\n",
      "Training Epoch: 2 [32256/60000 (54%)]\tLoss: 0.484634\n",
      "Training Epoch: 2 [32288/60000 (54%)]\tLoss: 0.573909\n",
      "Training Epoch: 2 [32320/60000 (54%)]\tLoss: 0.439739\n",
      "Training Epoch: 2 [32352/60000 (54%)]\tLoss: 0.293135\n",
      "Training Epoch: 2 [32384/60000 (54%)]\tLoss: 0.554868\n",
      "Training Epoch: 2 [32416/60000 (54%)]\tLoss: 0.659364\n",
      "Training Epoch: 2 [32448/60000 (54%)]\tLoss: 0.488166\n",
      "Training Epoch: 2 [32480/60000 (54%)]\tLoss: 0.786411\n",
      "Training Epoch: 2 [32512/60000 (54%)]\tLoss: 0.232322\n",
      "Training Epoch: 2 [32544/60000 (54%)]\tLoss: 0.334254\n",
      "Training Epoch: 2 [32576/60000 (54%)]\tLoss: 0.650383\n",
      "Training Epoch: 2 [32608/60000 (54%)]\tLoss: 0.620011\n",
      "Training Epoch: 2 [32640/60000 (54%)]\tLoss: 0.678916\n",
      "Training Epoch: 2 [32672/60000 (54%)]\tLoss: 0.301461\n",
      "Training Epoch: 2 [32704/60000 (55%)]\tLoss: 0.246378\n",
      "Training Epoch: 2 [32736/60000 (55%)]\tLoss: 0.254261\n",
      "Training Epoch: 2 [32768/60000 (55%)]\tLoss: 0.733890\n",
      "Training Epoch: 2 [32800/60000 (55%)]\tLoss: 0.316616\n",
      "Training Epoch: 2 [32832/60000 (55%)]\tLoss: 0.492199\n",
      "Training Epoch: 2 [32864/60000 (55%)]\tLoss: 0.299325\n",
      "Training Epoch: 2 [32896/60000 (55%)]\tLoss: 0.304739\n",
      "Training Epoch: 2 [32928/60000 (55%)]\tLoss: 0.379992\n",
      "Training Epoch: 2 [32960/60000 (55%)]\tLoss: 0.508034\n",
      "Training Epoch: 2 [32992/60000 (55%)]\tLoss: 0.994871\n",
      "Training Epoch: 2 [33024/60000 (55%)]\tLoss: 0.535720\n",
      "Training Epoch: 2 [33056/60000 (55%)]\tLoss: 0.287073\n",
      "Training Epoch: 2 [33088/60000 (55%)]\tLoss: 0.479114\n",
      "Training Epoch: 2 [33120/60000 (55%)]\tLoss: 0.470572\n",
      "Training Epoch: 2 [33152/60000 (55%)]\tLoss: 0.633605\n",
      "Training Epoch: 2 [33184/60000 (55%)]\tLoss: 0.581769\n",
      "Training Epoch: 2 [33216/60000 (55%)]\tLoss: 0.463039\n",
      "Training Epoch: 2 [33248/60000 (55%)]\tLoss: 0.638335\n",
      "Training Epoch: 2 [33280/60000 (55%)]\tLoss: 0.480863\n",
      "Training Epoch: 2 [33312/60000 (56%)]\tLoss: 0.418206\n",
      "Training Epoch: 2 [33344/60000 (56%)]\tLoss: 0.357807\n",
      "Training Epoch: 2 [33376/60000 (56%)]\tLoss: 0.283095\n",
      "Training Epoch: 2 [33408/60000 (56%)]\tLoss: 0.802558\n",
      "Training Epoch: 2 [33440/60000 (56%)]\tLoss: 0.407265\n",
      "Training Epoch: 2 [33472/60000 (56%)]\tLoss: 0.532363\n",
      "Training Epoch: 2 [33504/60000 (56%)]\tLoss: 0.702217\n",
      "Training Epoch: 2 [33536/60000 (56%)]\tLoss: 0.658954\n",
      "Training Epoch: 2 [33568/60000 (56%)]\tLoss: 0.570828\n",
      "Training Epoch: 2 [33600/60000 (56%)]\tLoss: 0.393519\n",
      "Training Epoch: 2 [33632/60000 (56%)]\tLoss: 0.257048\n",
      "Training Epoch: 2 [33664/60000 (56%)]\tLoss: 0.268226\n",
      "Training Epoch: 2 [33696/60000 (56%)]\tLoss: 0.230165\n",
      "Training Epoch: 2 [33728/60000 (56%)]\tLoss: 0.466029\n",
      "Training Epoch: 2 [33760/60000 (56%)]\tLoss: 0.582393\n",
      "Training Epoch: 2 [33792/60000 (56%)]\tLoss: 1.096442\n",
      "Training Epoch: 2 [33824/60000 (56%)]\tLoss: 0.546112\n",
      "Training Epoch: 2 [33856/60000 (56%)]\tLoss: 0.511338\n",
      "Training Epoch: 2 [33888/60000 (56%)]\tLoss: 0.256178\n",
      "Training Epoch: 2 [33920/60000 (57%)]\tLoss: 0.503054\n",
      "Training Epoch: 2 [33952/60000 (57%)]\tLoss: 0.166579\n",
      "Training Epoch: 2 [33984/60000 (57%)]\tLoss: 0.785030\n",
      "Training Epoch: 2 [34016/60000 (57%)]\tLoss: 0.752395\n",
      "Training Epoch: 2 [34048/60000 (57%)]\tLoss: 0.443559\n",
      "Training Epoch: 2 [34080/60000 (57%)]\tLoss: 0.181573\n",
      "Training Epoch: 2 [34112/60000 (57%)]\tLoss: 0.260307\n",
      "Training Epoch: 2 [34144/60000 (57%)]\tLoss: 0.600001\n",
      "Training Epoch: 2 [34176/60000 (57%)]\tLoss: 0.571678\n",
      "Training Epoch: 2 [34208/60000 (57%)]\tLoss: 0.366363\n",
      "Training Epoch: 2 [34240/60000 (57%)]\tLoss: 0.384418\n",
      "Training Epoch: 2 [34272/60000 (57%)]\tLoss: 0.856190\n",
      "Training Epoch: 2 [34304/60000 (57%)]\tLoss: 0.490133\n",
      "Training Epoch: 2 [34336/60000 (57%)]\tLoss: 0.345497\n",
      "Training Epoch: 2 [34368/60000 (57%)]\tLoss: 0.366028\n",
      "Training Epoch: 2 [34400/60000 (57%)]\tLoss: 0.390897\n",
      "Training Epoch: 2 [34432/60000 (57%)]\tLoss: 0.235532\n",
      "Training Epoch: 2 [34464/60000 (57%)]\tLoss: 0.480047\n",
      "Training Epoch: 2 [34496/60000 (57%)]\tLoss: 0.526094\n",
      "Training Epoch: 2 [34528/60000 (58%)]\tLoss: 0.670018\n",
      "Training Epoch: 2 [34560/60000 (58%)]\tLoss: 0.222865\n",
      "Training Epoch: 2 [34592/60000 (58%)]\tLoss: 0.279591\n",
      "Training Epoch: 2 [34624/60000 (58%)]\tLoss: 0.488364\n",
      "Training Epoch: 2 [34656/60000 (58%)]\tLoss: 0.366026\n",
      "Training Epoch: 2 [34688/60000 (58%)]\tLoss: 0.403575\n",
      "Training Epoch: 2 [34720/60000 (58%)]\tLoss: 0.281550\n",
      "Training Epoch: 2 [34752/60000 (58%)]\tLoss: 0.530432\n",
      "Training Epoch: 2 [34784/60000 (58%)]\tLoss: 0.712763\n",
      "Training Epoch: 2 [34816/60000 (58%)]\tLoss: 0.580099\n",
      "Training Epoch: 2 [34848/60000 (58%)]\tLoss: 0.569366\n",
      "Training Epoch: 2 [34880/60000 (58%)]\tLoss: 0.577439\n",
      "Training Epoch: 2 [34912/60000 (58%)]\tLoss: 0.329041\n",
      "Training Epoch: 2 [34944/60000 (58%)]\tLoss: 0.452551\n",
      "Training Epoch: 2 [34976/60000 (58%)]\tLoss: 0.290757\n",
      "Training Epoch: 2 [35008/60000 (58%)]\tLoss: 0.265214\n",
      "Training Epoch: 2 [35040/60000 (58%)]\tLoss: 0.778565\n",
      "Training Epoch: 2 [35072/60000 (58%)]\tLoss: 0.410647\n",
      "Training Epoch: 2 [35104/60000 (59%)]\tLoss: 0.385356\n",
      "Training Epoch: 2 [35136/60000 (59%)]\tLoss: 0.233325\n",
      "Training Epoch: 2 [35168/60000 (59%)]\tLoss: 0.534194\n",
      "Training Epoch: 2 [35200/60000 (59%)]\tLoss: 0.681321\n",
      "Training Epoch: 2 [35232/60000 (59%)]\tLoss: 0.467556\n",
      "Training Epoch: 2 [35264/60000 (59%)]\tLoss: 0.295312\n",
      "Training Epoch: 2 [35296/60000 (59%)]\tLoss: 0.651156\n",
      "Training Epoch: 2 [35328/60000 (59%)]\tLoss: 0.401248\n",
      "Training Epoch: 2 [35360/60000 (59%)]\tLoss: 0.778048\n",
      "Training Epoch: 2 [35392/60000 (59%)]\tLoss: 0.417224\n",
      "Training Epoch: 2 [35424/60000 (59%)]\tLoss: 0.381311\n",
      "Training Epoch: 2 [35456/60000 (59%)]\tLoss: 0.993504\n",
      "Training Epoch: 2 [35488/60000 (59%)]\tLoss: 0.497636\n",
      "Training Epoch: 2 [35520/60000 (59%)]\tLoss: 0.678627\n",
      "Training Epoch: 2 [35552/60000 (59%)]\tLoss: 0.265044\n",
      "Training Epoch: 2 [35584/60000 (59%)]\tLoss: 0.191964\n",
      "Training Epoch: 2 [35616/60000 (59%)]\tLoss: 0.288151\n",
      "Training Epoch: 2 [35648/60000 (59%)]\tLoss: 0.523579\n",
      "Training Epoch: 2 [35680/60000 (59%)]\tLoss: 0.349886\n",
      "Training Epoch: 2 [35712/60000 (60%)]\tLoss: 0.237202\n",
      "Training Epoch: 2 [35744/60000 (60%)]\tLoss: 0.410021\n",
      "Training Epoch: 2 [35776/60000 (60%)]\tLoss: 0.530420\n",
      "Training Epoch: 2 [35808/60000 (60%)]\tLoss: 0.345280\n",
      "Training Epoch: 2 [35840/60000 (60%)]\tLoss: 0.129799\n",
      "Training Epoch: 2 [35872/60000 (60%)]\tLoss: 0.379458\n",
      "Training Epoch: 2 [35904/60000 (60%)]\tLoss: 0.614342\n",
      "Training Epoch: 2 [35936/60000 (60%)]\tLoss: 0.226487\n",
      "Training Epoch: 2 [35968/60000 (60%)]\tLoss: 0.368786\n",
      "Training Epoch: 2 [36000/60000 (60%)]\tLoss: 0.398860\n",
      "Training Epoch: 2 [36032/60000 (60%)]\tLoss: 0.100791\n",
      "Training Epoch: 2 [36064/60000 (60%)]\tLoss: 1.152650\n",
      "Training Epoch: 2 [36096/60000 (60%)]\tLoss: 0.528010\n",
      "Training Epoch: 2 [36128/60000 (60%)]\tLoss: 0.463234\n",
      "Training Epoch: 2 [36160/60000 (60%)]\tLoss: 0.661816\n",
      "Training Epoch: 2 [36192/60000 (60%)]\tLoss: 0.529088\n",
      "Training Epoch: 2 [36224/60000 (60%)]\tLoss: 0.478789\n",
      "Training Epoch: 2 [36256/60000 (60%)]\tLoss: 0.472538\n",
      "Training Epoch: 2 [36288/60000 (60%)]\tLoss: 0.434765\n",
      "Training Epoch: 2 [36320/60000 (61%)]\tLoss: 0.286253\n",
      "Training Epoch: 2 [36352/60000 (61%)]\tLoss: 0.454177\n",
      "Training Epoch: 2 [36384/60000 (61%)]\tLoss: 0.508345\n",
      "Training Epoch: 2 [36416/60000 (61%)]\tLoss: 0.593107\n",
      "Training Epoch: 2 [36448/60000 (61%)]\tLoss: 0.743120\n",
      "Training Epoch: 2 [36480/60000 (61%)]\tLoss: 0.627818\n",
      "Training Epoch: 2 [36512/60000 (61%)]\tLoss: 0.366905\n",
      "Training Epoch: 2 [36544/60000 (61%)]\tLoss: 0.477127\n",
      "Training Epoch: 2 [36576/60000 (61%)]\tLoss: 0.435225\n",
      "Training Epoch: 2 [36608/60000 (61%)]\tLoss: 0.333602\n",
      "Training Epoch: 2 [36640/60000 (61%)]\tLoss: 0.453156\n",
      "Training Epoch: 2 [36672/60000 (61%)]\tLoss: 0.265153\n",
      "Training Epoch: 2 [36704/60000 (61%)]\tLoss: 0.416091\n",
      "Training Epoch: 2 [36736/60000 (61%)]\tLoss: 0.250703\n",
      "Training Epoch: 2 [36768/60000 (61%)]\tLoss: 0.419334\n",
      "Training Epoch: 2 [36800/60000 (61%)]\tLoss: 0.269145\n",
      "Training Epoch: 2 [36832/60000 (61%)]\tLoss: 0.514171\n",
      "Training Epoch: 2 [36864/60000 (61%)]\tLoss: 0.527507\n",
      "Training Epoch: 2 [36896/60000 (61%)]\tLoss: 0.594267\n",
      "Training Epoch: 2 [36928/60000 (62%)]\tLoss: 0.893189\n",
      "Training Epoch: 2 [36960/60000 (62%)]\tLoss: 0.580336\n",
      "Training Epoch: 2 [36992/60000 (62%)]\tLoss: 0.348674\n",
      "Training Epoch: 2 [37024/60000 (62%)]\tLoss: 0.348507\n",
      "Training Epoch: 2 [37056/60000 (62%)]\tLoss: 0.457787\n",
      "Training Epoch: 2 [37088/60000 (62%)]\tLoss: 0.500113\n",
      "Training Epoch: 2 [37120/60000 (62%)]\tLoss: 0.405773\n",
      "Training Epoch: 2 [37152/60000 (62%)]\tLoss: 0.366232\n",
      "Training Epoch: 2 [37184/60000 (62%)]\tLoss: 0.394044\n",
      "Training Epoch: 2 [37216/60000 (62%)]\tLoss: 0.233588\n",
      "Training Epoch: 2 [37248/60000 (62%)]\tLoss: 0.508575\n",
      "Training Epoch: 2 [37280/60000 (62%)]\tLoss: 0.831136\n",
      "Training Epoch: 2 [37312/60000 (62%)]\tLoss: 0.748793\n",
      "Training Epoch: 2 [37344/60000 (62%)]\tLoss: 0.351651\n",
      "Training Epoch: 2 [37376/60000 (62%)]\tLoss: 0.469000\n",
      "Training Epoch: 2 [37408/60000 (62%)]\tLoss: 0.348155\n",
      "Training Epoch: 2 [37440/60000 (62%)]\tLoss: 0.313725\n",
      "Training Epoch: 2 [37472/60000 (62%)]\tLoss: 0.359626\n",
      "Training Epoch: 2 [37504/60000 (63%)]\tLoss: 0.606046\n",
      "Training Epoch: 2 [37536/60000 (63%)]\tLoss: 0.270867\n",
      "Training Epoch: 2 [37568/60000 (63%)]\tLoss: 0.404386\n",
      "Training Epoch: 2 [37600/60000 (63%)]\tLoss: 0.409324\n",
      "Training Epoch: 2 [37632/60000 (63%)]\tLoss: 0.449649\n",
      "Training Epoch: 2 [37664/60000 (63%)]\tLoss: 0.629062\n",
      "Training Epoch: 2 [37696/60000 (63%)]\tLoss: 0.308470\n",
      "Training Epoch: 2 [37728/60000 (63%)]\tLoss: 0.320014\n",
      "Training Epoch: 2 [37760/60000 (63%)]\tLoss: 0.410544\n",
      "Training Epoch: 2 [37792/60000 (63%)]\tLoss: 0.500275\n",
      "Training Epoch: 2 [37824/60000 (63%)]\tLoss: 0.267525\n",
      "Training Epoch: 2 [37856/60000 (63%)]\tLoss: 0.207884\n",
      "Training Epoch: 2 [37888/60000 (63%)]\tLoss: 0.352312\n",
      "Training Epoch: 2 [37920/60000 (63%)]\tLoss: 0.382303\n",
      "Training Epoch: 2 [37952/60000 (63%)]\tLoss: 0.407342\n",
      "Training Epoch: 2 [37984/60000 (63%)]\tLoss: 0.652968\n",
      "Training Epoch: 2 [38016/60000 (63%)]\tLoss: 0.428712\n",
      "Training Epoch: 2 [38048/60000 (63%)]\tLoss: 0.176909\n",
      "Training Epoch: 2 [38080/60000 (63%)]\tLoss: 0.350830\n",
      "Training Epoch: 2 [38112/60000 (64%)]\tLoss: 0.529216\n",
      "Training Epoch: 2 [38144/60000 (64%)]\tLoss: 0.738440\n",
      "Training Epoch: 2 [38176/60000 (64%)]\tLoss: 0.394024\n",
      "Training Epoch: 2 [38208/60000 (64%)]\tLoss: 0.914197\n",
      "Training Epoch: 2 [38240/60000 (64%)]\tLoss: 0.512604\n",
      "Training Epoch: 2 [38272/60000 (64%)]\tLoss: 0.258841\n",
      "Training Epoch: 2 [38304/60000 (64%)]\tLoss: 0.339530\n",
      "Training Epoch: 2 [38336/60000 (64%)]\tLoss: 0.566031\n",
      "Training Epoch: 2 [38368/60000 (64%)]\tLoss: 0.348289\n",
      "Training Epoch: 2 [38400/60000 (64%)]\tLoss: 0.353715\n",
      "Training Epoch: 2 [38432/60000 (64%)]\tLoss: 0.266279\n",
      "Training Epoch: 2 [38464/60000 (64%)]\tLoss: 0.615235\n",
      "Training Epoch: 2 [38496/60000 (64%)]\tLoss: 0.346609\n",
      "Training Epoch: 2 [38528/60000 (64%)]\tLoss: 0.233401\n",
      "Training Epoch: 2 [38560/60000 (64%)]\tLoss: 0.599150\n",
      "Training Epoch: 2 [38592/60000 (64%)]\tLoss: 0.852513\n",
      "Training Epoch: 2 [38624/60000 (64%)]\tLoss: 0.181201\n",
      "Training Epoch: 2 [38656/60000 (64%)]\tLoss: 0.297044\n",
      "Training Epoch: 2 [38688/60000 (64%)]\tLoss: 0.422188\n",
      "Training Epoch: 2 [38720/60000 (65%)]\tLoss: 0.260701\n",
      "Training Epoch: 2 [38752/60000 (65%)]\tLoss: 0.671726\n",
      "Training Epoch: 2 [38784/60000 (65%)]\tLoss: 0.406025\n",
      "Training Epoch: 2 [38816/60000 (65%)]\tLoss: 0.462589\n",
      "Training Epoch: 2 [38848/60000 (65%)]\tLoss: 0.422218\n",
      "Training Epoch: 2 [38880/60000 (65%)]\tLoss: 0.205823\n",
      "Training Epoch: 2 [38912/60000 (65%)]\tLoss: 0.381005\n",
      "Training Epoch: 2 [38944/60000 (65%)]\tLoss: 0.464786\n",
      "Training Epoch: 2 [38976/60000 (65%)]\tLoss: 0.208459\n",
      "Training Epoch: 2 [39008/60000 (65%)]\tLoss: 0.449050\n",
      "Training Epoch: 2 [39040/60000 (65%)]\tLoss: 0.252779\n",
      "Training Epoch: 2 [39072/60000 (65%)]\tLoss: 0.472827\n",
      "Training Epoch: 2 [39104/60000 (65%)]\tLoss: 0.411257\n",
      "Training Epoch: 2 [39136/60000 (65%)]\tLoss: 0.611939\n",
      "Training Epoch: 2 [39168/60000 (65%)]\tLoss: 0.288585\n",
      "Training Epoch: 2 [39200/60000 (65%)]\tLoss: 0.360543\n",
      "Training Epoch: 2 [39232/60000 (65%)]\tLoss: 0.272258\n",
      "Training Epoch: 2 [39264/60000 (65%)]\tLoss: 0.343202\n",
      "Training Epoch: 2 [39296/60000 (65%)]\tLoss: 0.214810\n",
      "Training Epoch: 2 [39328/60000 (66%)]\tLoss: 0.821020\n",
      "Training Epoch: 2 [39360/60000 (66%)]\tLoss: 0.216753\n",
      "Training Epoch: 2 [39392/60000 (66%)]\tLoss: 0.657291\n",
      "Training Epoch: 2 [39424/60000 (66%)]\tLoss: 0.988500\n",
      "Training Epoch: 2 [39456/60000 (66%)]\tLoss: 0.671319\n",
      "Training Epoch: 2 [39488/60000 (66%)]\tLoss: 0.256489\n",
      "Training Epoch: 2 [39520/60000 (66%)]\tLoss: 0.429678\n",
      "Training Epoch: 2 [39552/60000 (66%)]\tLoss: 0.236937\n",
      "Training Epoch: 2 [39584/60000 (66%)]\tLoss: 0.522184\n",
      "Training Epoch: 2 [39616/60000 (66%)]\tLoss: 0.380389\n",
      "Training Epoch: 2 [39648/60000 (66%)]\tLoss: 0.751996\n",
      "Training Epoch: 2 [39680/60000 (66%)]\tLoss: 0.799823\n",
      "Training Epoch: 2 [39712/60000 (66%)]\tLoss: 0.628559\n",
      "Training Epoch: 2 [39744/60000 (66%)]\tLoss: 0.200857\n",
      "Training Epoch: 2 [39776/60000 (66%)]\tLoss: 0.509364\n",
      "Training Epoch: 2 [39808/60000 (66%)]\tLoss: 0.615452\n",
      "Training Epoch: 2 [39840/60000 (66%)]\tLoss: 0.299173\n",
      "Training Epoch: 2 [39872/60000 (66%)]\tLoss: 0.289021\n",
      "Training Epoch: 2 [39904/60000 (67%)]\tLoss: 0.483161\n",
      "Training Epoch: 2 [39936/60000 (67%)]\tLoss: 0.265891\n",
      "Training Epoch: 2 [39968/60000 (67%)]\tLoss: 0.732195\n",
      "Training Epoch: 2 [40000/60000 (67%)]\tLoss: 0.223032\n",
      "Training Epoch: 2 [40032/60000 (67%)]\tLoss: 0.163542\n",
      "Training Epoch: 2 [40064/60000 (67%)]\tLoss: 0.166954\n",
      "Training Epoch: 2 [40096/60000 (67%)]\tLoss: 0.378032\n",
      "Training Epoch: 2 [40128/60000 (67%)]\tLoss: 0.306029\n",
      "Training Epoch: 2 [40160/60000 (67%)]\tLoss: 0.240522\n",
      "Training Epoch: 2 [40192/60000 (67%)]\tLoss: 0.502502\n",
      "Training Epoch: 2 [40224/60000 (67%)]\tLoss: 0.311099\n",
      "Training Epoch: 2 [40256/60000 (67%)]\tLoss: 0.465343\n",
      "Training Epoch: 2 [40288/60000 (67%)]\tLoss: 0.394671\n",
      "Training Epoch: 2 [40320/60000 (67%)]\tLoss: 0.478994\n",
      "Training Epoch: 2 [40352/60000 (67%)]\tLoss: 0.713987\n",
      "Training Epoch: 2 [40384/60000 (67%)]\tLoss: 0.561785\n",
      "Training Epoch: 2 [40416/60000 (67%)]\tLoss: 0.274912\n",
      "Training Epoch: 2 [40448/60000 (67%)]\tLoss: 0.444766\n",
      "Training Epoch: 2 [40480/60000 (67%)]\tLoss: 0.652090\n",
      "Training Epoch: 2 [40512/60000 (68%)]\tLoss: 0.719540\n",
      "Training Epoch: 2 [40544/60000 (68%)]\tLoss: 0.408462\n",
      "Training Epoch: 2 [40576/60000 (68%)]\tLoss: 0.438665\n",
      "Training Epoch: 2 [40608/60000 (68%)]\tLoss: 0.959465\n",
      "Training Epoch: 2 [40640/60000 (68%)]\tLoss: 0.644736\n",
      "Training Epoch: 2 [40672/60000 (68%)]\tLoss: 0.580295\n",
      "Training Epoch: 2 [40704/60000 (68%)]\tLoss: 0.751176\n",
      "Training Epoch: 2 [40736/60000 (68%)]\tLoss: 0.268949\n",
      "Training Epoch: 2 [40768/60000 (68%)]\tLoss: 0.439775\n",
      "Training Epoch: 2 [40800/60000 (68%)]\tLoss: 0.206838\n",
      "Training Epoch: 2 [40832/60000 (68%)]\tLoss: 0.582321\n",
      "Training Epoch: 2 [40864/60000 (68%)]\tLoss: 0.159814\n",
      "Training Epoch: 2 [40896/60000 (68%)]\tLoss: 0.523749\n",
      "Training Epoch: 2 [40928/60000 (68%)]\tLoss: 0.383940\n",
      "Training Epoch: 2 [40960/60000 (68%)]\tLoss: 0.798299\n",
      "Training Epoch: 2 [40992/60000 (68%)]\tLoss: 0.786593\n",
      "Training Epoch: 2 [41024/60000 (68%)]\tLoss: 0.439776\n",
      "Training Epoch: 2 [41056/60000 (68%)]\tLoss: 0.238832\n",
      "Training Epoch: 2 [41088/60000 (68%)]\tLoss: 0.617993\n",
      "Training Epoch: 2 [41120/60000 (69%)]\tLoss: 0.622913\n",
      "Training Epoch: 2 [41152/60000 (69%)]\tLoss: 0.362529\n",
      "Training Epoch: 2 [41184/60000 (69%)]\tLoss: 0.639959\n",
      "Training Epoch: 2 [41216/60000 (69%)]\tLoss: 0.304648\n",
      "Training Epoch: 2 [41248/60000 (69%)]\tLoss: 0.449440\n",
      "Training Epoch: 2 [41280/60000 (69%)]\tLoss: 0.738392\n",
      "Training Epoch: 2 [41312/60000 (69%)]\tLoss: 0.126742\n",
      "Training Epoch: 2 [41344/60000 (69%)]\tLoss: 0.718632\n",
      "Training Epoch: 2 [41376/60000 (69%)]\tLoss: 0.489946\n",
      "Training Epoch: 2 [41408/60000 (69%)]\tLoss: 0.351254\n",
      "Training Epoch: 2 [41440/60000 (69%)]\tLoss: 0.217845\n",
      "Training Epoch: 2 [41472/60000 (69%)]\tLoss: 0.187776\n",
      "Training Epoch: 2 [41504/60000 (69%)]\tLoss: 0.729331\n",
      "Training Epoch: 2 [41536/60000 (69%)]\tLoss: 0.540187\n",
      "Training Epoch: 2 [41568/60000 (69%)]\tLoss: 0.201747\n",
      "Training Epoch: 2 [41600/60000 (69%)]\tLoss: 0.449944\n",
      "Training Epoch: 2 [41632/60000 (69%)]\tLoss: 0.294057\n",
      "Training Epoch: 2 [41664/60000 (69%)]\tLoss: 0.719321\n",
      "Training Epoch: 2 [41696/60000 (69%)]\tLoss: 0.485744\n",
      "Training Epoch: 2 [41728/60000 (70%)]\tLoss: 0.380206\n",
      "Training Epoch: 2 [41760/60000 (70%)]\tLoss: 0.575349\n",
      "Training Epoch: 2 [41792/60000 (70%)]\tLoss: 0.508227\n",
      "Training Epoch: 2 [41824/60000 (70%)]\tLoss: 0.519220\n",
      "Training Epoch: 2 [41856/60000 (70%)]\tLoss: 0.247783\n",
      "Training Epoch: 2 [41888/60000 (70%)]\tLoss: 0.466859\n",
      "Training Epoch: 2 [41920/60000 (70%)]\tLoss: 0.406497\n",
      "Training Epoch: 2 [41952/60000 (70%)]\tLoss: 0.298539\n",
      "Training Epoch: 2 [41984/60000 (70%)]\tLoss: 0.119870\n",
      "Training Epoch: 2 [42016/60000 (70%)]\tLoss: 0.608675\n",
      "Training Epoch: 2 [42048/60000 (70%)]\tLoss: 0.238497\n",
      "Training Epoch: 2 [42080/60000 (70%)]\tLoss: 0.535814\n",
      "Training Epoch: 2 [42112/60000 (70%)]\tLoss: 0.354135\n",
      "Training Epoch: 2 [42144/60000 (70%)]\tLoss: 0.258783\n",
      "Training Epoch: 2 [42176/60000 (70%)]\tLoss: 0.520070\n",
      "Training Epoch: 2 [42208/60000 (70%)]\tLoss: 0.626073\n",
      "Training Epoch: 2 [42240/60000 (70%)]\tLoss: 0.460040\n",
      "Training Epoch: 2 [42272/60000 (70%)]\tLoss: 0.404790\n",
      "Training Epoch: 2 [42304/60000 (71%)]\tLoss: 0.498612\n",
      "Training Epoch: 2 [42336/60000 (71%)]\tLoss: 0.484144\n",
      "Training Epoch: 2 [42368/60000 (71%)]\tLoss: 0.158250\n",
      "Training Epoch: 2 [42400/60000 (71%)]\tLoss: 0.472090\n",
      "Training Epoch: 2 [42432/60000 (71%)]\tLoss: 0.357427\n",
      "Training Epoch: 2 [42464/60000 (71%)]\tLoss: 0.325513\n",
      "Training Epoch: 2 [42496/60000 (71%)]\tLoss: 0.628377\n",
      "Training Epoch: 2 [42528/60000 (71%)]\tLoss: 0.612424\n",
      "Training Epoch: 2 [42560/60000 (71%)]\tLoss: 0.698424\n",
      "Training Epoch: 2 [42592/60000 (71%)]\tLoss: 0.517506\n",
      "Training Epoch: 2 [42624/60000 (71%)]\tLoss: 0.263402\n",
      "Training Epoch: 2 [42656/60000 (71%)]\tLoss: 0.430543\n",
      "Training Epoch: 2 [42688/60000 (71%)]\tLoss: 0.866991\n",
      "Training Epoch: 2 [42720/60000 (71%)]\tLoss: 0.305533\n",
      "Training Epoch: 2 [42752/60000 (71%)]\tLoss: 0.369840\n",
      "Training Epoch: 2 [42784/60000 (71%)]\tLoss: 0.151968\n",
      "Training Epoch: 2 [42816/60000 (71%)]\tLoss: 0.257725\n",
      "Training Epoch: 2 [42848/60000 (71%)]\tLoss: 0.228305\n",
      "Training Epoch: 2 [42880/60000 (71%)]\tLoss: 0.654504\n",
      "Training Epoch: 2 [42912/60000 (72%)]\tLoss: 0.550789\n",
      "Training Epoch: 2 [42944/60000 (72%)]\tLoss: 0.615730\n",
      "Training Epoch: 2 [42976/60000 (72%)]\tLoss: 0.307090\n",
      "Training Epoch: 2 [43008/60000 (72%)]\tLoss: 0.373706\n",
      "Training Epoch: 2 [43040/60000 (72%)]\tLoss: 0.414739\n",
      "Training Epoch: 2 [43072/60000 (72%)]\tLoss: 0.277162\n",
      "Training Epoch: 2 [43104/60000 (72%)]\tLoss: 0.574692\n",
      "Training Epoch: 2 [43136/60000 (72%)]\tLoss: 0.303417\n",
      "Training Epoch: 2 [43168/60000 (72%)]\tLoss: 0.518604\n",
      "Training Epoch: 2 [43200/60000 (72%)]\tLoss: 0.562594\n",
      "Training Epoch: 2 [43232/60000 (72%)]\tLoss: 0.405668\n",
      "Training Epoch: 2 [43264/60000 (72%)]\tLoss: 0.765463\n",
      "Training Epoch: 2 [43296/60000 (72%)]\tLoss: 0.331085\n",
      "Training Epoch: 2 [43328/60000 (72%)]\tLoss: 0.417530\n",
      "Training Epoch: 2 [43360/60000 (72%)]\tLoss: 0.392330\n",
      "Training Epoch: 2 [43392/60000 (72%)]\tLoss: 0.394245\n",
      "Training Epoch: 2 [43424/60000 (72%)]\tLoss: 0.667076\n",
      "Training Epoch: 2 [43456/60000 (72%)]\tLoss: 0.440273\n",
      "Training Epoch: 2 [43488/60000 (72%)]\tLoss: 0.332721\n",
      "Training Epoch: 2 [43520/60000 (73%)]\tLoss: 0.251020\n",
      "Training Epoch: 2 [43552/60000 (73%)]\tLoss: 0.968257\n",
      "Training Epoch: 2 [43584/60000 (73%)]\tLoss: 0.408232\n",
      "Training Epoch: 2 [43616/60000 (73%)]\tLoss: 0.605726\n",
      "Training Epoch: 2 [43648/60000 (73%)]\tLoss: 0.331127\n",
      "Training Epoch: 2 [43680/60000 (73%)]\tLoss: 0.234750\n",
      "Training Epoch: 2 [43712/60000 (73%)]\tLoss: 0.431393\n",
      "Training Epoch: 2 [43744/60000 (73%)]\tLoss: 0.370680\n",
      "Training Epoch: 2 [43776/60000 (73%)]\tLoss: 0.467914\n",
      "Training Epoch: 2 [43808/60000 (73%)]\tLoss: 0.414920\n",
      "Training Epoch: 2 [43840/60000 (73%)]\tLoss: 0.601062\n",
      "Training Epoch: 2 [43872/60000 (73%)]\tLoss: 0.283564\n",
      "Training Epoch: 2 [43904/60000 (73%)]\tLoss: 0.310934\n",
      "Training Epoch: 2 [43936/60000 (73%)]\tLoss: 0.361679\n",
      "Training Epoch: 2 [43968/60000 (73%)]\tLoss: 0.367362\n",
      "Training Epoch: 2 [44000/60000 (73%)]\tLoss: 0.577702\n",
      "Training Epoch: 2 [44032/60000 (73%)]\tLoss: 0.514308\n",
      "Training Epoch: 2 [44064/60000 (73%)]\tLoss: 0.148610\n",
      "Training Epoch: 2 [44096/60000 (73%)]\tLoss: 0.604085\n",
      "Training Epoch: 2 [44128/60000 (74%)]\tLoss: 0.513851\n",
      "Training Epoch: 2 [44160/60000 (74%)]\tLoss: 0.300003\n",
      "Training Epoch: 2 [44192/60000 (74%)]\tLoss: 0.727709\n",
      "Training Epoch: 2 [44224/60000 (74%)]\tLoss: 0.761253\n",
      "Training Epoch: 2 [44256/60000 (74%)]\tLoss: 0.615769\n",
      "Training Epoch: 2 [44288/60000 (74%)]\tLoss: 0.218470\n",
      "Training Epoch: 2 [44320/60000 (74%)]\tLoss: 0.616843\n",
      "Training Epoch: 2 [44352/60000 (74%)]\tLoss: 0.605710\n",
      "Training Epoch: 2 [44384/60000 (74%)]\tLoss: 0.254452\n",
      "Training Epoch: 2 [44416/60000 (74%)]\tLoss: 0.111808\n",
      "Training Epoch: 2 [44448/60000 (74%)]\tLoss: 0.698777\n",
      "Training Epoch: 2 [44480/60000 (74%)]\tLoss: 0.483673\n",
      "Training Epoch: 2 [44512/60000 (74%)]\tLoss: 0.539092\n",
      "Training Epoch: 2 [44544/60000 (74%)]\tLoss: 0.454041\n",
      "Training Epoch: 2 [44576/60000 (74%)]\tLoss: 1.291932\n",
      "Training Epoch: 2 [44608/60000 (74%)]\tLoss: 0.508636\n",
      "Training Epoch: 2 [44640/60000 (74%)]\tLoss: 0.370118\n",
      "Training Epoch: 2 [44672/60000 (74%)]\tLoss: 0.388607\n",
      "Training Epoch: 2 [44704/60000 (75%)]\tLoss: 0.693768\n",
      "Training Epoch: 2 [44736/60000 (75%)]\tLoss: 0.521562\n",
      "Training Epoch: 2 [44768/60000 (75%)]\tLoss: 0.260084\n",
      "Training Epoch: 2 [44800/60000 (75%)]\tLoss: 0.407859\n",
      "Training Epoch: 2 [44832/60000 (75%)]\tLoss: 0.696758\n",
      "Training Epoch: 2 [44864/60000 (75%)]\tLoss: 0.439588\n",
      "Training Epoch: 2 [44896/60000 (75%)]\tLoss: 0.374209\n",
      "Training Epoch: 2 [44928/60000 (75%)]\tLoss: 0.237623\n",
      "Training Epoch: 2 [44960/60000 (75%)]\tLoss: 0.363499\n",
      "Training Epoch: 2 [44992/60000 (75%)]\tLoss: 0.484984\n",
      "Training Epoch: 2 [45024/60000 (75%)]\tLoss: 0.379373\n",
      "Training Epoch: 2 [45056/60000 (75%)]\tLoss: 0.596391\n",
      "Training Epoch: 2 [45088/60000 (75%)]\tLoss: 0.177972\n",
      "Training Epoch: 2 [45120/60000 (75%)]\tLoss: 0.286934\n",
      "Training Epoch: 2 [45152/60000 (75%)]\tLoss: 0.624558\n",
      "Training Epoch: 2 [45184/60000 (75%)]\tLoss: 0.503042\n",
      "Training Epoch: 2 [45216/60000 (75%)]\tLoss: 0.521948\n",
      "Training Epoch: 2 [45248/60000 (75%)]\tLoss: 0.160638\n",
      "Training Epoch: 2 [45280/60000 (75%)]\tLoss: 0.504820\n",
      "Training Epoch: 2 [45312/60000 (76%)]\tLoss: 0.271943\n",
      "Training Epoch: 2 [45344/60000 (76%)]\tLoss: 0.381811\n",
      "Training Epoch: 2 [45376/60000 (76%)]\tLoss: 0.280285\n",
      "Training Epoch: 2 [45408/60000 (76%)]\tLoss: 0.423421\n",
      "Training Epoch: 2 [45440/60000 (76%)]\tLoss: 0.695418\n",
      "Training Epoch: 2 [45472/60000 (76%)]\tLoss: 0.513647\n",
      "Training Epoch: 2 [45504/60000 (76%)]\tLoss: 0.434603\n",
      "Training Epoch: 2 [45536/60000 (76%)]\tLoss: 0.394513\n",
      "Training Epoch: 2 [45568/60000 (76%)]\tLoss: 0.354887\n",
      "Training Epoch: 2 [45600/60000 (76%)]\tLoss: 0.151642\n",
      "Training Epoch: 2 [45632/60000 (76%)]\tLoss: 1.085434\n",
      "Training Epoch: 2 [45664/60000 (76%)]\tLoss: 0.380630\n",
      "Training Epoch: 2 [45696/60000 (76%)]\tLoss: 0.115728\n",
      "Training Epoch: 2 [45728/60000 (76%)]\tLoss: 0.289752\n",
      "Training Epoch: 2 [45760/60000 (76%)]\tLoss: 0.186570\n",
      "Training Epoch: 2 [45792/60000 (76%)]\tLoss: 0.282226\n",
      "Training Epoch: 2 [45824/60000 (76%)]\tLoss: 0.282752\n",
      "Training Epoch: 2 [45856/60000 (76%)]\tLoss: 0.414938\n",
      "Training Epoch: 2 [45888/60000 (76%)]\tLoss: 0.622838\n",
      "Training Epoch: 2 [45920/60000 (77%)]\tLoss: 0.330240\n",
      "Training Epoch: 2 [45952/60000 (77%)]\tLoss: 0.491607\n",
      "Training Epoch: 2 [45984/60000 (77%)]\tLoss: 0.491358\n",
      "Training Epoch: 2 [46016/60000 (77%)]\tLoss: 0.628986\n",
      "Training Epoch: 2 [46048/60000 (77%)]\tLoss: 0.374218\n",
      "Training Epoch: 2 [46080/60000 (77%)]\tLoss: 0.345826\n",
      "Training Epoch: 2 [46112/60000 (77%)]\tLoss: 0.690521\n",
      "Training Epoch: 2 [46144/60000 (77%)]\tLoss: 0.304648\n",
      "Training Epoch: 2 [46176/60000 (77%)]\tLoss: 0.359801\n",
      "Training Epoch: 2 [46208/60000 (77%)]\tLoss: 0.542865\n",
      "Training Epoch: 2 [46240/60000 (77%)]\tLoss: 0.554721\n",
      "Training Epoch: 2 [46272/60000 (77%)]\tLoss: 0.384131\n",
      "Training Epoch: 2 [46304/60000 (77%)]\tLoss: 0.274914\n",
      "Training Epoch: 2 [46336/60000 (77%)]\tLoss: 0.584783\n",
      "Training Epoch: 2 [46368/60000 (77%)]\tLoss: 0.387793\n",
      "Training Epoch: 2 [46400/60000 (77%)]\tLoss: 0.268770\n",
      "Training Epoch: 2 [46432/60000 (77%)]\tLoss: 0.360943\n",
      "Training Epoch: 2 [46464/60000 (77%)]\tLoss: 0.217637\n",
      "Training Epoch: 2 [46496/60000 (77%)]\tLoss: 0.272123\n",
      "Training Epoch: 2 [46528/60000 (78%)]\tLoss: 0.215393\n",
      "Training Epoch: 2 [46560/60000 (78%)]\tLoss: 0.184424\n",
      "Training Epoch: 2 [46592/60000 (78%)]\tLoss: 0.326351\n",
      "Training Epoch: 2 [46624/60000 (78%)]\tLoss: 0.337210\n",
      "Training Epoch: 2 [46656/60000 (78%)]\tLoss: 0.193007\n",
      "Training Epoch: 2 [46688/60000 (78%)]\tLoss: 0.279920\n",
      "Training Epoch: 2 [46720/60000 (78%)]\tLoss: 0.359358\n",
      "Training Epoch: 2 [46752/60000 (78%)]\tLoss: 0.493901\n",
      "Training Epoch: 2 [46784/60000 (78%)]\tLoss: 0.549686\n",
      "Training Epoch: 2 [46816/60000 (78%)]\tLoss: 0.287800\n",
      "Training Epoch: 2 [46848/60000 (78%)]\tLoss: 0.305119\n",
      "Training Epoch: 2 [46880/60000 (78%)]\tLoss: 0.360954\n",
      "Training Epoch: 2 [46912/60000 (78%)]\tLoss: 0.209885\n",
      "Training Epoch: 2 [46944/60000 (78%)]\tLoss: 0.813486\n",
      "Training Epoch: 2 [46976/60000 (78%)]\tLoss: 0.490177\n",
      "Training Epoch: 2 [47008/60000 (78%)]\tLoss: 0.618341\n",
      "Training Epoch: 2 [47040/60000 (78%)]\tLoss: 0.308604\n",
      "Training Epoch: 2 [47072/60000 (78%)]\tLoss: 0.429507\n",
      "Training Epoch: 2 [47104/60000 (79%)]\tLoss: 0.432215\n",
      "Training Epoch: 2 [47136/60000 (79%)]\tLoss: 0.340882\n",
      "Training Epoch: 2 [47168/60000 (79%)]\tLoss: 0.485530\n",
      "Training Epoch: 2 [47200/60000 (79%)]\tLoss: 0.452447\n",
      "Training Epoch: 2 [47232/60000 (79%)]\tLoss: 0.226568\n",
      "Training Epoch: 2 [47264/60000 (79%)]\tLoss: 0.297985\n",
      "Training Epoch: 2 [47296/60000 (79%)]\tLoss: 0.417150\n",
      "Training Epoch: 2 [47328/60000 (79%)]\tLoss: 0.233487\n",
      "Training Epoch: 2 [47360/60000 (79%)]\tLoss: 0.804186\n",
      "Training Epoch: 2 [47392/60000 (79%)]\tLoss: 0.412360\n",
      "Training Epoch: 2 [47424/60000 (79%)]\tLoss: 0.356848\n",
      "Training Epoch: 2 [47456/60000 (79%)]\tLoss: 0.268632\n",
      "Training Epoch: 2 [47488/60000 (79%)]\tLoss: 0.580298\n",
      "Training Epoch: 2 [47520/60000 (79%)]\tLoss: 0.544487\n",
      "Training Epoch: 2 [47552/60000 (79%)]\tLoss: 0.338126\n",
      "Training Epoch: 2 [47584/60000 (79%)]\tLoss: 0.248088\n",
      "Training Epoch: 2 [47616/60000 (79%)]\tLoss: 0.234546\n",
      "Training Epoch: 2 [47648/60000 (79%)]\tLoss: 0.303686\n",
      "Training Epoch: 2 [47680/60000 (79%)]\tLoss: 0.391584\n",
      "Training Epoch: 2 [47712/60000 (80%)]\tLoss: 0.166282\n",
      "Training Epoch: 2 [47744/60000 (80%)]\tLoss: 0.312771\n",
      "Training Epoch: 2 [47776/60000 (80%)]\tLoss: 0.187958\n",
      "Training Epoch: 2 [47808/60000 (80%)]\tLoss: 0.564552\n",
      "Training Epoch: 2 [47840/60000 (80%)]\tLoss: 0.522376\n",
      "Training Epoch: 2 [47872/60000 (80%)]\tLoss: 0.633513\n",
      "Training Epoch: 2 [47904/60000 (80%)]\tLoss: 0.579554\n",
      "Training Epoch: 2 [47936/60000 (80%)]\tLoss: 0.631015\n",
      "Training Epoch: 2 [47968/60000 (80%)]\tLoss: 0.640806\n",
      "Training Epoch: 2 [48000/60000 (80%)]\tLoss: 0.911656\n",
      "Training Epoch: 2 [48032/60000 (80%)]\tLoss: 0.375460\n",
      "Training Epoch: 2 [48064/60000 (80%)]\tLoss: 0.472569\n",
      "Training Epoch: 2 [48096/60000 (80%)]\tLoss: 0.548112\n",
      "Training Epoch: 2 [48128/60000 (80%)]\tLoss: 0.324443\n",
      "Training Epoch: 2 [48160/60000 (80%)]\tLoss: 0.407141\n",
      "Training Epoch: 2 [48192/60000 (80%)]\tLoss: 0.127173\n",
      "Training Epoch: 2 [48224/60000 (80%)]\tLoss: 0.103069\n",
      "Training Epoch: 2 [48256/60000 (80%)]\tLoss: 0.169780\n",
      "Training Epoch: 2 [48288/60000 (80%)]\tLoss: 0.128125\n",
      "Training Epoch: 2 [48320/60000 (81%)]\tLoss: 0.697544\n",
      "Training Epoch: 2 [48352/60000 (81%)]\tLoss: 0.362309\n",
      "Training Epoch: 2 [48384/60000 (81%)]\tLoss: 0.455052\n",
      "Training Epoch: 2 [48416/60000 (81%)]\tLoss: 0.291066\n",
      "Training Epoch: 2 [48448/60000 (81%)]\tLoss: 0.420242\n",
      "Training Epoch: 2 [48480/60000 (81%)]\tLoss: 0.204352\n",
      "Training Epoch: 2 [48512/60000 (81%)]\tLoss: 0.399559\n",
      "Training Epoch: 2 [48544/60000 (81%)]\tLoss: 0.274666\n",
      "Training Epoch: 2 [48576/60000 (81%)]\tLoss: 0.457550\n",
      "Training Epoch: 2 [48608/60000 (81%)]\tLoss: 0.538048\n",
      "Training Epoch: 2 [48640/60000 (81%)]\tLoss: 0.334078\n",
      "Training Epoch: 2 [48672/60000 (81%)]\tLoss: 0.407521\n",
      "Training Epoch: 2 [48704/60000 (81%)]\tLoss: 0.287693\n",
      "Training Epoch: 2 [48736/60000 (81%)]\tLoss: 0.960016\n",
      "Training Epoch: 2 [48768/60000 (81%)]\tLoss: 0.835479\n",
      "Training Epoch: 2 [48800/60000 (81%)]\tLoss: 0.483487\n",
      "Training Epoch: 2 [48832/60000 (81%)]\tLoss: 0.400326\n",
      "Training Epoch: 2 [48864/60000 (81%)]\tLoss: 0.339902\n",
      "Training Epoch: 2 [48896/60000 (81%)]\tLoss: 0.309007\n",
      "Training Epoch: 2 [48928/60000 (82%)]\tLoss: 0.498880\n",
      "Training Epoch: 2 [48960/60000 (82%)]\tLoss: 0.290843\n",
      "Training Epoch: 2 [48992/60000 (82%)]\tLoss: 0.465755\n",
      "Training Epoch: 2 [49024/60000 (82%)]\tLoss: 0.702281\n",
      "Training Epoch: 2 [49056/60000 (82%)]\tLoss: 0.533930\n",
      "Training Epoch: 2 [49088/60000 (82%)]\tLoss: 0.396335\n",
      "Training Epoch: 2 [49120/60000 (82%)]\tLoss: 0.702989\n",
      "Training Epoch: 2 [49152/60000 (82%)]\tLoss: 0.485104\n",
      "Training Epoch: 2 [49184/60000 (82%)]\tLoss: 0.686444\n",
      "Training Epoch: 2 [49216/60000 (82%)]\tLoss: 0.350533\n",
      "Training Epoch: 2 [49248/60000 (82%)]\tLoss: 0.606710\n",
      "Training Epoch: 2 [49280/60000 (82%)]\tLoss: 0.346047\n",
      "Training Epoch: 2 [49312/60000 (82%)]\tLoss: 0.270602\n",
      "Training Epoch: 2 [49344/60000 (82%)]\tLoss: 0.526424\n",
      "Training Epoch: 2 [49376/60000 (82%)]\tLoss: 0.418561\n",
      "Training Epoch: 2 [49408/60000 (82%)]\tLoss: 0.687178\n",
      "Training Epoch: 2 [49440/60000 (82%)]\tLoss: 0.296710\n",
      "Training Epoch: 2 [49472/60000 (82%)]\tLoss: 0.765846\n",
      "Training Epoch: 2 [49504/60000 (83%)]\tLoss: 0.422748\n",
      "Training Epoch: 2 [49536/60000 (83%)]\tLoss: 0.286659\n",
      "Training Epoch: 2 [49568/60000 (83%)]\tLoss: 0.358880\n",
      "Training Epoch: 2 [49600/60000 (83%)]\tLoss: 0.528365\n",
      "Training Epoch: 2 [49632/60000 (83%)]\tLoss: 0.583460\n",
      "Training Epoch: 2 [49664/60000 (83%)]\tLoss: 0.218337\n",
      "Training Epoch: 2 [49696/60000 (83%)]\tLoss: 0.596307\n",
      "Training Epoch: 2 [49728/60000 (83%)]\tLoss: 0.281691\n",
      "Training Epoch: 2 [49760/60000 (83%)]\tLoss: 0.709142\n",
      "Training Epoch: 2 [49792/60000 (83%)]\tLoss: 0.266529\n",
      "Training Epoch: 2 [49824/60000 (83%)]\tLoss: 0.410746\n",
      "Training Epoch: 2 [49856/60000 (83%)]\tLoss: 0.336164\n",
      "Training Epoch: 2 [49888/60000 (83%)]\tLoss: 0.233189\n",
      "Training Epoch: 2 [49920/60000 (83%)]\tLoss: 0.232420\n",
      "Training Epoch: 2 [49952/60000 (83%)]\tLoss: 0.409994\n",
      "Training Epoch: 2 [49984/60000 (83%)]\tLoss: 0.559416\n",
      "Training Epoch: 2 [50016/60000 (83%)]\tLoss: 0.430999\n",
      "Training Epoch: 2 [50048/60000 (83%)]\tLoss: 0.429041\n",
      "Training Epoch: 2 [50080/60000 (83%)]\tLoss: 0.096839\n",
      "Training Epoch: 2 [50112/60000 (84%)]\tLoss: 0.843128\n",
      "Training Epoch: 2 [50144/60000 (84%)]\tLoss: 0.569951\n",
      "Training Epoch: 2 [50176/60000 (84%)]\tLoss: 0.481312\n",
      "Training Epoch: 2 [50208/60000 (84%)]\tLoss: 0.348201\n",
      "Training Epoch: 2 [50240/60000 (84%)]\tLoss: 0.502934\n",
      "Training Epoch: 2 [50272/60000 (84%)]\tLoss: 0.171714\n",
      "Training Epoch: 2 [50304/60000 (84%)]\tLoss: 0.159303\n",
      "Training Epoch: 2 [50336/60000 (84%)]\tLoss: 0.282409\n",
      "Training Epoch: 2 [50368/60000 (84%)]\tLoss: 0.231925\n",
      "Training Epoch: 2 [50400/60000 (84%)]\tLoss: 0.528411\n",
      "Training Epoch: 2 [50432/60000 (84%)]\tLoss: 0.572629\n",
      "Training Epoch: 2 [50464/60000 (84%)]\tLoss: 0.224254\n",
      "Training Epoch: 2 [50496/60000 (84%)]\tLoss: 0.485935\n",
      "Training Epoch: 2 [50528/60000 (84%)]\tLoss: 0.276804\n",
      "Training Epoch: 2 [50560/60000 (84%)]\tLoss: 0.136673\n",
      "Training Epoch: 2 [50592/60000 (84%)]\tLoss: 0.312987\n",
      "Training Epoch: 2 [50624/60000 (84%)]\tLoss: 0.263371\n",
      "Training Epoch: 2 [50656/60000 (84%)]\tLoss: 0.335340\n",
      "Training Epoch: 2 [50688/60000 (84%)]\tLoss: 0.182292\n",
      "Training Epoch: 2 [50720/60000 (85%)]\tLoss: 0.346095\n",
      "Training Epoch: 2 [50752/60000 (85%)]\tLoss: 0.562375\n",
      "Training Epoch: 2 [50784/60000 (85%)]\tLoss: 0.357331\n",
      "Training Epoch: 2 [50816/60000 (85%)]\tLoss: 0.844566\n",
      "Training Epoch: 2 [50848/60000 (85%)]\tLoss: 0.427603\n",
      "Training Epoch: 2 [50880/60000 (85%)]\tLoss: 0.279536\n",
      "Training Epoch: 2 [50912/60000 (85%)]\tLoss: 0.544721\n",
      "Training Epoch: 2 [50944/60000 (85%)]\tLoss: 1.422951\n",
      "Training Epoch: 2 [50976/60000 (85%)]\tLoss: 0.345609\n",
      "Training Epoch: 2 [51008/60000 (85%)]\tLoss: 0.640489\n",
      "Training Epoch: 2 [51040/60000 (85%)]\tLoss: 0.532135\n",
      "Training Epoch: 2 [51072/60000 (85%)]\tLoss: 0.341568\n",
      "Training Epoch: 2 [51104/60000 (85%)]\tLoss: 0.260099\n",
      "Training Epoch: 2 [51136/60000 (85%)]\tLoss: 0.481297\n",
      "Training Epoch: 2 [51168/60000 (85%)]\tLoss: 0.629164\n",
      "Training Epoch: 2 [51200/60000 (85%)]\tLoss: 0.536349\n",
      "Training Epoch: 2 [51232/60000 (85%)]\tLoss: 0.192707\n",
      "Training Epoch: 2 [51264/60000 (85%)]\tLoss: 0.473019\n",
      "Training Epoch: 2 [51296/60000 (85%)]\tLoss: 0.240162\n",
      "Training Epoch: 2 [51328/60000 (86%)]\tLoss: 0.445293\n",
      "Training Epoch: 2 [51360/60000 (86%)]\tLoss: 0.256072\n",
      "Training Epoch: 2 [51392/60000 (86%)]\tLoss: 0.372622\n",
      "Training Epoch: 2 [51424/60000 (86%)]\tLoss: 0.403964\n",
      "Training Epoch: 2 [51456/60000 (86%)]\tLoss: 0.851726\n",
      "Training Epoch: 2 [51488/60000 (86%)]\tLoss: 0.192815\n",
      "Training Epoch: 2 [51520/60000 (86%)]\tLoss: 0.678583\n",
      "Training Epoch: 2 [51552/60000 (86%)]\tLoss: 0.511567\n",
      "Training Epoch: 2 [51584/60000 (86%)]\tLoss: 0.308867\n",
      "Training Epoch: 2 [51616/60000 (86%)]\tLoss: 0.268519\n",
      "Training Epoch: 2 [51648/60000 (86%)]\tLoss: 0.288323\n",
      "Training Epoch: 2 [51680/60000 (86%)]\tLoss: 0.065691\n",
      "Training Epoch: 2 [51712/60000 (86%)]\tLoss: 0.489817\n",
      "Training Epoch: 2 [51744/60000 (86%)]\tLoss: 0.571257\n",
      "Training Epoch: 2 [51776/60000 (86%)]\tLoss: 0.385511\n",
      "Training Epoch: 2 [51808/60000 (86%)]\tLoss: 0.473890\n",
      "Training Epoch: 2 [51840/60000 (86%)]\tLoss: 0.480101\n",
      "Training Epoch: 2 [51872/60000 (86%)]\tLoss: 0.324002\n",
      "Training Epoch: 2 [51904/60000 (87%)]\tLoss: 0.438984\n",
      "Training Epoch: 2 [51936/60000 (87%)]\tLoss: 0.381002\n",
      "Training Epoch: 2 [51968/60000 (87%)]\tLoss: 0.750297\n",
      "Training Epoch: 2 [52000/60000 (87%)]\tLoss: 0.230939\n",
      "Training Epoch: 2 [52032/60000 (87%)]\tLoss: 0.478587\n",
      "Training Epoch: 2 [52064/60000 (87%)]\tLoss: 0.722063\n",
      "Training Epoch: 2 [52096/60000 (87%)]\tLoss: 0.302143\n",
      "Training Epoch: 2 [52128/60000 (87%)]\tLoss: 0.442710\n",
      "Training Epoch: 2 [52160/60000 (87%)]\tLoss: 0.103759\n",
      "Training Epoch: 2 [52192/60000 (87%)]\tLoss: 0.631638\n",
      "Training Epoch: 2 [52224/60000 (87%)]\tLoss: 0.652905\n",
      "Training Epoch: 2 [52256/60000 (87%)]\tLoss: 0.298389\n",
      "Training Epoch: 2 [52288/60000 (87%)]\tLoss: 0.278876\n",
      "Training Epoch: 2 [52320/60000 (87%)]\tLoss: 0.250777\n",
      "Training Epoch: 2 [52352/60000 (87%)]\tLoss: 0.308532\n",
      "Training Epoch: 2 [52384/60000 (87%)]\tLoss: 0.191200\n",
      "Training Epoch: 2 [52416/60000 (87%)]\tLoss: 0.772563\n",
      "Training Epoch: 2 [52448/60000 (87%)]\tLoss: 0.421365\n",
      "Training Epoch: 2 [52480/60000 (87%)]\tLoss: 0.558162\n",
      "Training Epoch: 2 [52512/60000 (88%)]\tLoss: 0.491205\n",
      "Training Epoch: 2 [52544/60000 (88%)]\tLoss: 0.826011\n",
      "Training Epoch: 2 [52576/60000 (88%)]\tLoss: 1.584157\n",
      "Training Epoch: 2 [52608/60000 (88%)]\tLoss: 0.422639\n",
      "Training Epoch: 2 [52640/60000 (88%)]\tLoss: 0.213621\n",
      "Training Epoch: 2 [52672/60000 (88%)]\tLoss: 0.173891\n",
      "Training Epoch: 2 [52704/60000 (88%)]\tLoss: 1.064116\n",
      "Training Epoch: 2 [52736/60000 (88%)]\tLoss: 0.726769\n",
      "Training Epoch: 2 [52768/60000 (88%)]\tLoss: 0.509499\n",
      "Training Epoch: 2 [52800/60000 (88%)]\tLoss: 0.137609\n",
      "Training Epoch: 2 [52832/60000 (88%)]\tLoss: 0.730693\n",
      "Training Epoch: 2 [52864/60000 (88%)]\tLoss: 0.334705\n",
      "Training Epoch: 2 [52896/60000 (88%)]\tLoss: 0.251542\n",
      "Training Epoch: 2 [52928/60000 (88%)]\tLoss: 0.185960\n",
      "Training Epoch: 2 [52960/60000 (88%)]\tLoss: 0.321491\n",
      "Training Epoch: 2 [52992/60000 (88%)]\tLoss: 0.340696\n",
      "Training Epoch: 2 [53024/60000 (88%)]\tLoss: 0.655380\n",
      "Training Epoch: 2 [53056/60000 (88%)]\tLoss: 0.267074\n",
      "Training Epoch: 2 [53088/60000 (88%)]\tLoss: 0.372691\n",
      "Training Epoch: 2 [53120/60000 (89%)]\tLoss: 0.437778\n",
      "Training Epoch: 2 [53152/60000 (89%)]\tLoss: 0.809315\n",
      "Training Epoch: 2 [53184/60000 (89%)]\tLoss: 0.691654\n",
      "Training Epoch: 2 [53216/60000 (89%)]\tLoss: 0.402227\n",
      "Training Epoch: 2 [53248/60000 (89%)]\tLoss: 0.248364\n",
      "Training Epoch: 2 [53280/60000 (89%)]\tLoss: 0.366202\n",
      "Training Epoch: 2 [53312/60000 (89%)]\tLoss: 0.398344\n",
      "Training Epoch: 2 [53344/60000 (89%)]\tLoss: 0.333070\n",
      "Training Epoch: 2 [53376/60000 (89%)]\tLoss: 0.302832\n",
      "Training Epoch: 2 [53408/60000 (89%)]\tLoss: 0.590639\n",
      "Training Epoch: 2 [53440/60000 (89%)]\tLoss: 1.546696\n",
      "Training Epoch: 2 [53472/60000 (89%)]\tLoss: 0.327747\n",
      "Training Epoch: 2 [53504/60000 (89%)]\tLoss: 0.323017\n",
      "Training Epoch: 2 [53536/60000 (89%)]\tLoss: 0.455884\n",
      "Training Epoch: 2 [53568/60000 (89%)]\tLoss: 0.389787\n",
      "Training Epoch: 2 [53600/60000 (89%)]\tLoss: 0.223473\n",
      "Training Epoch: 2 [53632/60000 (89%)]\tLoss: 0.300601\n",
      "Training Epoch: 2 [53664/60000 (89%)]\tLoss: 0.513664\n",
      "Training Epoch: 2 [53696/60000 (89%)]\tLoss: 0.549258\n",
      "Training Epoch: 2 [53728/60000 (90%)]\tLoss: 0.354185\n",
      "Training Epoch: 2 [53760/60000 (90%)]\tLoss: 0.288308\n",
      "Training Epoch: 2 [53792/60000 (90%)]\tLoss: 0.294296\n",
      "Training Epoch: 2 [53824/60000 (90%)]\tLoss: 0.346462\n",
      "Training Epoch: 2 [53856/60000 (90%)]\tLoss: 0.934216\n",
      "Training Epoch: 2 [53888/60000 (90%)]\tLoss: 0.508393\n",
      "Training Epoch: 2 [53920/60000 (90%)]\tLoss: 0.504743\n",
      "Training Epoch: 2 [53952/60000 (90%)]\tLoss: 0.290891\n",
      "Training Epoch: 2 [53984/60000 (90%)]\tLoss: 0.643154\n",
      "Training Epoch: 2 [54016/60000 (90%)]\tLoss: 0.708381\n",
      "Training Epoch: 2 [54048/60000 (90%)]\tLoss: 0.521400\n",
      "Training Epoch: 2 [54080/60000 (90%)]\tLoss: 0.407838\n",
      "Training Epoch: 2 [54112/60000 (90%)]\tLoss: 0.618604\n",
      "Training Epoch: 2 [54144/60000 (90%)]\tLoss: 0.296993\n",
      "Training Epoch: 2 [54176/60000 (90%)]\tLoss: 0.441266\n",
      "Training Epoch: 2 [54208/60000 (90%)]\tLoss: 0.134689\n",
      "Training Epoch: 2 [54240/60000 (90%)]\tLoss: 0.737364\n",
      "Training Epoch: 2 [54272/60000 (90%)]\tLoss: 0.472939\n",
      "Training Epoch: 2 [54304/60000 (91%)]\tLoss: 0.822049\n",
      "Training Epoch: 2 [54336/60000 (91%)]\tLoss: 0.380608\n",
      "Training Epoch: 2 [54368/60000 (91%)]\tLoss: 0.539535\n",
      "Training Epoch: 2 [54400/60000 (91%)]\tLoss: 0.565378\n",
      "Training Epoch: 2 [54432/60000 (91%)]\tLoss: 0.403198\n",
      "Training Epoch: 2 [54464/60000 (91%)]\tLoss: 0.495493\n",
      "Training Epoch: 2 [54496/60000 (91%)]\tLoss: 0.328904\n",
      "Training Epoch: 2 [54528/60000 (91%)]\tLoss: 0.516491\n",
      "Training Epoch: 2 [54560/60000 (91%)]\tLoss: 0.386806\n",
      "Training Epoch: 2 [54592/60000 (91%)]\tLoss: 0.381062\n",
      "Training Epoch: 2 [54624/60000 (91%)]\tLoss: 0.301255\n",
      "Training Epoch: 2 [54656/60000 (91%)]\tLoss: 0.404572\n",
      "Training Epoch: 2 [54688/60000 (91%)]\tLoss: 0.494206\n",
      "Training Epoch: 2 [54720/60000 (91%)]\tLoss: 0.389420\n",
      "Training Epoch: 2 [54752/60000 (91%)]\tLoss: 0.355820\n",
      "Training Epoch: 2 [54784/60000 (91%)]\tLoss: 0.443500\n",
      "Training Epoch: 2 [54816/60000 (91%)]\tLoss: 0.369172\n",
      "Training Epoch: 2 [54848/60000 (91%)]\tLoss: 0.280115\n",
      "Training Epoch: 2 [54880/60000 (91%)]\tLoss: 0.569354\n",
      "Training Epoch: 2 [54912/60000 (92%)]\tLoss: 0.332696\n",
      "Training Epoch: 2 [54944/60000 (92%)]\tLoss: 0.460143\n",
      "Training Epoch: 2 [54976/60000 (92%)]\tLoss: 0.343656\n",
      "Training Epoch: 2 [55008/60000 (92%)]\tLoss: 0.422946\n",
      "Training Epoch: 2 [55040/60000 (92%)]\tLoss: 0.604550\n",
      "Training Epoch: 2 [55072/60000 (92%)]\tLoss: 0.593255\n",
      "Training Epoch: 2 [55104/60000 (92%)]\tLoss: 0.528954\n",
      "Training Epoch: 2 [55136/60000 (92%)]\tLoss: 0.345877\n",
      "Training Epoch: 2 [55168/60000 (92%)]\tLoss: 0.340520\n",
      "Training Epoch: 2 [55200/60000 (92%)]\tLoss: 0.299442\n",
      "Training Epoch: 2 [55232/60000 (92%)]\tLoss: 0.232784\n",
      "Training Epoch: 2 [55264/60000 (92%)]\tLoss: 0.670133\n",
      "Training Epoch: 2 [55296/60000 (92%)]\tLoss: 0.518338\n",
      "Training Epoch: 2 [55328/60000 (92%)]\tLoss: 0.408737\n",
      "Training Epoch: 2 [55360/60000 (92%)]\tLoss: 0.430458\n",
      "Training Epoch: 2 [55392/60000 (92%)]\tLoss: 0.371643\n",
      "Training Epoch: 2 [55424/60000 (92%)]\tLoss: 0.323002\n",
      "Training Epoch: 2 [55456/60000 (92%)]\tLoss: 0.892752\n",
      "Training Epoch: 2 [55488/60000 (92%)]\tLoss: 0.423107\n",
      "Training Epoch: 2 [55520/60000 (93%)]\tLoss: 0.436788\n",
      "Training Epoch: 2 [55552/60000 (93%)]\tLoss: 0.367308\n",
      "Training Epoch: 2 [55584/60000 (93%)]\tLoss: 0.431398\n",
      "Training Epoch: 2 [55616/60000 (93%)]\tLoss: 0.329264\n",
      "Training Epoch: 2 [55648/60000 (93%)]\tLoss: 0.295338\n",
      "Training Epoch: 2 [55680/60000 (93%)]\tLoss: 0.635140\n",
      "Training Epoch: 2 [55712/60000 (93%)]\tLoss: 0.540329\n",
      "Training Epoch: 2 [55744/60000 (93%)]\tLoss: 0.464282\n",
      "Training Epoch: 2 [55776/60000 (93%)]\tLoss: 0.375741\n",
      "Training Epoch: 2 [55808/60000 (93%)]\tLoss: 0.096310\n",
      "Training Epoch: 2 [55840/60000 (93%)]\tLoss: 0.314489\n",
      "Training Epoch: 2 [55872/60000 (93%)]\tLoss: 0.232933\n",
      "Training Epoch: 2 [55904/60000 (93%)]\tLoss: 0.436130\n",
      "Training Epoch: 2 [55936/60000 (93%)]\tLoss: 0.598404\n",
      "Training Epoch: 2 [55968/60000 (93%)]\tLoss: 0.338701\n",
      "Training Epoch: 2 [56000/60000 (93%)]\tLoss: 0.417148\n",
      "Training Epoch: 2 [56032/60000 (93%)]\tLoss: 0.255109\n",
      "Training Epoch: 2 [56064/60000 (93%)]\tLoss: 0.666205\n",
      "Training Epoch: 2 [56096/60000 (93%)]\tLoss: 0.673088\n",
      "Training Epoch: 2 [56128/60000 (94%)]\tLoss: 0.401577\n",
      "Training Epoch: 2 [56160/60000 (94%)]\tLoss: 0.506091\n",
      "Training Epoch: 2 [56192/60000 (94%)]\tLoss: 0.413605\n",
      "Training Epoch: 2 [56224/60000 (94%)]\tLoss: 0.543668\n",
      "Training Epoch: 2 [56256/60000 (94%)]\tLoss: 0.126890\n",
      "Training Epoch: 2 [56288/60000 (94%)]\tLoss: 0.488648\n",
      "Training Epoch: 2 [56320/60000 (94%)]\tLoss: 0.279582\n",
      "Training Epoch: 2 [56352/60000 (94%)]\tLoss: 0.634652\n",
      "Training Epoch: 2 [56384/60000 (94%)]\tLoss: 0.667286\n",
      "Training Epoch: 2 [56416/60000 (94%)]\tLoss: 0.603484\n",
      "Training Epoch: 2 [56448/60000 (94%)]\tLoss: 0.289534\n",
      "Training Epoch: 2 [56480/60000 (94%)]\tLoss: 0.308401\n",
      "Training Epoch: 2 [56512/60000 (94%)]\tLoss: 0.458340\n",
      "Training Epoch: 2 [56544/60000 (94%)]\tLoss: 0.434066\n",
      "Training Epoch: 2 [56576/60000 (94%)]\tLoss: 0.447442\n",
      "Training Epoch: 2 [56608/60000 (94%)]\tLoss: 0.182450\n",
      "Training Epoch: 2 [56640/60000 (94%)]\tLoss: 0.474924\n",
      "Training Epoch: 2 [56672/60000 (94%)]\tLoss: 0.462401\n",
      "Training Epoch: 2 [56704/60000 (95%)]\tLoss: 0.079639\n",
      "Training Epoch: 2 [56736/60000 (95%)]\tLoss: 0.154581\n",
      "Training Epoch: 2 [56768/60000 (95%)]\tLoss: 0.730708\n",
      "Training Epoch: 2 [56800/60000 (95%)]\tLoss: 0.392475\n",
      "Training Epoch: 2 [56832/60000 (95%)]\tLoss: 0.515051\n",
      "Training Epoch: 2 [56864/60000 (95%)]\tLoss: 0.356476\n",
      "Training Epoch: 2 [56896/60000 (95%)]\tLoss: 0.346512\n",
      "Training Epoch: 2 [56928/60000 (95%)]\tLoss: 0.706048\n",
      "Training Epoch: 2 [56960/60000 (95%)]\tLoss: 0.270949\n",
      "Training Epoch: 2 [56992/60000 (95%)]\tLoss: 0.269508\n",
      "Training Epoch: 2 [57024/60000 (95%)]\tLoss: 0.479062\n",
      "Training Epoch: 2 [57056/60000 (95%)]\tLoss: 0.654970\n",
      "Training Epoch: 2 [57088/60000 (95%)]\tLoss: 0.498422\n",
      "Training Epoch: 2 [57120/60000 (95%)]\tLoss: 0.560677\n",
      "Training Epoch: 2 [57152/60000 (95%)]\tLoss: 0.298885\n",
      "Training Epoch: 2 [57184/60000 (95%)]\tLoss: 0.672095\n",
      "Training Epoch: 2 [57216/60000 (95%)]\tLoss: 0.323331\n",
      "Training Epoch: 2 [57248/60000 (95%)]\tLoss: 0.491437\n",
      "Training Epoch: 2 [57280/60000 (95%)]\tLoss: 0.377525\n",
      "Training Epoch: 2 [57312/60000 (96%)]\tLoss: 0.450226\n",
      "Training Epoch: 2 [57344/60000 (96%)]\tLoss: 0.192147\n",
      "Training Epoch: 2 [57376/60000 (96%)]\tLoss: 0.306238\n",
      "Training Epoch: 2 [57408/60000 (96%)]\tLoss: 0.435804\n",
      "Training Epoch: 2 [57440/60000 (96%)]\tLoss: 0.397295\n",
      "Training Epoch: 2 [57472/60000 (96%)]\tLoss: 0.572202\n",
      "Training Epoch: 2 [57504/60000 (96%)]\tLoss: 0.361661\n",
      "Training Epoch: 2 [57536/60000 (96%)]\tLoss: 0.407828\n",
      "Training Epoch: 2 [57568/60000 (96%)]\tLoss: 0.439057\n",
      "Training Epoch: 2 [57600/60000 (96%)]\tLoss: 0.517667\n",
      "Training Epoch: 2 [57632/60000 (96%)]\tLoss: 0.660072\n",
      "Training Epoch: 2 [57664/60000 (96%)]\tLoss: 0.439465\n",
      "Training Epoch: 2 [57696/60000 (96%)]\tLoss: 0.873652\n",
      "Training Epoch: 2 [57728/60000 (96%)]\tLoss: 0.366842\n",
      "Training Epoch: 2 [57760/60000 (96%)]\tLoss: 0.458688\n",
      "Training Epoch: 2 [57792/60000 (96%)]\tLoss: 0.820034\n",
      "Training Epoch: 2 [57824/60000 (96%)]\tLoss: 0.271925\n",
      "Training Epoch: 2 [57856/60000 (96%)]\tLoss: 0.336099\n",
      "Training Epoch: 2 [57888/60000 (96%)]\tLoss: 0.160746\n",
      "Training Epoch: 2 [57920/60000 (97%)]\tLoss: 0.256068\n",
      "Training Epoch: 2 [57952/60000 (97%)]\tLoss: 0.287595\n",
      "Training Epoch: 2 [57984/60000 (97%)]\tLoss: 0.610286\n",
      "Training Epoch: 2 [58016/60000 (97%)]\tLoss: 0.426823\n",
      "Training Epoch: 2 [58048/60000 (97%)]\tLoss: 0.133922\n",
      "Training Epoch: 2 [58080/60000 (97%)]\tLoss: 0.240607\n",
      "Training Epoch: 2 [58112/60000 (97%)]\tLoss: 0.407408\n",
      "Training Epoch: 2 [58144/60000 (97%)]\tLoss: 0.462216\n",
      "Training Epoch: 2 [58176/60000 (97%)]\tLoss: 0.536363\n",
      "Training Epoch: 2 [58208/60000 (97%)]\tLoss: 0.306059\n",
      "Training Epoch: 2 [58240/60000 (97%)]\tLoss: 0.309381\n",
      "Training Epoch: 2 [58272/60000 (97%)]\tLoss: 0.483583\n",
      "Training Epoch: 2 [58304/60000 (97%)]\tLoss: 0.478407\n",
      "Training Epoch: 2 [58336/60000 (97%)]\tLoss: 0.210881\n",
      "Training Epoch: 2 [58368/60000 (97%)]\tLoss: 0.257094\n",
      "Training Epoch: 2 [58400/60000 (97%)]\tLoss: 0.396798\n",
      "Training Epoch: 2 [58432/60000 (97%)]\tLoss: 0.529843\n",
      "Training Epoch: 2 [58464/60000 (97%)]\tLoss: 0.296652\n",
      "Training Epoch: 2 [58496/60000 (97%)]\tLoss: 0.155571\n",
      "Training Epoch: 2 [58528/60000 (98%)]\tLoss: 0.297183\n",
      "Training Epoch: 2 [58560/60000 (98%)]\tLoss: 0.259301\n",
      "Training Epoch: 2 [58592/60000 (98%)]\tLoss: 0.443468\n",
      "Training Epoch: 2 [58624/60000 (98%)]\tLoss: 0.375260\n",
      "Training Epoch: 2 [58656/60000 (98%)]\tLoss: 0.235066\n",
      "Training Epoch: 2 [58688/60000 (98%)]\tLoss: 0.393981\n",
      "Training Epoch: 2 [58720/60000 (98%)]\tLoss: 0.456453\n",
      "Training Epoch: 2 [58752/60000 (98%)]\tLoss: 0.317747\n",
      "Training Epoch: 2 [58784/60000 (98%)]\tLoss: 0.206521\n",
      "Training Epoch: 2 [58816/60000 (98%)]\tLoss: 0.404832\n",
      "Training Epoch: 2 [58848/60000 (98%)]\tLoss: 0.122010\n",
      "Training Epoch: 2 [58880/60000 (98%)]\tLoss: 0.343289\n",
      "Training Epoch: 2 [58912/60000 (98%)]\tLoss: 0.587339\n",
      "Training Epoch: 2 [58944/60000 (98%)]\tLoss: 0.273306\n",
      "Training Epoch: 2 [58976/60000 (98%)]\tLoss: 0.246978\n",
      "Training Epoch: 2 [59008/60000 (98%)]\tLoss: 0.536808\n",
      "Training Epoch: 2 [59040/60000 (98%)]\tLoss: 0.354098\n",
      "Training Epoch: 2 [59072/60000 (98%)]\tLoss: 0.326314\n",
      "Training Epoch: 2 [59104/60000 (99%)]\tLoss: 0.154826\n",
      "Training Epoch: 2 [59136/60000 (99%)]\tLoss: 0.239084\n",
      "Training Epoch: 2 [59168/60000 (99%)]\tLoss: 0.576894\n",
      "Training Epoch: 2 [59200/60000 (99%)]\tLoss: 0.418235\n",
      "Training Epoch: 2 [59232/60000 (99%)]\tLoss: 0.390649\n",
      "Training Epoch: 2 [59264/60000 (99%)]\tLoss: 0.451754\n",
      "Training Epoch: 2 [59296/60000 (99%)]\tLoss: 0.138026\n",
      "Training Epoch: 2 [59328/60000 (99%)]\tLoss: 0.675265\n",
      "Training Epoch: 2 [59360/60000 (99%)]\tLoss: 0.522846\n",
      "Training Epoch: 2 [59392/60000 (99%)]\tLoss: 0.900106\n",
      "Training Epoch: 2 [59424/60000 (99%)]\tLoss: 0.381088\n",
      "Training Epoch: 2 [59456/60000 (99%)]\tLoss: 0.325576\n",
      "Training Epoch: 2 [59488/60000 (99%)]\tLoss: 0.201488\n",
      "Training Epoch: 2 [59520/60000 (99%)]\tLoss: 0.219016\n",
      "Training Epoch: 2 [59552/60000 (99%)]\tLoss: 0.132376\n",
      "Training Epoch: 2 [59584/60000 (99%)]\tLoss: 0.203131\n",
      "Training Epoch: 2 [59616/60000 (99%)]\tLoss: 0.656474\n",
      "Training Epoch: 2 [59648/60000 (99%)]\tLoss: 0.405677\n",
      "Training Epoch: 2 [59680/60000 (99%)]\tLoss: 0.281518\n",
      "Training Epoch: 2 [59712/60000 (100%)]\tLoss: 0.262809\n",
      "Training Epoch: 2 [59744/60000 (100%)]\tLoss: 0.347148\n",
      "Training Epoch: 2 [59776/60000 (100%)]\tLoss: 0.252937\n",
      "Training Epoch: 2 [59808/60000 (100%)]\tLoss: 0.478596\n",
      "Training Epoch: 2 [59840/60000 (100%)]\tLoss: 0.320738\n",
      "Training Epoch: 2 [59872/60000 (100%)]\tLoss: 0.326178\n",
      "Training Epoch: 2 [59904/60000 (100%)]\tLoss: 0.414052\n",
      "Training Epoch: 2 [59936/60000 (100%)]\tLoss: 0.443703\n",
      "Training Epoch: 2 [59968/60000 (100%)]\tLoss: 0.555979\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9681/10000 (97%)\n",
      "\n",
      "Training Epoch: 3 [0/60000 (0%)]\tLoss: 0.237766\n",
      "Training Epoch: 3 [32/60000 (0%)]\tLoss: 0.103545\n",
      "Training Epoch: 3 [64/60000 (0%)]\tLoss: 0.123599\n",
      "Training Epoch: 3 [96/60000 (0%)]\tLoss: 0.312849\n",
      "Training Epoch: 3 [128/60000 (0%)]\tLoss: 0.111117\n",
      "Training Epoch: 3 [160/60000 (0%)]\tLoss: 0.197990\n",
      "Training Epoch: 3 [192/60000 (0%)]\tLoss: 0.326762\n",
      "Training Epoch: 3 [224/60000 (0%)]\tLoss: 0.306722\n",
      "Training Epoch: 3 [256/60000 (0%)]\tLoss: 0.102314\n",
      "Training Epoch: 3 [288/60000 (0%)]\tLoss: 0.346969\n",
      "Training Epoch: 3 [320/60000 (1%)]\tLoss: 0.488289\n",
      "Training Epoch: 3 [352/60000 (1%)]\tLoss: 0.731813\n",
      "Training Epoch: 3 [384/60000 (1%)]\tLoss: 0.507407\n",
      "Training Epoch: 3 [416/60000 (1%)]\tLoss: 0.587127\n",
      "Training Epoch: 3 [448/60000 (1%)]\tLoss: 0.884663\n",
      "Training Epoch: 3 [480/60000 (1%)]\tLoss: 0.535208\n",
      "Training Epoch: 3 [512/60000 (1%)]\tLoss: 0.242369\n",
      "Training Epoch: 3 [544/60000 (1%)]\tLoss: 0.478817\n",
      "Training Epoch: 3 [576/60000 (1%)]\tLoss: 0.300723\n",
      "Training Epoch: 3 [608/60000 (1%)]\tLoss: 0.174908\n",
      "Training Epoch: 3 [640/60000 (1%)]\tLoss: 0.398746\n",
      "Training Epoch: 3 [672/60000 (1%)]\tLoss: 0.908714\n",
      "Training Epoch: 3 [704/60000 (1%)]\tLoss: 0.313437\n",
      "Training Epoch: 3 [736/60000 (1%)]\tLoss: 0.944662\n",
      "Training Epoch: 3 [768/60000 (1%)]\tLoss: 0.511205\n",
      "Training Epoch: 3 [800/60000 (1%)]\tLoss: 0.328751\n",
      "Training Epoch: 3 [832/60000 (1%)]\tLoss: 0.190069\n",
      "Training Epoch: 3 [864/60000 (1%)]\tLoss: 0.142454\n",
      "Training Epoch: 3 [896/60000 (1%)]\tLoss: 0.427995\n",
      "Training Epoch: 3 [928/60000 (2%)]\tLoss: 0.568606\n",
      "Training Epoch: 3 [960/60000 (2%)]\tLoss: 0.721202\n",
      "Training Epoch: 3 [992/60000 (2%)]\tLoss: 1.080065\n",
      "Training Epoch: 3 [1024/60000 (2%)]\tLoss: 0.259427\n",
      "Training Epoch: 3 [1056/60000 (2%)]\tLoss: 0.995225\n",
      "Training Epoch: 3 [1088/60000 (2%)]\tLoss: 0.412522\n",
      "Training Epoch: 3 [1120/60000 (2%)]\tLoss: 0.301047\n",
      "Training Epoch: 3 [1152/60000 (2%)]\tLoss: 0.303591\n",
      "Training Epoch: 3 [1184/60000 (2%)]\tLoss: 0.364287\n",
      "Training Epoch: 3 [1216/60000 (2%)]\tLoss: 0.302222\n",
      "Training Epoch: 3 [1248/60000 (2%)]\tLoss: 0.453735\n",
      "Training Epoch: 3 [1280/60000 (2%)]\tLoss: 0.674773\n",
      "Training Epoch: 3 [1312/60000 (2%)]\tLoss: 0.279913\n",
      "Training Epoch: 3 [1344/60000 (2%)]\tLoss: 0.336781\n",
      "Training Epoch: 3 [1376/60000 (2%)]\tLoss: 0.203453\n",
      "Training Epoch: 3 [1408/60000 (2%)]\tLoss: 0.269849\n",
      "Training Epoch: 3 [1440/60000 (2%)]\tLoss: 0.496376\n",
      "Training Epoch: 3 [1472/60000 (2%)]\tLoss: 0.539454\n",
      "Training Epoch: 3 [1504/60000 (3%)]\tLoss: 0.502788\n",
      "Training Epoch: 3 [1536/60000 (3%)]\tLoss: 0.436685\n",
      "Training Epoch: 3 [1568/60000 (3%)]\tLoss: 0.577273\n",
      "Training Epoch: 3 [1600/60000 (3%)]\tLoss: 0.296325\n",
      "Training Epoch: 3 [1632/60000 (3%)]\tLoss: 0.296523\n",
      "Training Epoch: 3 [1664/60000 (3%)]\tLoss: 0.668388\n",
      "Training Epoch: 3 [1696/60000 (3%)]\tLoss: 0.207673\n",
      "Training Epoch: 3 [1728/60000 (3%)]\tLoss: 0.570098\n",
      "Training Epoch: 3 [1760/60000 (3%)]\tLoss: 0.280541\n",
      "Training Epoch: 3 [1792/60000 (3%)]\tLoss: 0.199779\n",
      "Training Epoch: 3 [1824/60000 (3%)]\tLoss: 0.318957\n",
      "Training Epoch: 3 [1856/60000 (3%)]\tLoss: 0.392458\n",
      "Training Epoch: 3 [1888/60000 (3%)]\tLoss: 0.172388\n",
      "Training Epoch: 3 [1920/60000 (3%)]\tLoss: 0.409393\n",
      "Training Epoch: 3 [1952/60000 (3%)]\tLoss: 0.615151\n",
      "Training Epoch: 3 [1984/60000 (3%)]\tLoss: 0.621587\n",
      "Training Epoch: 3 [2016/60000 (3%)]\tLoss: 0.580557\n",
      "Training Epoch: 3 [2048/60000 (3%)]\tLoss: 0.399496\n",
      "Training Epoch: 3 [2080/60000 (3%)]\tLoss: 0.404012\n",
      "Training Epoch: 3 [2112/60000 (4%)]\tLoss: 0.259918\n",
      "Training Epoch: 3 [2144/60000 (4%)]\tLoss: 0.143678\n",
      "Training Epoch: 3 [2176/60000 (4%)]\tLoss: 0.263400\n",
      "Training Epoch: 3 [2208/60000 (4%)]\tLoss: 0.366093\n",
      "Training Epoch: 3 [2240/60000 (4%)]\tLoss: 0.638415\n",
      "Training Epoch: 3 [2272/60000 (4%)]\tLoss: 0.374523\n",
      "Training Epoch: 3 [2304/60000 (4%)]\tLoss: 0.193151\n",
      "Training Epoch: 3 [2336/60000 (4%)]\tLoss: 0.177938\n",
      "Training Epoch: 3 [2368/60000 (4%)]\tLoss: 0.838983\n",
      "Training Epoch: 3 [2400/60000 (4%)]\tLoss: 0.196991\n",
      "Training Epoch: 3 [2432/60000 (4%)]\tLoss: 0.236508\n",
      "Training Epoch: 3 [2464/60000 (4%)]\tLoss: 0.843505\n",
      "Training Epoch: 3 [2496/60000 (4%)]\tLoss: 0.804099\n",
      "Training Epoch: 3 [2528/60000 (4%)]\tLoss: 0.247534\n",
      "Training Epoch: 3 [2560/60000 (4%)]\tLoss: 0.727358\n",
      "Training Epoch: 3 [2592/60000 (4%)]\tLoss: 0.405263\n",
      "Training Epoch: 3 [2624/60000 (4%)]\tLoss: 0.539868\n",
      "Training Epoch: 3 [2656/60000 (4%)]\tLoss: 0.505583\n",
      "Training Epoch: 3 [2688/60000 (4%)]\tLoss: 0.597642\n",
      "Training Epoch: 3 [2720/60000 (5%)]\tLoss: 0.288357\n",
      "Training Epoch: 3 [2752/60000 (5%)]\tLoss: 0.406108\n",
      "Training Epoch: 3 [2784/60000 (5%)]\tLoss: 0.277832\n",
      "Training Epoch: 3 [2816/60000 (5%)]\tLoss: 0.187689\n",
      "Training Epoch: 3 [2848/60000 (5%)]\tLoss: 0.344089\n",
      "Training Epoch: 3 [2880/60000 (5%)]\tLoss: 0.595890\n",
      "Training Epoch: 3 [2912/60000 (5%)]\tLoss: 0.349685\n",
      "Training Epoch: 3 [2944/60000 (5%)]\tLoss: 0.525260\n",
      "Training Epoch: 3 [2976/60000 (5%)]\tLoss: 0.485298\n",
      "Training Epoch: 3 [3008/60000 (5%)]\tLoss: 0.432181\n",
      "Training Epoch: 3 [3040/60000 (5%)]\tLoss: 0.402041\n",
      "Training Epoch: 3 [3072/60000 (5%)]\tLoss: 0.840361\n",
      "Training Epoch: 3 [3104/60000 (5%)]\tLoss: 0.431011\n",
      "Training Epoch: 3 [3136/60000 (5%)]\tLoss: 0.341033\n",
      "Training Epoch: 3 [3168/60000 (5%)]\tLoss: 0.438191\n",
      "Training Epoch: 3 [3200/60000 (5%)]\tLoss: 0.204120\n",
      "Training Epoch: 3 [3232/60000 (5%)]\tLoss: 0.249024\n",
      "Training Epoch: 3 [3264/60000 (5%)]\tLoss: 0.291143\n",
      "Training Epoch: 3 [3296/60000 (5%)]\tLoss: 0.358372\n",
      "Training Epoch: 3 [3328/60000 (6%)]\tLoss: 0.325229\n",
      "Training Epoch: 3 [3360/60000 (6%)]\tLoss: 1.243128\n",
      "Training Epoch: 3 [3392/60000 (6%)]\tLoss: 0.523706\n",
      "Training Epoch: 3 [3424/60000 (6%)]\tLoss: 0.466898\n",
      "Training Epoch: 3 [3456/60000 (6%)]\tLoss: 0.508581\n",
      "Training Epoch: 3 [3488/60000 (6%)]\tLoss: 0.163609\n",
      "Training Epoch: 3 [3520/60000 (6%)]\tLoss: 0.398060\n",
      "Training Epoch: 3 [3552/60000 (6%)]\tLoss: 0.347029\n",
      "Training Epoch: 3 [3584/60000 (6%)]\tLoss: 0.617441\n",
      "Training Epoch: 3 [3616/60000 (6%)]\tLoss: 0.439009\n",
      "Training Epoch: 3 [3648/60000 (6%)]\tLoss: 0.196073\n",
      "Training Epoch: 3 [3680/60000 (6%)]\tLoss: 0.259100\n",
      "Training Epoch: 3 [3712/60000 (6%)]\tLoss: 0.370622\n",
      "Training Epoch: 3 [3744/60000 (6%)]\tLoss: 0.257450\n",
      "Training Epoch: 3 [3776/60000 (6%)]\tLoss: 0.257322\n",
      "Training Epoch: 3 [3808/60000 (6%)]\tLoss: 0.248718\n",
      "Training Epoch: 3 [3840/60000 (6%)]\tLoss: 0.237018\n",
      "Training Epoch: 3 [3872/60000 (6%)]\tLoss: 0.450665\n",
      "Training Epoch: 3 [3904/60000 (7%)]\tLoss: 0.278145\n",
      "Training Epoch: 3 [3936/60000 (7%)]\tLoss: 0.311661\n",
      "Training Epoch: 3 [3968/60000 (7%)]\tLoss: 0.250776\n",
      "Training Epoch: 3 [4000/60000 (7%)]\tLoss: 0.378987\n",
      "Training Epoch: 3 [4032/60000 (7%)]\tLoss: 0.649989\n",
      "Training Epoch: 3 [4064/60000 (7%)]\tLoss: 0.641150\n",
      "Training Epoch: 3 [4096/60000 (7%)]\tLoss: 0.318459\n",
      "Training Epoch: 3 [4128/60000 (7%)]\tLoss: 0.563034\n",
      "Training Epoch: 3 [4160/60000 (7%)]\tLoss: 0.368177\n",
      "Training Epoch: 3 [4192/60000 (7%)]\tLoss: 0.621316\n",
      "Training Epoch: 3 [4224/60000 (7%)]\tLoss: 0.237424\n",
      "Training Epoch: 3 [4256/60000 (7%)]\tLoss: 0.403166\n",
      "Training Epoch: 3 [4288/60000 (7%)]\tLoss: 0.313543\n",
      "Training Epoch: 3 [4320/60000 (7%)]\tLoss: 0.080560\n",
      "Training Epoch: 3 [4352/60000 (7%)]\tLoss: 0.347933\n",
      "Training Epoch: 3 [4384/60000 (7%)]\tLoss: 0.208525\n",
      "Training Epoch: 3 [4416/60000 (7%)]\tLoss: 0.432439\n",
      "Training Epoch: 3 [4448/60000 (7%)]\tLoss: 0.210751\n",
      "Training Epoch: 3 [4480/60000 (7%)]\tLoss: 0.627247\n",
      "Training Epoch: 3 [4512/60000 (8%)]\tLoss: 0.060973\n",
      "Training Epoch: 3 [4544/60000 (8%)]\tLoss: 0.354292\n",
      "Training Epoch: 3 [4576/60000 (8%)]\tLoss: 0.332065\n",
      "Training Epoch: 3 [4608/60000 (8%)]\tLoss: 0.310451\n",
      "Training Epoch: 3 [4640/60000 (8%)]\tLoss: 0.225534\n",
      "Training Epoch: 3 [4672/60000 (8%)]\tLoss: 0.351605\n",
      "Training Epoch: 3 [4704/60000 (8%)]\tLoss: 0.509560\n",
      "Training Epoch: 3 [4736/60000 (8%)]\tLoss: 0.257722\n",
      "Training Epoch: 3 [4768/60000 (8%)]\tLoss: 0.358044\n",
      "Training Epoch: 3 [4800/60000 (8%)]\tLoss: 0.182284\n",
      "Training Epoch: 3 [4832/60000 (8%)]\tLoss: 0.292664\n",
      "Training Epoch: 3 [4864/60000 (8%)]\tLoss: 0.611351\n",
      "Training Epoch: 3 [4896/60000 (8%)]\tLoss: 0.526057\n",
      "Training Epoch: 3 [4928/60000 (8%)]\tLoss: 0.237059\n",
      "Training Epoch: 3 [4960/60000 (8%)]\tLoss: 0.436723\n",
      "Training Epoch: 3 [4992/60000 (8%)]\tLoss: 0.288375\n",
      "Training Epoch: 3 [5024/60000 (8%)]\tLoss: 0.054587\n",
      "Training Epoch: 3 [5056/60000 (8%)]\tLoss: 0.269370\n",
      "Training Epoch: 3 [5088/60000 (8%)]\tLoss: 0.369874\n",
      "Training Epoch: 3 [5120/60000 (9%)]\tLoss: 0.449618\n",
      "Training Epoch: 3 [5152/60000 (9%)]\tLoss: 0.166807\n",
      "Training Epoch: 3 [5184/60000 (9%)]\tLoss: 0.643673\n",
      "Training Epoch: 3 [5216/60000 (9%)]\tLoss: 0.281245\n",
      "Training Epoch: 3 [5248/60000 (9%)]\tLoss: 0.267972\n",
      "Training Epoch: 3 [5280/60000 (9%)]\tLoss: 0.533153\n",
      "Training Epoch: 3 [5312/60000 (9%)]\tLoss: 0.662820\n",
      "Training Epoch: 3 [5344/60000 (9%)]\tLoss: 0.542477\n",
      "Training Epoch: 3 [5376/60000 (9%)]\tLoss: 0.586952\n",
      "Training Epoch: 3 [5408/60000 (9%)]\tLoss: 0.596046\n",
      "Training Epoch: 3 [5440/60000 (9%)]\tLoss: 0.341493\n",
      "Training Epoch: 3 [5472/60000 (9%)]\tLoss: 0.554708\n",
      "Training Epoch: 3 [5504/60000 (9%)]\tLoss: 0.454668\n",
      "Training Epoch: 3 [5536/60000 (9%)]\tLoss: 0.328735\n",
      "Training Epoch: 3 [5568/60000 (9%)]\tLoss: 0.548363\n",
      "Training Epoch: 3 [5600/60000 (9%)]\tLoss: 0.264801\n",
      "Training Epoch: 3 [5632/60000 (9%)]\tLoss: 0.424260\n",
      "Training Epoch: 3 [5664/60000 (9%)]\tLoss: 0.265646\n",
      "Training Epoch: 3 [5696/60000 (9%)]\tLoss: 0.621912\n",
      "Training Epoch: 3 [5728/60000 (10%)]\tLoss: 0.326152\n",
      "Training Epoch: 3 [5760/60000 (10%)]\tLoss: 0.541823\n",
      "Training Epoch: 3 [5792/60000 (10%)]\tLoss: 0.533276\n",
      "Training Epoch: 3 [5824/60000 (10%)]\tLoss: 0.426203\n",
      "Training Epoch: 3 [5856/60000 (10%)]\tLoss: 0.500067\n",
      "Training Epoch: 3 [5888/60000 (10%)]\tLoss: 0.493431\n",
      "Training Epoch: 3 [5920/60000 (10%)]\tLoss: 0.260869\n",
      "Training Epoch: 3 [5952/60000 (10%)]\tLoss: 0.594591\n",
      "Training Epoch: 3 [5984/60000 (10%)]\tLoss: 0.287006\n",
      "Training Epoch: 3 [6016/60000 (10%)]\tLoss: 0.427650\n",
      "Training Epoch: 3 [6048/60000 (10%)]\tLoss: 0.060774\n",
      "Training Epoch: 3 [6080/60000 (10%)]\tLoss: 0.935117\n",
      "Training Epoch: 3 [6112/60000 (10%)]\tLoss: 0.574658\n",
      "Training Epoch: 3 [6144/60000 (10%)]\tLoss: 0.345811\n",
      "Training Epoch: 3 [6176/60000 (10%)]\tLoss: 0.567217\n",
      "Training Epoch: 3 [6208/60000 (10%)]\tLoss: 0.424504\n",
      "Training Epoch: 3 [6240/60000 (10%)]\tLoss: 0.541864\n",
      "Training Epoch: 3 [6272/60000 (10%)]\tLoss: 0.424047\n",
      "Training Epoch: 3 [6304/60000 (11%)]\tLoss: 0.314259\n",
      "Training Epoch: 3 [6336/60000 (11%)]\tLoss: 0.489336\n",
      "Training Epoch: 3 [6368/60000 (11%)]\tLoss: 0.331493\n",
      "Training Epoch: 3 [6400/60000 (11%)]\tLoss: 0.345962\n",
      "Training Epoch: 3 [6432/60000 (11%)]\tLoss: 0.722309\n",
      "Training Epoch: 3 [6464/60000 (11%)]\tLoss: 0.588729\n",
      "Training Epoch: 3 [6496/60000 (11%)]\tLoss: 0.329384\n",
      "Training Epoch: 3 [6528/60000 (11%)]\tLoss: 0.767923\n",
      "Training Epoch: 3 [6560/60000 (11%)]\tLoss: 0.484146\n",
      "Training Epoch: 3 [6592/60000 (11%)]\tLoss: 0.382620\n",
      "Training Epoch: 3 [6624/60000 (11%)]\tLoss: 0.704202\n",
      "Training Epoch: 3 [6656/60000 (11%)]\tLoss: 0.483239\n",
      "Training Epoch: 3 [6688/60000 (11%)]\tLoss: 0.261628\n",
      "Training Epoch: 3 [6720/60000 (11%)]\tLoss: 0.414545\n",
      "Training Epoch: 3 [6752/60000 (11%)]\tLoss: 0.439329\n",
      "Training Epoch: 3 [6784/60000 (11%)]\tLoss: 0.653046\n",
      "Training Epoch: 3 [6816/60000 (11%)]\tLoss: 0.409819\n",
      "Training Epoch: 3 [6848/60000 (11%)]\tLoss: 0.275800\n",
      "Training Epoch: 3 [6880/60000 (11%)]\tLoss: 1.020493\n",
      "Training Epoch: 3 [6912/60000 (12%)]\tLoss: 0.182857\n",
      "Training Epoch: 3 [6944/60000 (12%)]\tLoss: 0.386969\n",
      "Training Epoch: 3 [6976/60000 (12%)]\tLoss: 0.342880\n",
      "Training Epoch: 3 [7008/60000 (12%)]\tLoss: 0.303969\n",
      "Training Epoch: 3 [7040/60000 (12%)]\tLoss: 0.358823\n",
      "Training Epoch: 3 [7072/60000 (12%)]\tLoss: 0.466952\n",
      "Training Epoch: 3 [7104/60000 (12%)]\tLoss: 0.496785\n",
      "Training Epoch: 3 [7136/60000 (12%)]\tLoss: 0.148990\n",
      "Training Epoch: 3 [7168/60000 (12%)]\tLoss: 0.276271\n",
      "Training Epoch: 3 [7200/60000 (12%)]\tLoss: 0.287268\n",
      "Training Epoch: 3 [7232/60000 (12%)]\tLoss: 0.242320\n",
      "Training Epoch: 3 [7264/60000 (12%)]\tLoss: 0.328253\n",
      "Training Epoch: 3 [7296/60000 (12%)]\tLoss: 0.307555\n",
      "Training Epoch: 3 [7328/60000 (12%)]\tLoss: 0.568877\n",
      "Training Epoch: 3 [7360/60000 (12%)]\tLoss: 0.420181\n",
      "Training Epoch: 3 [7392/60000 (12%)]\tLoss: 0.328404\n",
      "Training Epoch: 3 [7424/60000 (12%)]\tLoss: 0.357514\n",
      "Training Epoch: 3 [7456/60000 (12%)]\tLoss: 0.254079\n",
      "Training Epoch: 3 [7488/60000 (12%)]\tLoss: 0.451025\n",
      "Training Epoch: 3 [7520/60000 (13%)]\tLoss: 0.460621\n",
      "Training Epoch: 3 [7552/60000 (13%)]\tLoss: 0.657889\n",
      "Training Epoch: 3 [7584/60000 (13%)]\tLoss: 0.672892\n",
      "Training Epoch: 3 [7616/60000 (13%)]\tLoss: 0.593670\n",
      "Training Epoch: 3 [7648/60000 (13%)]\tLoss: 0.378946\n",
      "Training Epoch: 3 [7680/60000 (13%)]\tLoss: 0.588206\n",
      "Training Epoch: 3 [7712/60000 (13%)]\tLoss: 0.598987\n",
      "Training Epoch: 3 [7744/60000 (13%)]\tLoss: 0.290199\n",
      "Training Epoch: 3 [7776/60000 (13%)]\tLoss: 0.583531\n",
      "Training Epoch: 3 [7808/60000 (13%)]\tLoss: 0.155166\n",
      "Training Epoch: 3 [7840/60000 (13%)]\tLoss: 0.195943\n",
      "Training Epoch: 3 [7872/60000 (13%)]\tLoss: 0.294466\n",
      "Training Epoch: 3 [7904/60000 (13%)]\tLoss: 0.757822\n",
      "Training Epoch: 3 [7936/60000 (13%)]\tLoss: 0.333313\n",
      "Training Epoch: 3 [7968/60000 (13%)]\tLoss: 0.409488\n",
      "Training Epoch: 3 [8000/60000 (13%)]\tLoss: 0.392959\n",
      "Training Epoch: 3 [8032/60000 (13%)]\tLoss: 0.169541\n",
      "Training Epoch: 3 [8064/60000 (13%)]\tLoss: 0.615805\n",
      "Training Epoch: 3 [8096/60000 (13%)]\tLoss: 0.553664\n",
      "Training Epoch: 3 [8128/60000 (14%)]\tLoss: 0.558126\n",
      "Training Epoch: 3 [8160/60000 (14%)]\tLoss: 0.385700\n",
      "Training Epoch: 3 [8192/60000 (14%)]\tLoss: 0.273272\n",
      "Training Epoch: 3 [8224/60000 (14%)]\tLoss: 0.369548\n",
      "Training Epoch: 3 [8256/60000 (14%)]\tLoss: 0.748386\n",
      "Training Epoch: 3 [8288/60000 (14%)]\tLoss: 0.354088\n",
      "Training Epoch: 3 [8320/60000 (14%)]\tLoss: 0.658509\n",
      "Training Epoch: 3 [8352/60000 (14%)]\tLoss: 0.427566\n",
      "Training Epoch: 3 [8384/60000 (14%)]\tLoss: 0.496916\n",
      "Training Epoch: 3 [8416/60000 (14%)]\tLoss: 0.297328\n",
      "Training Epoch: 3 [8448/60000 (14%)]\tLoss: 0.350944\n",
      "Training Epoch: 3 [8480/60000 (14%)]\tLoss: 0.200042\n",
      "Training Epoch: 3 [8512/60000 (14%)]\tLoss: 0.374291\n",
      "Training Epoch: 3 [8544/60000 (14%)]\tLoss: 0.319263\n",
      "Training Epoch: 3 [8576/60000 (14%)]\tLoss: 0.613876\n",
      "Training Epoch: 3 [8608/60000 (14%)]\tLoss: 0.196577\n",
      "Training Epoch: 3 [8640/60000 (14%)]\tLoss: 0.667131\n",
      "Training Epoch: 3 [8672/60000 (14%)]\tLoss: 0.446107\n",
      "Training Epoch: 3 [8704/60000 (15%)]\tLoss: 0.620908\n",
      "Training Epoch: 3 [8736/60000 (15%)]\tLoss: 0.381745\n",
      "Training Epoch: 3 [8768/60000 (15%)]\tLoss: 0.091802\n",
      "Training Epoch: 3 [8800/60000 (15%)]\tLoss: 0.057927\n",
      "Training Epoch: 3 [8832/60000 (15%)]\tLoss: 0.458412\n",
      "Training Epoch: 3 [8864/60000 (15%)]\tLoss: 0.829186\n",
      "Training Epoch: 3 [8896/60000 (15%)]\tLoss: 0.342985\n",
      "Training Epoch: 3 [8928/60000 (15%)]\tLoss: 0.247132\n",
      "Training Epoch: 3 [8960/60000 (15%)]\tLoss: 0.621672\n",
      "Training Epoch: 3 [8992/60000 (15%)]\tLoss: 0.382106\n",
      "Training Epoch: 3 [9024/60000 (15%)]\tLoss: 0.401555\n",
      "Training Epoch: 3 [9056/60000 (15%)]\tLoss: 0.331064\n",
      "Training Epoch: 3 [9088/60000 (15%)]\tLoss: 0.171677\n",
      "Training Epoch: 3 [9120/60000 (15%)]\tLoss: 0.376253\n",
      "Training Epoch: 3 [9152/60000 (15%)]\tLoss: 0.679807\n",
      "Training Epoch: 3 [9184/60000 (15%)]\tLoss: 0.274003\n",
      "Training Epoch: 3 [9216/60000 (15%)]\tLoss: 0.205215\n",
      "Training Epoch: 3 [9248/60000 (15%)]\tLoss: 0.706745\n",
      "Training Epoch: 3 [9280/60000 (15%)]\tLoss: 0.268079\n",
      "Training Epoch: 3 [9312/60000 (16%)]\tLoss: 0.423049\n",
      "Training Epoch: 3 [9344/60000 (16%)]\tLoss: 0.225497\n",
      "Training Epoch: 3 [9376/60000 (16%)]\tLoss: 0.403894\n",
      "Training Epoch: 3 [9408/60000 (16%)]\tLoss: 0.073597\n",
      "Training Epoch: 3 [9440/60000 (16%)]\tLoss: 0.341585\n",
      "Training Epoch: 3 [9472/60000 (16%)]\tLoss: 0.540614\n",
      "Training Epoch: 3 [9504/60000 (16%)]\tLoss: 1.064209\n",
      "Training Epoch: 3 [9536/60000 (16%)]\tLoss: 0.468438\n",
      "Training Epoch: 3 [9568/60000 (16%)]\tLoss: 0.234357\n",
      "Training Epoch: 3 [9600/60000 (16%)]\tLoss: 0.568859\n",
      "Training Epoch: 3 [9632/60000 (16%)]\tLoss: 0.421274\n",
      "Training Epoch: 3 [9664/60000 (16%)]\tLoss: 0.586345\n",
      "Training Epoch: 3 [9696/60000 (16%)]\tLoss: 0.544341\n",
      "Training Epoch: 3 [9728/60000 (16%)]\tLoss: 0.401413\n",
      "Training Epoch: 3 [9760/60000 (16%)]\tLoss: 0.552384\n",
      "Training Epoch: 3 [9792/60000 (16%)]\tLoss: 0.270964\n",
      "Training Epoch: 3 [9824/60000 (16%)]\tLoss: 0.171856\n",
      "Training Epoch: 3 [9856/60000 (16%)]\tLoss: 0.627061\n",
      "Training Epoch: 3 [9888/60000 (16%)]\tLoss: 0.476195\n",
      "Training Epoch: 3 [9920/60000 (17%)]\tLoss: 0.364786\n",
      "Training Epoch: 3 [9952/60000 (17%)]\tLoss: 0.299051\n",
      "Training Epoch: 3 [9984/60000 (17%)]\tLoss: 0.342606\n",
      "Training Epoch: 3 [10016/60000 (17%)]\tLoss: 0.457412\n",
      "Training Epoch: 3 [10048/60000 (17%)]\tLoss: 0.692960\n",
      "Training Epoch: 3 [10080/60000 (17%)]\tLoss: 0.388358\n",
      "Training Epoch: 3 [10112/60000 (17%)]\tLoss: 0.476600\n",
      "Training Epoch: 3 [10144/60000 (17%)]\tLoss: 0.148042\n",
      "Training Epoch: 3 [10176/60000 (17%)]\tLoss: 0.441160\n",
      "Training Epoch: 3 [10208/60000 (17%)]\tLoss: 0.405373\n",
      "Training Epoch: 3 [10240/60000 (17%)]\tLoss: 0.410050\n",
      "Training Epoch: 3 [10272/60000 (17%)]\tLoss: 0.342856\n",
      "Training Epoch: 3 [10304/60000 (17%)]\tLoss: 0.293926\n",
      "Training Epoch: 3 [10336/60000 (17%)]\tLoss: 0.634705\n",
      "Training Epoch: 3 [10368/60000 (17%)]\tLoss: 0.544426\n",
      "Training Epoch: 3 [10400/60000 (17%)]\tLoss: 0.642959\n",
      "Training Epoch: 3 [10432/60000 (17%)]\tLoss: 0.716956\n",
      "Training Epoch: 3 [10464/60000 (17%)]\tLoss: 0.471885\n",
      "Training Epoch: 3 [10496/60000 (17%)]\tLoss: 0.302632\n",
      "Training Epoch: 3 [10528/60000 (18%)]\tLoss: 0.311560\n",
      "Training Epoch: 3 [10560/60000 (18%)]\tLoss: 0.155285\n",
      "Training Epoch: 3 [10592/60000 (18%)]\tLoss: 0.295258\n",
      "Training Epoch: 3 [10624/60000 (18%)]\tLoss: 0.615452\n",
      "Training Epoch: 3 [10656/60000 (18%)]\tLoss: 0.738078\n",
      "Training Epoch: 3 [10688/60000 (18%)]\tLoss: 0.336541\n",
      "Training Epoch: 3 [10720/60000 (18%)]\tLoss: 0.508175\n",
      "Training Epoch: 3 [10752/60000 (18%)]\tLoss: 0.327718\n",
      "Training Epoch: 3 [10784/60000 (18%)]\tLoss: 0.511968\n",
      "Training Epoch: 3 [10816/60000 (18%)]\tLoss: 0.542106\n",
      "Training Epoch: 3 [10848/60000 (18%)]\tLoss: 0.620369\n",
      "Training Epoch: 3 [10880/60000 (18%)]\tLoss: 0.177754\n",
      "Training Epoch: 3 [10912/60000 (18%)]\tLoss: 0.406810\n",
      "Training Epoch: 3 [10944/60000 (18%)]\tLoss: 0.626006\n",
      "Training Epoch: 3 [10976/60000 (18%)]\tLoss: 0.533067\n",
      "Training Epoch: 3 [11008/60000 (18%)]\tLoss: 0.234724\n",
      "Training Epoch: 3 [11040/60000 (18%)]\tLoss: 0.340129\n",
      "Training Epoch: 3 [11072/60000 (18%)]\tLoss: 0.404195\n",
      "Training Epoch: 3 [11104/60000 (19%)]\tLoss: 0.347493\n",
      "Training Epoch: 3 [11136/60000 (19%)]\tLoss: 0.761298\n",
      "Training Epoch: 3 [11168/60000 (19%)]\tLoss: 0.381326\n",
      "Training Epoch: 3 [11200/60000 (19%)]\tLoss: 0.367576\n",
      "Training Epoch: 3 [11232/60000 (19%)]\tLoss: 0.365698\n",
      "Training Epoch: 3 [11264/60000 (19%)]\tLoss: 0.702208\n",
      "Training Epoch: 3 [11296/60000 (19%)]\tLoss: 0.440907\n",
      "Training Epoch: 3 [11328/60000 (19%)]\tLoss: 0.355843\n",
      "Training Epoch: 3 [11360/60000 (19%)]\tLoss: 0.338776\n",
      "Training Epoch: 3 [11392/60000 (19%)]\tLoss: 0.413254\n",
      "Training Epoch: 3 [11424/60000 (19%)]\tLoss: 0.487207\n",
      "Training Epoch: 3 [11456/60000 (19%)]\tLoss: 0.261921\n",
      "Training Epoch: 3 [11488/60000 (19%)]\tLoss: 0.817808\n",
      "Training Epoch: 3 [11520/60000 (19%)]\tLoss: 0.442476\n",
      "Training Epoch: 3 [11552/60000 (19%)]\tLoss: 0.272725\n",
      "Training Epoch: 3 [11584/60000 (19%)]\tLoss: 0.223257\n",
      "Training Epoch: 3 [11616/60000 (19%)]\tLoss: 0.352591\n",
      "Training Epoch: 3 [11648/60000 (19%)]\tLoss: 0.740935\n",
      "Training Epoch: 3 [11680/60000 (19%)]\tLoss: 0.434998\n",
      "Training Epoch: 3 [11712/60000 (20%)]\tLoss: 0.203006\n",
      "Training Epoch: 3 [11744/60000 (20%)]\tLoss: 0.482391\n",
      "Training Epoch: 3 [11776/60000 (20%)]\tLoss: 0.424335\n",
      "Training Epoch: 3 [11808/60000 (20%)]\tLoss: 0.350016\n",
      "Training Epoch: 3 [11840/60000 (20%)]\tLoss: 0.557582\n",
      "Training Epoch: 3 [11872/60000 (20%)]\tLoss: 0.462422\n",
      "Training Epoch: 3 [11904/60000 (20%)]\tLoss: 0.581506\n",
      "Training Epoch: 3 [11936/60000 (20%)]\tLoss: 0.340595\n",
      "Training Epoch: 3 [11968/60000 (20%)]\tLoss: 0.528027\n",
      "Training Epoch: 3 [12000/60000 (20%)]\tLoss: 0.223964\n",
      "Training Epoch: 3 [12032/60000 (20%)]\tLoss: 0.943256\n",
      "Training Epoch: 3 [12064/60000 (20%)]\tLoss: 0.216641\n",
      "Training Epoch: 3 [12096/60000 (20%)]\tLoss: 0.308433\n",
      "Training Epoch: 3 [12128/60000 (20%)]\tLoss: 0.170984\n",
      "Training Epoch: 3 [12160/60000 (20%)]\tLoss: 0.322576\n",
      "Training Epoch: 3 [12192/60000 (20%)]\tLoss: 0.556801\n",
      "Training Epoch: 3 [12224/60000 (20%)]\tLoss: 0.405826\n",
      "Training Epoch: 3 [12256/60000 (20%)]\tLoss: 0.399634\n",
      "Training Epoch: 3 [12288/60000 (20%)]\tLoss: 0.259041\n",
      "Training Epoch: 3 [12320/60000 (21%)]\tLoss: 0.399822\n",
      "Training Epoch: 3 [12352/60000 (21%)]\tLoss: 0.443933\n",
      "Training Epoch: 3 [12384/60000 (21%)]\tLoss: 0.664097\n",
      "Training Epoch: 3 [12416/60000 (21%)]\tLoss: 0.224858\n",
      "Training Epoch: 3 [12448/60000 (21%)]\tLoss: 0.315582\n",
      "Training Epoch: 3 [12480/60000 (21%)]\tLoss: 0.124651\n",
      "Training Epoch: 3 [12512/60000 (21%)]\tLoss: 0.495408\n",
      "Training Epoch: 3 [12544/60000 (21%)]\tLoss: 0.611874\n",
      "Training Epoch: 3 [12576/60000 (21%)]\tLoss: 0.991139\n",
      "Training Epoch: 3 [12608/60000 (21%)]\tLoss: 0.303349\n",
      "Training Epoch: 3 [12640/60000 (21%)]\tLoss: 0.496056\n",
      "Training Epoch: 3 [12672/60000 (21%)]\tLoss: 0.224650\n",
      "Training Epoch: 3 [12704/60000 (21%)]\tLoss: 0.478655\n",
      "Training Epoch: 3 [12736/60000 (21%)]\tLoss: 0.253406\n",
      "Training Epoch: 3 [12768/60000 (21%)]\tLoss: 0.186840\n",
      "Training Epoch: 3 [12800/60000 (21%)]\tLoss: 1.041975\n",
      "Training Epoch: 3 [12832/60000 (21%)]\tLoss: 0.669289\n",
      "Training Epoch: 3 [12864/60000 (21%)]\tLoss: 0.277301\n",
      "Training Epoch: 3 [12896/60000 (21%)]\tLoss: 0.303597\n",
      "Training Epoch: 3 [12928/60000 (22%)]\tLoss: 0.361427\n",
      "Training Epoch: 3 [12960/60000 (22%)]\tLoss: 0.226649\n",
      "Training Epoch: 3 [12992/60000 (22%)]\tLoss: 0.533005\n",
      "Training Epoch: 3 [13024/60000 (22%)]\tLoss: 0.276013\n",
      "Training Epoch: 3 [13056/60000 (22%)]\tLoss: 0.414750\n",
      "Training Epoch: 3 [13088/60000 (22%)]\tLoss: 0.332781\n",
      "Training Epoch: 3 [13120/60000 (22%)]\tLoss: 0.261736\n",
      "Training Epoch: 3 [13152/60000 (22%)]\tLoss: 0.302797\n",
      "Training Epoch: 3 [13184/60000 (22%)]\tLoss: 0.725938\n",
      "Training Epoch: 3 [13216/60000 (22%)]\tLoss: 0.563358\n",
      "Training Epoch: 3 [13248/60000 (22%)]\tLoss: 0.869351\n",
      "Training Epoch: 3 [13280/60000 (22%)]\tLoss: 0.333183\n",
      "Training Epoch: 3 [13312/60000 (22%)]\tLoss: 0.568841\n",
      "Training Epoch: 3 [13344/60000 (22%)]\tLoss: 0.159831\n",
      "Training Epoch: 3 [13376/60000 (22%)]\tLoss: 0.403655\n",
      "Training Epoch: 3 [13408/60000 (22%)]\tLoss: 0.523080\n",
      "Training Epoch: 3 [13440/60000 (22%)]\tLoss: 0.339053\n",
      "Training Epoch: 3 [13472/60000 (22%)]\tLoss: 0.371031\n",
      "Training Epoch: 3 [13504/60000 (23%)]\tLoss: 0.907058\n",
      "Training Epoch: 3 [13536/60000 (23%)]\tLoss: 0.201864\n",
      "Training Epoch: 3 [13568/60000 (23%)]\tLoss: 0.135549\n",
      "Training Epoch: 3 [13600/60000 (23%)]\tLoss: 0.170076\n",
      "Training Epoch: 3 [13632/60000 (23%)]\tLoss: 0.444383\n",
      "Training Epoch: 3 [13664/60000 (23%)]\tLoss: 0.696042\n",
      "Training Epoch: 3 [13696/60000 (23%)]\tLoss: 0.605682\n",
      "Training Epoch: 3 [13728/60000 (23%)]\tLoss: 0.244561\n",
      "Training Epoch: 3 [13760/60000 (23%)]\tLoss: 0.688241\n",
      "Training Epoch: 3 [13792/60000 (23%)]\tLoss: 0.535564\n",
      "Training Epoch: 3 [13824/60000 (23%)]\tLoss: 0.714972\n",
      "Training Epoch: 3 [13856/60000 (23%)]\tLoss: 0.724191\n",
      "Training Epoch: 3 [13888/60000 (23%)]\tLoss: 0.337699\n",
      "Training Epoch: 3 [13920/60000 (23%)]\tLoss: 0.379912\n",
      "Training Epoch: 3 [13952/60000 (23%)]\tLoss: 0.250395\n",
      "Training Epoch: 3 [13984/60000 (23%)]\tLoss: 0.356907\n",
      "Training Epoch: 3 [14016/60000 (23%)]\tLoss: 0.496192\n",
      "Training Epoch: 3 [14048/60000 (23%)]\tLoss: 0.233261\n",
      "Training Epoch: 3 [14080/60000 (23%)]\tLoss: 0.769275\n",
      "Training Epoch: 3 [14112/60000 (24%)]\tLoss: 0.422464\n",
      "Training Epoch: 3 [14144/60000 (24%)]\tLoss: 0.347791\n",
      "Training Epoch: 3 [14176/60000 (24%)]\tLoss: 0.350775\n",
      "Training Epoch: 3 [14208/60000 (24%)]\tLoss: 0.752285\n",
      "Training Epoch: 3 [14240/60000 (24%)]\tLoss: 0.409370\n",
      "Training Epoch: 3 [14272/60000 (24%)]\tLoss: 0.546713\n",
      "Training Epoch: 3 [14304/60000 (24%)]\tLoss: 0.352935\n",
      "Training Epoch: 3 [14336/60000 (24%)]\tLoss: 0.503190\n",
      "Training Epoch: 3 [14368/60000 (24%)]\tLoss: 0.255398\n",
      "Training Epoch: 3 [14400/60000 (24%)]\tLoss: 0.718747\n",
      "Training Epoch: 3 [14432/60000 (24%)]\tLoss: 0.600323\n",
      "Training Epoch: 3 [14464/60000 (24%)]\tLoss: 0.787227\n",
      "Training Epoch: 3 [14496/60000 (24%)]\tLoss: 0.324779\n",
      "Training Epoch: 3 [14528/60000 (24%)]\tLoss: 0.298305\n",
      "Training Epoch: 3 [14560/60000 (24%)]\tLoss: 0.371372\n",
      "Training Epoch: 3 [14592/60000 (24%)]\tLoss: 0.336996\n",
      "Training Epoch: 3 [14624/60000 (24%)]\tLoss: 0.477696\n",
      "Training Epoch: 3 [14656/60000 (24%)]\tLoss: 0.676000\n",
      "Training Epoch: 3 [14688/60000 (24%)]\tLoss: 0.685443\n",
      "Training Epoch: 3 [14720/60000 (25%)]\tLoss: 0.322084\n",
      "Training Epoch: 3 [14752/60000 (25%)]\tLoss: 0.553449\n",
      "Training Epoch: 3 [14784/60000 (25%)]\tLoss: 0.554811\n",
      "Training Epoch: 3 [14816/60000 (25%)]\tLoss: 0.169207\n",
      "Training Epoch: 3 [14848/60000 (25%)]\tLoss: 0.819351\n",
      "Training Epoch: 3 [14880/60000 (25%)]\tLoss: 0.206407\n",
      "Training Epoch: 3 [14912/60000 (25%)]\tLoss: 0.179939\n",
      "Training Epoch: 3 [14944/60000 (25%)]\tLoss: 0.415680\n",
      "Training Epoch: 3 [14976/60000 (25%)]\tLoss: 0.445391\n",
      "Training Epoch: 3 [15008/60000 (25%)]\tLoss: 0.241884\n",
      "Training Epoch: 3 [15040/60000 (25%)]\tLoss: 0.404985\n",
      "Training Epoch: 3 [15072/60000 (25%)]\tLoss: 0.257301\n",
      "Training Epoch: 3 [15104/60000 (25%)]\tLoss: 0.299592\n",
      "Training Epoch: 3 [15136/60000 (25%)]\tLoss: 0.238879\n",
      "Training Epoch: 3 [15168/60000 (25%)]\tLoss: 0.772146\n",
      "Training Epoch: 3 [15200/60000 (25%)]\tLoss: 0.574448\n",
      "Training Epoch: 3 [15232/60000 (25%)]\tLoss: 0.137404\n",
      "Training Epoch: 3 [15264/60000 (25%)]\tLoss: 0.417788\n",
      "Training Epoch: 3 [15296/60000 (25%)]\tLoss: 0.688899\n",
      "Training Epoch: 3 [15328/60000 (26%)]\tLoss: 0.147902\n",
      "Training Epoch: 3 [15360/60000 (26%)]\tLoss: 0.377450\n",
      "Training Epoch: 3 [15392/60000 (26%)]\tLoss: 0.364082\n",
      "Training Epoch: 3 [15424/60000 (26%)]\tLoss: 0.513055\n",
      "Training Epoch: 3 [15456/60000 (26%)]\tLoss: 0.400206\n",
      "Training Epoch: 3 [15488/60000 (26%)]\tLoss: 0.769187\n",
      "Training Epoch: 3 [15520/60000 (26%)]\tLoss: 0.697401\n",
      "Training Epoch: 3 [15552/60000 (26%)]\tLoss: 0.310484\n",
      "Training Epoch: 3 [15584/60000 (26%)]\tLoss: 0.508294\n",
      "Training Epoch: 3 [15616/60000 (26%)]\tLoss: 0.483083\n",
      "Training Epoch: 3 [15648/60000 (26%)]\tLoss: 0.421962\n",
      "Training Epoch: 3 [15680/60000 (26%)]\tLoss: 0.460760\n",
      "Training Epoch: 3 [15712/60000 (26%)]\tLoss: 0.445185\n",
      "Training Epoch: 3 [15744/60000 (26%)]\tLoss: 0.112297\n",
      "Training Epoch: 3 [15776/60000 (26%)]\tLoss: 0.540563\n",
      "Training Epoch: 3 [15808/60000 (26%)]\tLoss: 0.597110\n",
      "Training Epoch: 3 [15840/60000 (26%)]\tLoss: 0.554715\n",
      "Training Epoch: 3 [15872/60000 (26%)]\tLoss: 0.282778\n",
      "Training Epoch: 3 [15904/60000 (27%)]\tLoss: 0.243264\n",
      "Training Epoch: 3 [15936/60000 (27%)]\tLoss: 0.211140\n",
      "Training Epoch: 3 [15968/60000 (27%)]\tLoss: 0.226355\n",
      "Training Epoch: 3 [16000/60000 (27%)]\tLoss: 0.318674\n",
      "Training Epoch: 3 [16032/60000 (27%)]\tLoss: 0.236514\n",
      "Training Epoch: 3 [16064/60000 (27%)]\tLoss: 0.312907\n",
      "Training Epoch: 3 [16096/60000 (27%)]\tLoss: 0.510265\n",
      "Training Epoch: 3 [16128/60000 (27%)]\tLoss: 0.339564\n",
      "Training Epoch: 3 [16160/60000 (27%)]\tLoss: 0.338926\n",
      "Training Epoch: 3 [16192/60000 (27%)]\tLoss: 0.603248\n",
      "Training Epoch: 3 [16224/60000 (27%)]\tLoss: 0.409238\n",
      "Training Epoch: 3 [16256/60000 (27%)]\tLoss: 0.353384\n",
      "Training Epoch: 3 [16288/60000 (27%)]\tLoss: 0.533137\n",
      "Training Epoch: 3 [16320/60000 (27%)]\tLoss: 0.407380\n",
      "Training Epoch: 3 [16352/60000 (27%)]\tLoss: 0.312631\n",
      "Training Epoch: 3 [16384/60000 (27%)]\tLoss: 0.310629\n",
      "Training Epoch: 3 [16416/60000 (27%)]\tLoss: 0.385156\n",
      "Training Epoch: 3 [16448/60000 (27%)]\tLoss: 0.467283\n",
      "Training Epoch: 3 [16480/60000 (27%)]\tLoss: 0.428941\n",
      "Training Epoch: 3 [16512/60000 (28%)]\tLoss: 0.106685\n",
      "Training Epoch: 3 [16544/60000 (28%)]\tLoss: 0.352629\n",
      "Training Epoch: 3 [16576/60000 (28%)]\tLoss: 0.477710\n",
      "Training Epoch: 3 [16608/60000 (28%)]\tLoss: 0.375876\n",
      "Training Epoch: 3 [16640/60000 (28%)]\tLoss: 0.197568\n",
      "Training Epoch: 3 [16672/60000 (28%)]\tLoss: 0.280434\n",
      "Training Epoch: 3 [16704/60000 (28%)]\tLoss: 0.221265\n",
      "Training Epoch: 3 [16736/60000 (28%)]\tLoss: 0.377698\n",
      "Training Epoch: 3 [16768/60000 (28%)]\tLoss: 0.438321\n",
      "Training Epoch: 3 [16800/60000 (28%)]\tLoss: 0.134253\n",
      "Training Epoch: 3 [16832/60000 (28%)]\tLoss: 0.518820\n",
      "Training Epoch: 3 [16864/60000 (28%)]\tLoss: 0.485567\n",
      "Training Epoch: 3 [16896/60000 (28%)]\tLoss: 0.300483\n",
      "Training Epoch: 3 [16928/60000 (28%)]\tLoss: 0.342761\n",
      "Training Epoch: 3 [16960/60000 (28%)]\tLoss: 0.277702\n",
      "Training Epoch: 3 [16992/60000 (28%)]\tLoss: 0.338789\n",
      "Training Epoch: 3 [17024/60000 (28%)]\tLoss: 0.331819\n",
      "Training Epoch: 3 [17056/60000 (28%)]\tLoss: 0.498094\n",
      "Training Epoch: 3 [17088/60000 (28%)]\tLoss: 0.108353\n",
      "Training Epoch: 3 [17120/60000 (29%)]\tLoss: 0.628182\n",
      "Training Epoch: 3 [17152/60000 (29%)]\tLoss: 0.824601\n",
      "Training Epoch: 3 [17184/60000 (29%)]\tLoss: 0.650559\n",
      "Training Epoch: 3 [17216/60000 (29%)]\tLoss: 0.920367\n",
      "Training Epoch: 3 [17248/60000 (29%)]\tLoss: 0.798282\n",
      "Training Epoch: 3 [17280/60000 (29%)]\tLoss: 0.464420\n",
      "Training Epoch: 3 [17312/60000 (29%)]\tLoss: 0.356117\n",
      "Training Epoch: 3 [17344/60000 (29%)]\tLoss: 0.715901\n",
      "Training Epoch: 3 [17376/60000 (29%)]\tLoss: 0.594925\n",
      "Training Epoch: 3 [17408/60000 (29%)]\tLoss: 0.286771\n",
      "Training Epoch: 3 [17440/60000 (29%)]\tLoss: 0.285435\n",
      "Training Epoch: 3 [17472/60000 (29%)]\tLoss: 0.431708\n",
      "Training Epoch: 3 [17504/60000 (29%)]\tLoss: 0.291609\n",
      "Training Epoch: 3 [17536/60000 (29%)]\tLoss: 0.748902\n",
      "Training Epoch: 3 [17568/60000 (29%)]\tLoss: 0.271113\n",
      "Training Epoch: 3 [17600/60000 (29%)]\tLoss: 0.572073\n",
      "Training Epoch: 3 [17632/60000 (29%)]\tLoss: 0.600691\n",
      "Training Epoch: 3 [17664/60000 (29%)]\tLoss: 0.348316\n",
      "Training Epoch: 3 [17696/60000 (29%)]\tLoss: 0.515022\n",
      "Training Epoch: 3 [17728/60000 (30%)]\tLoss: 0.313918\n",
      "Training Epoch: 3 [17760/60000 (30%)]\tLoss: 0.844363\n",
      "Training Epoch: 3 [17792/60000 (30%)]\tLoss: 0.401972\n",
      "Training Epoch: 3 [17824/60000 (30%)]\tLoss: 0.124281\n",
      "Training Epoch: 3 [17856/60000 (30%)]\tLoss: 0.450797\n",
      "Training Epoch: 3 [17888/60000 (30%)]\tLoss: 0.287358\n",
      "Training Epoch: 3 [17920/60000 (30%)]\tLoss: 0.299220\n",
      "Training Epoch: 3 [17952/60000 (30%)]\tLoss: 0.554021\n",
      "Training Epoch: 3 [17984/60000 (30%)]\tLoss: 0.511725\n",
      "Training Epoch: 3 [18016/60000 (30%)]\tLoss: 0.176125\n",
      "Training Epoch: 3 [18048/60000 (30%)]\tLoss: 0.358087\n",
      "Training Epoch: 3 [18080/60000 (30%)]\tLoss: 0.287024\n",
      "Training Epoch: 3 [18112/60000 (30%)]\tLoss: 0.454636\n",
      "Training Epoch: 3 [18144/60000 (30%)]\tLoss: 0.198142\n",
      "Training Epoch: 3 [18176/60000 (30%)]\tLoss: 0.241606\n",
      "Training Epoch: 3 [18208/60000 (30%)]\tLoss: 0.213473\n",
      "Training Epoch: 3 [18240/60000 (30%)]\tLoss: 0.073376\n",
      "Training Epoch: 3 [18272/60000 (30%)]\tLoss: 0.104921\n",
      "Training Epoch: 3 [18304/60000 (31%)]\tLoss: 0.588212\n",
      "Training Epoch: 3 [18336/60000 (31%)]\tLoss: 0.623114\n",
      "Training Epoch: 3 [18368/60000 (31%)]\tLoss: 0.635424\n",
      "Training Epoch: 3 [18400/60000 (31%)]\tLoss: 0.143197\n",
      "Training Epoch: 3 [18432/60000 (31%)]\tLoss: 0.373717\n",
      "Training Epoch: 3 [18464/60000 (31%)]\tLoss: 0.936126\n",
      "Training Epoch: 3 [18496/60000 (31%)]\tLoss: 0.235711\n",
      "Training Epoch: 3 [18528/60000 (31%)]\tLoss: 0.689726\n",
      "Training Epoch: 3 [18560/60000 (31%)]\tLoss: 0.355862\n",
      "Training Epoch: 3 [18592/60000 (31%)]\tLoss: 0.155940\n",
      "Training Epoch: 3 [18624/60000 (31%)]\tLoss: 0.210225\n",
      "Training Epoch: 3 [18656/60000 (31%)]\tLoss: 0.859472\n",
      "Training Epoch: 3 [18688/60000 (31%)]\tLoss: 0.252108\n",
      "Training Epoch: 3 [18720/60000 (31%)]\tLoss: 0.565297\n",
      "Training Epoch: 3 [18752/60000 (31%)]\tLoss: 0.414339\n",
      "Training Epoch: 3 [18784/60000 (31%)]\tLoss: 0.173056\n",
      "Training Epoch: 3 [18816/60000 (31%)]\tLoss: 0.197918\n",
      "Training Epoch: 3 [18848/60000 (31%)]\tLoss: 0.579092\n",
      "Training Epoch: 3 [18880/60000 (31%)]\tLoss: 0.543597\n",
      "Training Epoch: 3 [18912/60000 (32%)]\tLoss: 0.164094\n",
      "Training Epoch: 3 [18944/60000 (32%)]\tLoss: 0.309467\n",
      "Training Epoch: 3 [18976/60000 (32%)]\tLoss: 0.492781\n",
      "Training Epoch: 3 [19008/60000 (32%)]\tLoss: 0.811994\n",
      "Training Epoch: 3 [19040/60000 (32%)]\tLoss: 0.595073\n",
      "Training Epoch: 3 [19072/60000 (32%)]\tLoss: 0.149847\n",
      "Training Epoch: 3 [19104/60000 (32%)]\tLoss: 0.380441\n",
      "Training Epoch: 3 [19136/60000 (32%)]\tLoss: 0.661507\n",
      "Training Epoch: 3 [19168/60000 (32%)]\tLoss: 0.231126\n",
      "Training Epoch: 3 [19200/60000 (32%)]\tLoss: 0.338769\n",
      "Training Epoch: 3 [19232/60000 (32%)]\tLoss: 0.428189\n",
      "Training Epoch: 3 [19264/60000 (32%)]\tLoss: 0.581607\n",
      "Training Epoch: 3 [19296/60000 (32%)]\tLoss: 0.237609\n",
      "Training Epoch: 3 [19328/60000 (32%)]\tLoss: 0.334255\n",
      "Training Epoch: 3 [19360/60000 (32%)]\tLoss: 0.294894\n",
      "Training Epoch: 3 [19392/60000 (32%)]\tLoss: 0.269744\n",
      "Training Epoch: 3 [19424/60000 (32%)]\tLoss: 0.121741\n",
      "Training Epoch: 3 [19456/60000 (32%)]\tLoss: 0.496525\n",
      "Training Epoch: 3 [19488/60000 (32%)]\tLoss: 0.247700\n",
      "Training Epoch: 3 [19520/60000 (33%)]\tLoss: 0.236799\n",
      "Training Epoch: 3 [19552/60000 (33%)]\tLoss: 0.866482\n",
      "Training Epoch: 3 [19584/60000 (33%)]\tLoss: 0.352451\n",
      "Training Epoch: 3 [19616/60000 (33%)]\tLoss: 0.533014\n",
      "Training Epoch: 3 [19648/60000 (33%)]\tLoss: 0.309424\n",
      "Training Epoch: 3 [19680/60000 (33%)]\tLoss: 0.359097\n",
      "Training Epoch: 3 [19712/60000 (33%)]\tLoss: 0.437546\n",
      "Training Epoch: 3 [19744/60000 (33%)]\tLoss: 0.365693\n",
      "Training Epoch: 3 [19776/60000 (33%)]\tLoss: 0.310634\n",
      "Training Epoch: 3 [19808/60000 (33%)]\tLoss: 0.663013\n",
      "Training Epoch: 3 [19840/60000 (33%)]\tLoss: 0.048779\n",
      "Training Epoch: 3 [19872/60000 (33%)]\tLoss: 0.337187\n",
      "Training Epoch: 3 [19904/60000 (33%)]\tLoss: 0.101873\n",
      "Training Epoch: 3 [19936/60000 (33%)]\tLoss: 0.374986\n",
      "Training Epoch: 3 [19968/60000 (33%)]\tLoss: 0.298292\n",
      "Training Epoch: 3 [20000/60000 (33%)]\tLoss: 0.444408\n",
      "Training Epoch: 3 [20032/60000 (33%)]\tLoss: 0.318474\n",
      "Training Epoch: 3 [20064/60000 (33%)]\tLoss: 0.399474\n",
      "Training Epoch: 3 [20096/60000 (33%)]\tLoss: 0.260392\n",
      "Training Epoch: 3 [20128/60000 (34%)]\tLoss: 0.501037\n",
      "Training Epoch: 3 [20160/60000 (34%)]\tLoss: 0.184794\n",
      "Training Epoch: 3 [20192/60000 (34%)]\tLoss: 0.516508\n",
      "Training Epoch: 3 [20224/60000 (34%)]\tLoss: 0.435928\n",
      "Training Epoch: 3 [20256/60000 (34%)]\tLoss: 0.193738\n",
      "Training Epoch: 3 [20288/60000 (34%)]\tLoss: 0.331131\n",
      "Training Epoch: 3 [20320/60000 (34%)]\tLoss: 0.187616\n",
      "Training Epoch: 3 [20352/60000 (34%)]\tLoss: 0.675143\n",
      "Training Epoch: 3 [20384/60000 (34%)]\tLoss: 1.312261\n",
      "Training Epoch: 3 [20416/60000 (34%)]\tLoss: 0.349378\n",
      "Training Epoch: 3 [20448/60000 (34%)]\tLoss: 0.272036\n",
      "Training Epoch: 3 [20480/60000 (34%)]\tLoss: 0.264949\n",
      "Training Epoch: 3 [20512/60000 (34%)]\tLoss: 0.353345\n",
      "Training Epoch: 3 [20544/60000 (34%)]\tLoss: 0.422636\n",
      "Training Epoch: 3 [20576/60000 (34%)]\tLoss: 0.590773\n",
      "Training Epoch: 3 [20608/60000 (34%)]\tLoss: 0.049817\n",
      "Training Epoch: 3 [20640/60000 (34%)]\tLoss: 0.466434\n",
      "Training Epoch: 3 [20672/60000 (34%)]\tLoss: 0.743381\n",
      "Training Epoch: 3 [20704/60000 (35%)]\tLoss: 0.120793\n",
      "Training Epoch: 3 [20736/60000 (35%)]\tLoss: 0.277119\n",
      "Training Epoch: 3 [20768/60000 (35%)]\tLoss: 0.623514\n",
      "Training Epoch: 3 [20800/60000 (35%)]\tLoss: 0.445420\n",
      "Training Epoch: 3 [20832/60000 (35%)]\tLoss: 0.409925\n",
      "Training Epoch: 3 [20864/60000 (35%)]\tLoss: 0.608020\n",
      "Training Epoch: 3 [20896/60000 (35%)]\tLoss: 0.462184\n",
      "Training Epoch: 3 [20928/60000 (35%)]\tLoss: 0.334711\n",
      "Training Epoch: 3 [20960/60000 (35%)]\tLoss: 0.449666\n",
      "Training Epoch: 3 [20992/60000 (35%)]\tLoss: 0.162885\n",
      "Training Epoch: 3 [21024/60000 (35%)]\tLoss: 0.360159\n",
      "Training Epoch: 3 [21056/60000 (35%)]\tLoss: 0.937771\n",
      "Training Epoch: 3 [21088/60000 (35%)]\tLoss: 0.183594\n",
      "Training Epoch: 3 [21120/60000 (35%)]\tLoss: 0.394022\n",
      "Training Epoch: 3 [21152/60000 (35%)]\tLoss: 0.562036\n",
      "Training Epoch: 3 [21184/60000 (35%)]\tLoss: 0.615171\n",
      "Training Epoch: 3 [21216/60000 (35%)]\tLoss: 0.273425\n",
      "Training Epoch: 3 [21248/60000 (35%)]\tLoss: 0.566350\n",
      "Training Epoch: 3 [21280/60000 (35%)]\tLoss: 0.303447\n",
      "Training Epoch: 3 [21312/60000 (36%)]\tLoss: 0.335724\n",
      "Training Epoch: 3 [21344/60000 (36%)]\tLoss: 0.239175\n",
      "Training Epoch: 3 [21376/60000 (36%)]\tLoss: 0.233431\n",
      "Training Epoch: 3 [21408/60000 (36%)]\tLoss: 0.524462\n",
      "Training Epoch: 3 [21440/60000 (36%)]\tLoss: 0.701271\n",
      "Training Epoch: 3 [21472/60000 (36%)]\tLoss: 0.433328\n",
      "Training Epoch: 3 [21504/60000 (36%)]\tLoss: 0.467731\n",
      "Training Epoch: 3 [21536/60000 (36%)]\tLoss: 0.465446\n",
      "Training Epoch: 3 [21568/60000 (36%)]\tLoss: 0.365581\n",
      "Training Epoch: 3 [21600/60000 (36%)]\tLoss: 0.266460\n",
      "Training Epoch: 3 [21632/60000 (36%)]\tLoss: 0.337940\n",
      "Training Epoch: 3 [21664/60000 (36%)]\tLoss: 0.299703\n",
      "Training Epoch: 3 [21696/60000 (36%)]\tLoss: 0.313011\n",
      "Training Epoch: 3 [21728/60000 (36%)]\tLoss: 0.229453\n",
      "Training Epoch: 3 [21760/60000 (36%)]\tLoss: 0.737667\n",
      "Training Epoch: 3 [21792/60000 (36%)]\tLoss: 0.426085\n",
      "Training Epoch: 3 [21824/60000 (36%)]\tLoss: 0.444111\n",
      "Training Epoch: 3 [21856/60000 (36%)]\tLoss: 1.068125\n",
      "Training Epoch: 3 [21888/60000 (36%)]\tLoss: 0.456230\n",
      "Training Epoch: 3 [21920/60000 (37%)]\tLoss: 0.201384\n",
      "Training Epoch: 3 [21952/60000 (37%)]\tLoss: 0.327289\n",
      "Training Epoch: 3 [21984/60000 (37%)]\tLoss: 0.094418\n",
      "Training Epoch: 3 [22016/60000 (37%)]\tLoss: 0.294822\n",
      "Training Epoch: 3 [22048/60000 (37%)]\tLoss: 0.556671\n",
      "Training Epoch: 3 [22080/60000 (37%)]\tLoss: 0.281499\n",
      "Training Epoch: 3 [22112/60000 (37%)]\tLoss: 0.455515\n",
      "Training Epoch: 3 [22144/60000 (37%)]\tLoss: 0.283283\n",
      "Training Epoch: 3 [22176/60000 (37%)]\tLoss: 0.273225\n",
      "Training Epoch: 3 [22208/60000 (37%)]\tLoss: 0.366486\n",
      "Training Epoch: 3 [22240/60000 (37%)]\tLoss: 0.620259\n",
      "Training Epoch: 3 [22272/60000 (37%)]\tLoss: 0.201553\n",
      "Training Epoch: 3 [22304/60000 (37%)]\tLoss: 0.325533\n",
      "Training Epoch: 3 [22336/60000 (37%)]\tLoss: 0.429654\n",
      "Training Epoch: 3 [22368/60000 (37%)]\tLoss: 0.894239\n",
      "Training Epoch: 3 [22400/60000 (37%)]\tLoss: 0.098760\n",
      "Training Epoch: 3 [22432/60000 (37%)]\tLoss: 0.652405\n",
      "Training Epoch: 3 [22464/60000 (37%)]\tLoss: 0.211328\n",
      "Training Epoch: 3 [22496/60000 (37%)]\tLoss: 0.205505\n",
      "Training Epoch: 3 [22528/60000 (38%)]\tLoss: 0.446991\n",
      "Training Epoch: 3 [22560/60000 (38%)]\tLoss: 0.212546\n",
      "Training Epoch: 3 [22592/60000 (38%)]\tLoss: 0.459367\n",
      "Training Epoch: 3 [22624/60000 (38%)]\tLoss: 0.593671\n",
      "Training Epoch: 3 [22656/60000 (38%)]\tLoss: 0.092186\n",
      "Training Epoch: 3 [22688/60000 (38%)]\tLoss: 0.383304\n",
      "Training Epoch: 3 [22720/60000 (38%)]\tLoss: 0.294143\n",
      "Training Epoch: 3 [22752/60000 (38%)]\tLoss: 0.228612\n",
      "Training Epoch: 3 [22784/60000 (38%)]\tLoss: 0.249515\n",
      "Training Epoch: 3 [22816/60000 (38%)]\tLoss: 0.330741\n",
      "Training Epoch: 3 [22848/60000 (38%)]\tLoss: 0.612274\n",
      "Training Epoch: 3 [22880/60000 (38%)]\tLoss: 0.614476\n",
      "Training Epoch: 3 [22912/60000 (38%)]\tLoss: 0.508408\n",
      "Training Epoch: 3 [22944/60000 (38%)]\tLoss: 0.389562\n",
      "Training Epoch: 3 [22976/60000 (38%)]\tLoss: 0.518019\n",
      "Training Epoch: 3 [23008/60000 (38%)]\tLoss: 0.384354\n",
      "Training Epoch: 3 [23040/60000 (38%)]\tLoss: 0.353207\n",
      "Training Epoch: 3 [23072/60000 (38%)]\tLoss: 0.157788\n",
      "Training Epoch: 3 [23104/60000 (39%)]\tLoss: 0.288534\n",
      "Training Epoch: 3 [23136/60000 (39%)]\tLoss: 0.300353\n",
      "Training Epoch: 3 [23168/60000 (39%)]\tLoss: 0.066820\n",
      "Training Epoch: 3 [23200/60000 (39%)]\tLoss: 0.759743\n",
      "Training Epoch: 3 [23232/60000 (39%)]\tLoss: 0.267558\n",
      "Training Epoch: 3 [23264/60000 (39%)]\tLoss: 0.265382\n",
      "Training Epoch: 3 [23296/60000 (39%)]\tLoss: 0.053068\n",
      "Training Epoch: 3 [23328/60000 (39%)]\tLoss: 0.400843\n",
      "Training Epoch: 3 [23360/60000 (39%)]\tLoss: 0.370693\n",
      "Training Epoch: 3 [23392/60000 (39%)]\tLoss: 0.323854\n",
      "Training Epoch: 3 [23424/60000 (39%)]\tLoss: 0.660730\n",
      "Training Epoch: 3 [23456/60000 (39%)]\tLoss: 0.425383\n",
      "Training Epoch: 3 [23488/60000 (39%)]\tLoss: 0.253745\n",
      "Training Epoch: 3 [23520/60000 (39%)]\tLoss: 0.399148\n",
      "Training Epoch: 3 [23552/60000 (39%)]\tLoss: 0.681854\n",
      "Training Epoch: 3 [23584/60000 (39%)]\tLoss: 0.328099\n",
      "Training Epoch: 3 [23616/60000 (39%)]\tLoss: 0.540045\n",
      "Training Epoch: 3 [23648/60000 (39%)]\tLoss: 0.574127\n",
      "Training Epoch: 3 [23680/60000 (39%)]\tLoss: 0.425984\n",
      "Training Epoch: 3 [23712/60000 (40%)]\tLoss: 0.422793\n",
      "Training Epoch: 3 [23744/60000 (40%)]\tLoss: 0.597733\n",
      "Training Epoch: 3 [23776/60000 (40%)]\tLoss: 0.317459\n",
      "Training Epoch: 3 [23808/60000 (40%)]\tLoss: 0.313997\n",
      "Training Epoch: 3 [23840/60000 (40%)]\tLoss: 0.251279\n",
      "Training Epoch: 3 [23872/60000 (40%)]\tLoss: 0.244880\n",
      "Training Epoch: 3 [23904/60000 (40%)]\tLoss: 0.273544\n",
      "Training Epoch: 3 [23936/60000 (40%)]\tLoss: 0.212568\n",
      "Training Epoch: 3 [23968/60000 (40%)]\tLoss: 0.198483\n",
      "Training Epoch: 3 [24000/60000 (40%)]\tLoss: 0.424383\n",
      "Training Epoch: 3 [24032/60000 (40%)]\tLoss: 0.887951\n",
      "Training Epoch: 3 [24064/60000 (40%)]\tLoss: 0.279263\n",
      "Training Epoch: 3 [24096/60000 (40%)]\tLoss: 0.222853\n",
      "Training Epoch: 3 [24128/60000 (40%)]\tLoss: 0.465407\n",
      "Training Epoch: 3 [24160/60000 (40%)]\tLoss: 0.498076\n",
      "Training Epoch: 3 [24192/60000 (40%)]\tLoss: 0.473260\n",
      "Training Epoch: 3 [24224/60000 (40%)]\tLoss: 0.253722\n",
      "Training Epoch: 3 [24256/60000 (40%)]\tLoss: 0.334472\n",
      "Training Epoch: 3 [24288/60000 (40%)]\tLoss: 0.203229\n",
      "Training Epoch: 3 [24320/60000 (41%)]\tLoss: 0.200332\n",
      "Training Epoch: 3 [24352/60000 (41%)]\tLoss: 0.195173\n",
      "Training Epoch: 3 [24384/60000 (41%)]\tLoss: 0.358857\n",
      "Training Epoch: 3 [24416/60000 (41%)]\tLoss: 0.215477\n",
      "Training Epoch: 3 [24448/60000 (41%)]\tLoss: 0.584892\n",
      "Training Epoch: 3 [24480/60000 (41%)]\tLoss: 0.206905\n",
      "Training Epoch: 3 [24512/60000 (41%)]\tLoss: 0.344836\n",
      "Training Epoch: 3 [24544/60000 (41%)]\tLoss: 0.907731\n",
      "Training Epoch: 3 [24576/60000 (41%)]\tLoss: 0.150462\n",
      "Training Epoch: 3 [24608/60000 (41%)]\tLoss: 0.106410\n",
      "Training Epoch: 3 [24640/60000 (41%)]\tLoss: 0.391720\n",
      "Training Epoch: 3 [24672/60000 (41%)]\tLoss: 0.361836\n",
      "Training Epoch: 3 [24704/60000 (41%)]\tLoss: 0.388042\n",
      "Training Epoch: 3 [24736/60000 (41%)]\tLoss: 0.640726\n",
      "Training Epoch: 3 [24768/60000 (41%)]\tLoss: 0.302605\n",
      "Training Epoch: 3 [24800/60000 (41%)]\tLoss: 0.212138\n",
      "Training Epoch: 3 [24832/60000 (41%)]\tLoss: 0.867865\n",
      "Training Epoch: 3 [24864/60000 (41%)]\tLoss: 0.284373\n",
      "Training Epoch: 3 [24896/60000 (41%)]\tLoss: 0.314182\n",
      "Training Epoch: 3 [24928/60000 (42%)]\tLoss: 0.337781\n",
      "Training Epoch: 3 [24960/60000 (42%)]\tLoss: 0.334673\n",
      "Training Epoch: 3 [24992/60000 (42%)]\tLoss: 0.750319\n",
      "Training Epoch: 3 [25024/60000 (42%)]\tLoss: 0.115701\n",
      "Training Epoch: 3 [25056/60000 (42%)]\tLoss: 0.395305\n",
      "Training Epoch: 3 [25088/60000 (42%)]\tLoss: 0.615254\n",
      "Training Epoch: 3 [25120/60000 (42%)]\tLoss: 0.315892\n",
      "Training Epoch: 3 [25152/60000 (42%)]\tLoss: 0.309694\n",
      "Training Epoch: 3 [25184/60000 (42%)]\tLoss: 0.189630\n",
      "Training Epoch: 3 [25216/60000 (42%)]\tLoss: 0.325890\n",
      "Training Epoch: 3 [25248/60000 (42%)]\tLoss: 0.636423\n",
      "Training Epoch: 3 [25280/60000 (42%)]\tLoss: 0.352783\n",
      "Training Epoch: 3 [25312/60000 (42%)]\tLoss: 0.216430\n",
      "Training Epoch: 3 [25344/60000 (42%)]\tLoss: 0.249744\n",
      "Training Epoch: 3 [25376/60000 (42%)]\tLoss: 0.217510\n",
      "Training Epoch: 3 [25408/60000 (42%)]\tLoss: 0.528476\n",
      "Training Epoch: 3 [25440/60000 (42%)]\tLoss: 0.400610\n",
      "Training Epoch: 3 [25472/60000 (42%)]\tLoss: 0.643143\n",
      "Training Epoch: 3 [25504/60000 (43%)]\tLoss: 0.253106\n",
      "Training Epoch: 3 [25536/60000 (43%)]\tLoss: 0.079307\n",
      "Training Epoch: 3 [25568/60000 (43%)]\tLoss: 0.164402\n",
      "Training Epoch: 3 [25600/60000 (43%)]\tLoss: 0.108016\n",
      "Training Epoch: 3 [25632/60000 (43%)]\tLoss: 0.412064\n",
      "Training Epoch: 3 [25664/60000 (43%)]\tLoss: 0.459601\n",
      "Training Epoch: 3 [25696/60000 (43%)]\tLoss: 0.413638\n",
      "Training Epoch: 3 [25728/60000 (43%)]\tLoss: 0.388266\n",
      "Training Epoch: 3 [25760/60000 (43%)]\tLoss: 0.284037\n",
      "Training Epoch: 3 [25792/60000 (43%)]\tLoss: 0.575576\n",
      "Training Epoch: 3 [25824/60000 (43%)]\tLoss: 0.573934\n",
      "Training Epoch: 3 [25856/60000 (43%)]\tLoss: 0.320084\n",
      "Training Epoch: 3 [25888/60000 (43%)]\tLoss: 0.710160\n",
      "Training Epoch: 3 [25920/60000 (43%)]\tLoss: 0.272535\n",
      "Training Epoch: 3 [25952/60000 (43%)]\tLoss: 0.249361\n",
      "Training Epoch: 3 [25984/60000 (43%)]\tLoss: 0.200197\n",
      "Training Epoch: 3 [26016/60000 (43%)]\tLoss: 0.176122\n",
      "Training Epoch: 3 [26048/60000 (43%)]\tLoss: 0.411502\n",
      "Training Epoch: 3 [26080/60000 (43%)]\tLoss: 0.226708\n",
      "Training Epoch: 3 [26112/60000 (44%)]\tLoss: 0.248819\n",
      "Training Epoch: 3 [26144/60000 (44%)]\tLoss: 0.282503\n",
      "Training Epoch: 3 [26176/60000 (44%)]\tLoss: 0.292465\n",
      "Training Epoch: 3 [26208/60000 (44%)]\tLoss: 0.244277\n",
      "Training Epoch: 3 [26240/60000 (44%)]\tLoss: 0.180128\n",
      "Training Epoch: 3 [26272/60000 (44%)]\tLoss: 0.587111\n",
      "Training Epoch: 3 [26304/60000 (44%)]\tLoss: 0.151723\n",
      "Training Epoch: 3 [26336/60000 (44%)]\tLoss: 0.190311\n",
      "Training Epoch: 3 [26368/60000 (44%)]\tLoss: 0.553404\n",
      "Training Epoch: 3 [26400/60000 (44%)]\tLoss: 0.142982\n",
      "Training Epoch: 3 [26432/60000 (44%)]\tLoss: 0.541272\n",
      "Training Epoch: 3 [26464/60000 (44%)]\tLoss: 0.280345\n",
      "Training Epoch: 3 [26496/60000 (44%)]\tLoss: 0.244215\n",
      "Training Epoch: 3 [26528/60000 (44%)]\tLoss: 0.068849\n",
      "Training Epoch: 3 [26560/60000 (44%)]\tLoss: 0.045269\n",
      "Training Epoch: 3 [26592/60000 (44%)]\tLoss: 0.115219\n",
      "Training Epoch: 3 [26624/60000 (44%)]\tLoss: 0.779807\n",
      "Training Epoch: 3 [26656/60000 (44%)]\tLoss: 0.451700\n",
      "Training Epoch: 3 [26688/60000 (44%)]\tLoss: 1.360132\n",
      "Training Epoch: 3 [26720/60000 (45%)]\tLoss: 0.365010\n",
      "Training Epoch: 3 [26752/60000 (45%)]\tLoss: 0.062611\n",
      "Training Epoch: 3 [26784/60000 (45%)]\tLoss: 0.268377\n",
      "Training Epoch: 3 [26816/60000 (45%)]\tLoss: 0.113844\n",
      "Training Epoch: 3 [26848/60000 (45%)]\tLoss: 0.677127\n",
      "Training Epoch: 3 [26880/60000 (45%)]\tLoss: 0.559131\n",
      "Training Epoch: 3 [26912/60000 (45%)]\tLoss: 0.574476\n",
      "Training Epoch: 3 [26944/60000 (45%)]\tLoss: 0.256000\n",
      "Training Epoch: 3 [26976/60000 (45%)]\tLoss: 0.432643\n",
      "Training Epoch: 3 [27008/60000 (45%)]\tLoss: 0.483189\n",
      "Training Epoch: 3 [27040/60000 (45%)]\tLoss: 0.490481\n",
      "Training Epoch: 3 [27072/60000 (45%)]\tLoss: 0.316252\n",
      "Training Epoch: 3 [27104/60000 (45%)]\tLoss: 0.571669\n",
      "Training Epoch: 3 [27136/60000 (45%)]\tLoss: 0.581037\n",
      "Training Epoch: 3 [27168/60000 (45%)]\tLoss: 0.346500\n",
      "Training Epoch: 3 [27200/60000 (45%)]\tLoss: 0.402859\n",
      "Training Epoch: 3 [27232/60000 (45%)]\tLoss: 0.737057\n",
      "Training Epoch: 3 [27264/60000 (45%)]\tLoss: 0.752834\n",
      "Training Epoch: 3 [27296/60000 (45%)]\tLoss: 0.189431\n",
      "Training Epoch: 3 [27328/60000 (46%)]\tLoss: 0.633705\n",
      "Training Epoch: 3 [27360/60000 (46%)]\tLoss: 0.411784\n",
      "Training Epoch: 3 [27392/60000 (46%)]\tLoss: 0.194301\n",
      "Training Epoch: 3 [27424/60000 (46%)]\tLoss: 0.473467\n",
      "Training Epoch: 3 [27456/60000 (46%)]\tLoss: 1.155967\n",
      "Training Epoch: 3 [27488/60000 (46%)]\tLoss: 0.394371\n",
      "Training Epoch: 3 [27520/60000 (46%)]\tLoss: 0.810128\n",
      "Training Epoch: 3 [27552/60000 (46%)]\tLoss: 0.725794\n",
      "Training Epoch: 3 [27584/60000 (46%)]\tLoss: 0.406928\n",
      "Training Epoch: 3 [27616/60000 (46%)]\tLoss: 0.416280\n",
      "Training Epoch: 3 [27648/60000 (46%)]\tLoss: 0.533988\n",
      "Training Epoch: 3 [27680/60000 (46%)]\tLoss: 0.190508\n",
      "Training Epoch: 3 [27712/60000 (46%)]\tLoss: 0.432779\n",
      "Training Epoch: 3 [27744/60000 (46%)]\tLoss: 0.491009\n",
      "Training Epoch: 3 [27776/60000 (46%)]\tLoss: 0.309333\n",
      "Training Epoch: 3 [27808/60000 (46%)]\tLoss: 0.368482\n",
      "Training Epoch: 3 [27840/60000 (46%)]\tLoss: 0.416081\n",
      "Training Epoch: 3 [27872/60000 (46%)]\tLoss: 0.606011\n",
      "Training Epoch: 3 [27904/60000 (47%)]\tLoss: 0.246333\n",
      "Training Epoch: 3 [27936/60000 (47%)]\tLoss: 0.344574\n",
      "Training Epoch: 3 [27968/60000 (47%)]\tLoss: 0.208227\n",
      "Training Epoch: 3 [28000/60000 (47%)]\tLoss: 0.405264\n",
      "Training Epoch: 3 [28032/60000 (47%)]\tLoss: 0.650968\n",
      "Training Epoch: 3 [28064/60000 (47%)]\tLoss: 0.177439\n",
      "Training Epoch: 3 [28096/60000 (47%)]\tLoss: 0.461325\n",
      "Training Epoch: 3 [28128/60000 (47%)]\tLoss: 0.230564\n",
      "Training Epoch: 3 [28160/60000 (47%)]\tLoss: 0.255322\n",
      "Training Epoch: 3 [28192/60000 (47%)]\tLoss: 0.391012\n",
      "Training Epoch: 3 [28224/60000 (47%)]\tLoss: 0.494794\n",
      "Training Epoch: 3 [28256/60000 (47%)]\tLoss: 0.315945\n",
      "Training Epoch: 3 [28288/60000 (47%)]\tLoss: 0.188810\n",
      "Training Epoch: 3 [28320/60000 (47%)]\tLoss: 0.468878\n",
      "Training Epoch: 3 [28352/60000 (47%)]\tLoss: 0.509139\n",
      "Training Epoch: 3 [28384/60000 (47%)]\tLoss: 0.198130\n",
      "Training Epoch: 3 [28416/60000 (47%)]\tLoss: 0.478790\n",
      "Training Epoch: 3 [28448/60000 (47%)]\tLoss: 0.517094\n",
      "Training Epoch: 3 [28480/60000 (47%)]\tLoss: 0.488287\n",
      "Training Epoch: 3 [28512/60000 (48%)]\tLoss: 0.195544\n",
      "Training Epoch: 3 [28544/60000 (48%)]\tLoss: 0.515535\n",
      "Training Epoch: 3 [28576/60000 (48%)]\tLoss: 0.297264\n",
      "Training Epoch: 3 [28608/60000 (48%)]\tLoss: 0.293119\n",
      "Training Epoch: 3 [28640/60000 (48%)]\tLoss: 0.605895\n",
      "Training Epoch: 3 [28672/60000 (48%)]\tLoss: 0.241560\n",
      "Training Epoch: 3 [28704/60000 (48%)]\tLoss: 0.454227\n",
      "Training Epoch: 3 [28736/60000 (48%)]\tLoss: 0.431364\n",
      "Training Epoch: 3 [28768/60000 (48%)]\tLoss: 0.362496\n",
      "Training Epoch: 3 [28800/60000 (48%)]\tLoss: 0.357556\n",
      "Training Epoch: 3 [28832/60000 (48%)]\tLoss: 0.606222\n",
      "Training Epoch: 3 [28864/60000 (48%)]\tLoss: 0.168936\n",
      "Training Epoch: 3 [28896/60000 (48%)]\tLoss: 0.213584\n",
      "Training Epoch: 3 [28928/60000 (48%)]\tLoss: 0.799352\n",
      "Training Epoch: 3 [28960/60000 (48%)]\tLoss: 0.230321\n",
      "Training Epoch: 3 [28992/60000 (48%)]\tLoss: 0.359428\n",
      "Training Epoch: 3 [29024/60000 (48%)]\tLoss: 0.383402\n",
      "Training Epoch: 3 [29056/60000 (48%)]\tLoss: 0.250238\n",
      "Training Epoch: 3 [29088/60000 (48%)]\tLoss: 0.204765\n",
      "Training Epoch: 3 [29120/60000 (49%)]\tLoss: 0.600049\n",
      "Training Epoch: 3 [29152/60000 (49%)]\tLoss: 0.276209\n",
      "Training Epoch: 3 [29184/60000 (49%)]\tLoss: 0.210670\n",
      "Training Epoch: 3 [29216/60000 (49%)]\tLoss: 0.291743\n",
      "Training Epoch: 3 [29248/60000 (49%)]\tLoss: 0.336933\n",
      "Training Epoch: 3 [29280/60000 (49%)]\tLoss: 0.155973\n",
      "Training Epoch: 3 [29312/60000 (49%)]\tLoss: 0.339140\n",
      "Training Epoch: 3 [29344/60000 (49%)]\tLoss: 0.341530\n",
      "Training Epoch: 3 [29376/60000 (49%)]\tLoss: 0.543028\n",
      "Training Epoch: 3 [29408/60000 (49%)]\tLoss: 0.118033\n",
      "Training Epoch: 3 [29440/60000 (49%)]\tLoss: 0.396279\n",
      "Training Epoch: 3 [29472/60000 (49%)]\tLoss: 0.308043\n",
      "Training Epoch: 3 [29504/60000 (49%)]\tLoss: 0.105513\n",
      "Training Epoch: 3 [29536/60000 (49%)]\tLoss: 0.447582\n",
      "Training Epoch: 3 [29568/60000 (49%)]\tLoss: 0.264543\n",
      "Training Epoch: 3 [29600/60000 (49%)]\tLoss: 0.275979\n",
      "Training Epoch: 3 [29632/60000 (49%)]\tLoss: 0.186602\n",
      "Training Epoch: 3 [29664/60000 (49%)]\tLoss: 1.115280\n",
      "Training Epoch: 3 [29696/60000 (49%)]\tLoss: 0.738652\n",
      "Training Epoch: 3 [29728/60000 (50%)]\tLoss: 0.105059\n",
      "Training Epoch: 3 [29760/60000 (50%)]\tLoss: 0.294994\n",
      "Training Epoch: 3 [29792/60000 (50%)]\tLoss: 0.194229\n",
      "Training Epoch: 3 [29824/60000 (50%)]\tLoss: 0.220448\n",
      "Training Epoch: 3 [29856/60000 (50%)]\tLoss: 0.455275\n",
      "Training Epoch: 3 [29888/60000 (50%)]\tLoss: 0.136435\n",
      "Training Epoch: 3 [29920/60000 (50%)]\tLoss: 0.162145\n",
      "Training Epoch: 3 [29952/60000 (50%)]\tLoss: 0.535657\n",
      "Training Epoch: 3 [29984/60000 (50%)]\tLoss: 0.285773\n",
      "Training Epoch: 3 [30016/60000 (50%)]\tLoss: 0.240450\n",
      "Training Epoch: 3 [30048/60000 (50%)]\tLoss: 0.610852\n",
      "Training Epoch: 3 [30080/60000 (50%)]\tLoss: 0.398739\n",
      "Training Epoch: 3 [30112/60000 (50%)]\tLoss: 0.293337\n",
      "Training Epoch: 3 [30144/60000 (50%)]\tLoss: 0.166426\n",
      "Training Epoch: 3 [30176/60000 (50%)]\tLoss: 0.890307\n",
      "Training Epoch: 3 [30208/60000 (50%)]\tLoss: 0.256063\n",
      "Training Epoch: 3 [30240/60000 (50%)]\tLoss: 0.354776\n",
      "Training Epoch: 3 [30272/60000 (50%)]\tLoss: 0.712994\n",
      "Training Epoch: 3 [30304/60000 (51%)]\tLoss: 0.250191\n",
      "Training Epoch: 3 [30336/60000 (51%)]\tLoss: 0.543190\n",
      "Training Epoch: 3 [30368/60000 (51%)]\tLoss: 0.711073\n",
      "Training Epoch: 3 [30400/60000 (51%)]\tLoss: 0.062726\n",
      "Training Epoch: 3 [30432/60000 (51%)]\tLoss: 0.167159\n",
      "Training Epoch: 3 [30464/60000 (51%)]\tLoss: 0.548316\n",
      "Training Epoch: 3 [30496/60000 (51%)]\tLoss: 0.408691\n",
      "Training Epoch: 3 [30528/60000 (51%)]\tLoss: 0.345933\n",
      "Training Epoch: 3 [30560/60000 (51%)]\tLoss: 0.504758\n",
      "Training Epoch: 3 [30592/60000 (51%)]\tLoss: 0.494275\n",
      "Training Epoch: 3 [30624/60000 (51%)]\tLoss: 0.662158\n",
      "Training Epoch: 3 [30656/60000 (51%)]\tLoss: 0.333269\n",
      "Training Epoch: 3 [30688/60000 (51%)]\tLoss: 0.649614\n",
      "Training Epoch: 3 [30720/60000 (51%)]\tLoss: 0.526727\n",
      "Training Epoch: 3 [30752/60000 (51%)]\tLoss: 0.249744\n",
      "Training Epoch: 3 [30784/60000 (51%)]\tLoss: 0.380903\n",
      "Training Epoch: 3 [30816/60000 (51%)]\tLoss: 0.317741\n",
      "Training Epoch: 3 [30848/60000 (51%)]\tLoss: 0.472598\n",
      "Training Epoch: 3 [30880/60000 (51%)]\tLoss: 0.543382\n",
      "Training Epoch: 3 [30912/60000 (52%)]\tLoss: 0.618991\n",
      "Training Epoch: 3 [30944/60000 (52%)]\tLoss: 0.498889\n",
      "Training Epoch: 3 [30976/60000 (52%)]\tLoss: 0.449737\n",
      "Training Epoch: 3 [31008/60000 (52%)]\tLoss: 0.204651\n",
      "Training Epoch: 3 [31040/60000 (52%)]\tLoss: 0.177779\n",
      "Training Epoch: 3 [31072/60000 (52%)]\tLoss: 0.176438\n",
      "Training Epoch: 3 [31104/60000 (52%)]\tLoss: 0.387028\n",
      "Training Epoch: 3 [31136/60000 (52%)]\tLoss: 0.444848\n",
      "Training Epoch: 3 [31168/60000 (52%)]\tLoss: 0.260283\n",
      "Training Epoch: 3 [31200/60000 (52%)]\tLoss: 0.609500\n",
      "Training Epoch: 3 [31232/60000 (52%)]\tLoss: 0.512372\n",
      "Training Epoch: 3 [31264/60000 (52%)]\tLoss: 0.290699\n",
      "Training Epoch: 3 [31296/60000 (52%)]\tLoss: 0.544910\n",
      "Training Epoch: 3 [31328/60000 (52%)]\tLoss: 0.639357\n",
      "Training Epoch: 3 [31360/60000 (52%)]\tLoss: 0.552310\n",
      "Training Epoch: 3 [31392/60000 (52%)]\tLoss: 0.330308\n",
      "Training Epoch: 3 [31424/60000 (52%)]\tLoss: 0.378104\n",
      "Training Epoch: 3 [31456/60000 (52%)]\tLoss: 0.275868\n",
      "Training Epoch: 3 [31488/60000 (52%)]\tLoss: 0.708809\n",
      "Training Epoch: 3 [31520/60000 (53%)]\tLoss: 0.316171\n",
      "Training Epoch: 3 [31552/60000 (53%)]\tLoss: 0.419572\n",
      "Training Epoch: 3 [31584/60000 (53%)]\tLoss: 0.213066\n",
      "Training Epoch: 3 [31616/60000 (53%)]\tLoss: 0.459394\n",
      "Training Epoch: 3 [31648/60000 (53%)]\tLoss: 0.542999\n",
      "Training Epoch: 3 [31680/60000 (53%)]\tLoss: 0.321215\n",
      "Training Epoch: 3 [31712/60000 (53%)]\tLoss: 0.334912\n",
      "Training Epoch: 3 [31744/60000 (53%)]\tLoss: 0.316199\n",
      "Training Epoch: 3 [31776/60000 (53%)]\tLoss: 0.503816\n",
      "Training Epoch: 3 [31808/60000 (53%)]\tLoss: 0.205457\n",
      "Training Epoch: 3 [31840/60000 (53%)]\tLoss: 0.305777\n",
      "Training Epoch: 3 [31872/60000 (53%)]\tLoss: 0.390498\n",
      "Training Epoch: 3 [31904/60000 (53%)]\tLoss: 0.703878\n",
      "Training Epoch: 3 [31936/60000 (53%)]\tLoss: 0.322226\n",
      "Training Epoch: 3 [31968/60000 (53%)]\tLoss: 0.440549\n",
      "Training Epoch: 3 [32000/60000 (53%)]\tLoss: 0.409569\n",
      "Training Epoch: 3 [32032/60000 (53%)]\tLoss: 0.602604\n",
      "Training Epoch: 3 [32064/60000 (53%)]\tLoss: 0.399443\n",
      "Training Epoch: 3 [32096/60000 (53%)]\tLoss: 0.219634\n",
      "Training Epoch: 3 [32128/60000 (54%)]\tLoss: 0.255704\n",
      "Training Epoch: 3 [32160/60000 (54%)]\tLoss: 0.264066\n",
      "Training Epoch: 3 [32192/60000 (54%)]\tLoss: 0.170857\n",
      "Training Epoch: 3 [32224/60000 (54%)]\tLoss: 0.133519\n",
      "Training Epoch: 3 [32256/60000 (54%)]\tLoss: 0.201442\n",
      "Training Epoch: 3 [32288/60000 (54%)]\tLoss: 0.285026\n",
      "Training Epoch: 3 [32320/60000 (54%)]\tLoss: 0.838073\n",
      "Training Epoch: 3 [32352/60000 (54%)]\tLoss: 0.753471\n",
      "Training Epoch: 3 [32384/60000 (54%)]\tLoss: 0.274665\n",
      "Training Epoch: 3 [32416/60000 (54%)]\tLoss: 0.449784\n",
      "Training Epoch: 3 [32448/60000 (54%)]\tLoss: 0.473384\n",
      "Training Epoch: 3 [32480/60000 (54%)]\tLoss: 0.580896\n",
      "Training Epoch: 3 [32512/60000 (54%)]\tLoss: 0.857080\n",
      "Training Epoch: 3 [32544/60000 (54%)]\tLoss: 0.285852\n",
      "Training Epoch: 3 [32576/60000 (54%)]\tLoss: 0.585596\n",
      "Training Epoch: 3 [32608/60000 (54%)]\tLoss: 0.560252\n",
      "Training Epoch: 3 [32640/60000 (54%)]\tLoss: 0.610189\n",
      "Training Epoch: 3 [32672/60000 (54%)]\tLoss: 0.684617\n",
      "Training Epoch: 3 [32704/60000 (55%)]\tLoss: 0.298370\n",
      "Training Epoch: 3 [32736/60000 (55%)]\tLoss: 0.104260\n",
      "Training Epoch: 3 [32768/60000 (55%)]\tLoss: 0.447182\n",
      "Training Epoch: 3 [32800/60000 (55%)]\tLoss: 0.340440\n",
      "Training Epoch: 3 [32832/60000 (55%)]\tLoss: 0.905779\n",
      "Training Epoch: 3 [32864/60000 (55%)]\tLoss: 0.256703\n",
      "Training Epoch: 3 [32896/60000 (55%)]\tLoss: 0.034092\n",
      "Training Epoch: 3 [32928/60000 (55%)]\tLoss: 0.242917\n",
      "Training Epoch: 3 [32960/60000 (55%)]\tLoss: 0.780445\n",
      "Training Epoch: 3 [32992/60000 (55%)]\tLoss: 0.888580\n",
      "Training Epoch: 3 [33024/60000 (55%)]\tLoss: 0.276180\n",
      "Training Epoch: 3 [33056/60000 (55%)]\tLoss: 0.218973\n",
      "Training Epoch: 3 [33088/60000 (55%)]\tLoss: 0.336578\n",
      "Training Epoch: 3 [33120/60000 (55%)]\tLoss: 0.371748\n",
      "Training Epoch: 3 [33152/60000 (55%)]\tLoss: 0.760413\n",
      "Training Epoch: 3 [33184/60000 (55%)]\tLoss: 0.702614\n",
      "Training Epoch: 3 [33216/60000 (55%)]\tLoss: 0.568951\n",
      "Training Epoch: 3 [33248/60000 (55%)]\tLoss: 0.258226\n",
      "Training Epoch: 3 [33280/60000 (55%)]\tLoss: 1.210075\n",
      "Training Epoch: 3 [33312/60000 (56%)]\tLoss: 0.928815\n",
      "Training Epoch: 3 [33344/60000 (56%)]\tLoss: 0.302205\n",
      "Training Epoch: 3 [33376/60000 (56%)]\tLoss: 0.327925\n",
      "Training Epoch: 3 [33408/60000 (56%)]\tLoss: 0.547419\n",
      "Training Epoch: 3 [33440/60000 (56%)]\tLoss: 0.255284\n",
      "Training Epoch: 3 [33472/60000 (56%)]\tLoss: 0.395975\n",
      "Training Epoch: 3 [33504/60000 (56%)]\tLoss: 0.300061\n",
      "Training Epoch: 3 [33536/60000 (56%)]\tLoss: 0.669330\n",
      "Training Epoch: 3 [33568/60000 (56%)]\tLoss: 0.363089\n",
      "Training Epoch: 3 [33600/60000 (56%)]\tLoss: 0.544414\n",
      "Training Epoch: 3 [33632/60000 (56%)]\tLoss: 0.280496\n",
      "Training Epoch: 3 [33664/60000 (56%)]\tLoss: 0.441050\n",
      "Training Epoch: 3 [33696/60000 (56%)]\tLoss: 0.253092\n",
      "Training Epoch: 3 [33728/60000 (56%)]\tLoss: 0.510408\n",
      "Training Epoch: 3 [33760/60000 (56%)]\tLoss: 0.817287\n",
      "Training Epoch: 3 [33792/60000 (56%)]\tLoss: 0.262528\n",
      "Training Epoch: 3 [33824/60000 (56%)]\tLoss: 0.197984\n",
      "Training Epoch: 3 [33856/60000 (56%)]\tLoss: 0.560466\n",
      "Training Epoch: 3 [33888/60000 (56%)]\tLoss: 0.716673\n",
      "Training Epoch: 3 [33920/60000 (57%)]\tLoss: 0.394786\n",
      "Training Epoch: 3 [33952/60000 (57%)]\tLoss: 0.778300\n",
      "Training Epoch: 3 [33984/60000 (57%)]\tLoss: 1.016554\n",
      "Training Epoch: 3 [34016/60000 (57%)]\tLoss: 0.213736\n",
      "Training Epoch: 3 [34048/60000 (57%)]\tLoss: 0.273151\n",
      "Training Epoch: 3 [34080/60000 (57%)]\tLoss: 0.299415\n",
      "Training Epoch: 3 [34112/60000 (57%)]\tLoss: 0.439202\n",
      "Training Epoch: 3 [34144/60000 (57%)]\tLoss: 0.254947\n",
      "Training Epoch: 3 [34176/60000 (57%)]\tLoss: 0.636477\n",
      "Training Epoch: 3 [34208/60000 (57%)]\tLoss: 0.351166\n",
      "Training Epoch: 3 [34240/60000 (57%)]\tLoss: 0.651210\n",
      "Training Epoch: 3 [34272/60000 (57%)]\tLoss: 0.477606\n",
      "Training Epoch: 3 [34304/60000 (57%)]\tLoss: 0.251651\n",
      "Training Epoch: 3 [34336/60000 (57%)]\tLoss: 0.253473\n",
      "Training Epoch: 3 [34368/60000 (57%)]\tLoss: 0.436531\n",
      "Training Epoch: 3 [34400/60000 (57%)]\tLoss: 0.281335\n",
      "Training Epoch: 3 [34432/60000 (57%)]\tLoss: 0.589877\n",
      "Training Epoch: 3 [34464/60000 (57%)]\tLoss: 0.438195\n",
      "Training Epoch: 3 [34496/60000 (57%)]\tLoss: 0.258937\n",
      "Training Epoch: 3 [34528/60000 (58%)]\tLoss: 0.242072\n",
      "Training Epoch: 3 [34560/60000 (58%)]\tLoss: 0.671518\n",
      "Training Epoch: 3 [34592/60000 (58%)]\tLoss: 0.510383\n",
      "Training Epoch: 3 [34624/60000 (58%)]\tLoss: 0.290963\n",
      "Training Epoch: 3 [34656/60000 (58%)]\tLoss: 0.345911\n",
      "Training Epoch: 3 [34688/60000 (58%)]\tLoss: 0.321840\n",
      "Training Epoch: 3 [34720/60000 (58%)]\tLoss: 0.416255\n",
      "Training Epoch: 3 [34752/60000 (58%)]\tLoss: 0.762790\n",
      "Training Epoch: 3 [34784/60000 (58%)]\tLoss: 0.380154\n",
      "Training Epoch: 3 [34816/60000 (58%)]\tLoss: 0.736645\n",
      "Training Epoch: 3 [34848/60000 (58%)]\tLoss: 0.667678\n",
      "Training Epoch: 3 [34880/60000 (58%)]\tLoss: 0.245400\n",
      "Training Epoch: 3 [34912/60000 (58%)]\tLoss: 0.533564\n",
      "Training Epoch: 3 [34944/60000 (58%)]\tLoss: 0.550440\n",
      "Training Epoch: 3 [34976/60000 (58%)]\tLoss: 0.798767\n",
      "Training Epoch: 3 [35008/60000 (58%)]\tLoss: 0.339213\n",
      "Training Epoch: 3 [35040/60000 (58%)]\tLoss: 0.551043\n",
      "Training Epoch: 3 [35072/60000 (58%)]\tLoss: 0.699794\n",
      "Training Epoch: 3 [35104/60000 (59%)]\tLoss: 0.384484\n",
      "Training Epoch: 3 [35136/60000 (59%)]\tLoss: 0.346207\n",
      "Training Epoch: 3 [35168/60000 (59%)]\tLoss: 0.300594\n",
      "Training Epoch: 3 [35200/60000 (59%)]\tLoss: 0.265609\n",
      "Training Epoch: 3 [35232/60000 (59%)]\tLoss: 0.462596\n",
      "Training Epoch: 3 [35264/60000 (59%)]\tLoss: 0.875320\n",
      "Training Epoch: 3 [35296/60000 (59%)]\tLoss: 0.122613\n",
      "Training Epoch: 3 [35328/60000 (59%)]\tLoss: 0.427222\n",
      "Training Epoch: 3 [35360/60000 (59%)]\tLoss: 0.364646\n",
      "Training Epoch: 3 [35392/60000 (59%)]\tLoss: 0.367918\n",
      "Training Epoch: 3 [35424/60000 (59%)]\tLoss: 0.727810\n",
      "Training Epoch: 3 [35456/60000 (59%)]\tLoss: 0.236526\n",
      "Training Epoch: 3 [35488/60000 (59%)]\tLoss: 0.298571\n",
      "Training Epoch: 3 [35520/60000 (59%)]\tLoss: 0.137814\n",
      "Training Epoch: 3 [35552/60000 (59%)]\tLoss: 0.353453\n",
      "Training Epoch: 3 [35584/60000 (59%)]\tLoss: 0.789929\n",
      "Training Epoch: 3 [35616/60000 (59%)]\tLoss: 0.408035\n",
      "Training Epoch: 3 [35648/60000 (59%)]\tLoss: 0.589064\n",
      "Training Epoch: 3 [35680/60000 (59%)]\tLoss: 0.368787\n",
      "Training Epoch: 3 [35712/60000 (60%)]\tLoss: 0.358639\n",
      "Training Epoch: 3 [35744/60000 (60%)]\tLoss: 0.593163\n",
      "Training Epoch: 3 [35776/60000 (60%)]\tLoss: 0.364972\n",
      "Training Epoch: 3 [35808/60000 (60%)]\tLoss: 0.157420\n",
      "Training Epoch: 3 [35840/60000 (60%)]\tLoss: 0.228162\n",
      "Training Epoch: 3 [35872/60000 (60%)]\tLoss: 0.753359\n",
      "Training Epoch: 3 [35904/60000 (60%)]\tLoss: 0.216392\n",
      "Training Epoch: 3 [35936/60000 (60%)]\tLoss: 0.435877\n",
      "Training Epoch: 3 [35968/60000 (60%)]\tLoss: 0.513536\n",
      "Training Epoch: 3 [36000/60000 (60%)]\tLoss: 0.320875\n",
      "Training Epoch: 3 [36032/60000 (60%)]\tLoss: 0.490911\n",
      "Training Epoch: 3 [36064/60000 (60%)]\tLoss: 0.742343\n",
      "Training Epoch: 3 [36096/60000 (60%)]\tLoss: 0.382068\n",
      "Training Epoch: 3 [36128/60000 (60%)]\tLoss: 0.399621\n",
      "Training Epoch: 3 [36160/60000 (60%)]\tLoss: 0.577572\n",
      "Training Epoch: 3 [36192/60000 (60%)]\tLoss: 0.379823\n",
      "Training Epoch: 3 [36224/60000 (60%)]\tLoss: 0.099048\n",
      "Training Epoch: 3 [36256/60000 (60%)]\tLoss: 0.637332\n",
      "Training Epoch: 3 [36288/60000 (60%)]\tLoss: 0.446074\n",
      "Training Epoch: 3 [36320/60000 (61%)]\tLoss: 0.502063\n",
      "Training Epoch: 3 [36352/60000 (61%)]\tLoss: 0.303537\n",
      "Training Epoch: 3 [36384/60000 (61%)]\tLoss: 0.729518\n",
      "Training Epoch: 3 [36416/60000 (61%)]\tLoss: 0.712983\n",
      "Training Epoch: 3 [36448/60000 (61%)]\tLoss: 0.533964\n",
      "Training Epoch: 3 [36480/60000 (61%)]\tLoss: 0.213406\n",
      "Training Epoch: 3 [36512/60000 (61%)]\tLoss: 0.464241\n",
      "Training Epoch: 3 [36544/60000 (61%)]\tLoss: 0.324288\n",
      "Training Epoch: 3 [36576/60000 (61%)]\tLoss: 0.229714\n",
      "Training Epoch: 3 [36608/60000 (61%)]\tLoss: 0.822396\n",
      "Training Epoch: 3 [36640/60000 (61%)]\tLoss: 0.306873\n",
      "Training Epoch: 3 [36672/60000 (61%)]\tLoss: 0.516754\n",
      "Training Epoch: 3 [36704/60000 (61%)]\tLoss: 0.316604\n",
      "Training Epoch: 3 [36736/60000 (61%)]\tLoss: 0.194451\n",
      "Training Epoch: 3 [36768/60000 (61%)]\tLoss: 0.613646\n",
      "Training Epoch: 3 [36800/60000 (61%)]\tLoss: 0.707515\n",
      "Training Epoch: 3 [36832/60000 (61%)]\tLoss: 0.521290\n",
      "Training Epoch: 3 [36864/60000 (61%)]\tLoss: 0.432579\n",
      "Training Epoch: 3 [36896/60000 (61%)]\tLoss: 0.239386\n",
      "Training Epoch: 3 [36928/60000 (62%)]\tLoss: 0.524444\n",
      "Training Epoch: 3 [36960/60000 (62%)]\tLoss: 0.412252\n",
      "Training Epoch: 3 [36992/60000 (62%)]\tLoss: 0.181772\n",
      "Training Epoch: 3 [37024/60000 (62%)]\tLoss: 0.345556\n",
      "Training Epoch: 3 [37056/60000 (62%)]\tLoss: 0.176404\n",
      "Training Epoch: 3 [37088/60000 (62%)]\tLoss: 0.482116\n",
      "Training Epoch: 3 [37120/60000 (62%)]\tLoss: 0.374493\n",
      "Training Epoch: 3 [37152/60000 (62%)]\tLoss: 0.417266\n",
      "Training Epoch: 3 [37184/60000 (62%)]\tLoss: 0.288083\n",
      "Training Epoch: 3 [37216/60000 (62%)]\tLoss: 0.112223\n",
      "Training Epoch: 3 [37248/60000 (62%)]\tLoss: 0.405209\n",
      "Training Epoch: 3 [37280/60000 (62%)]\tLoss: 0.397965\n",
      "Training Epoch: 3 [37312/60000 (62%)]\tLoss: 0.570584\n",
      "Training Epoch: 3 [37344/60000 (62%)]\tLoss: 0.527662\n",
      "Training Epoch: 3 [37376/60000 (62%)]\tLoss: 0.463081\n",
      "Training Epoch: 3 [37408/60000 (62%)]\tLoss: 0.198733\n",
      "Training Epoch: 3 [37440/60000 (62%)]\tLoss: 0.204570\n",
      "Training Epoch: 3 [37472/60000 (62%)]\tLoss: 0.334641\n",
      "Training Epoch: 3 [37504/60000 (63%)]\tLoss: 0.558263\n",
      "Training Epoch: 3 [37536/60000 (63%)]\tLoss: 0.228645\n",
      "Training Epoch: 3 [37568/60000 (63%)]\tLoss: 0.555183\n",
      "Training Epoch: 3 [37600/60000 (63%)]\tLoss: 0.541375\n",
      "Training Epoch: 3 [37632/60000 (63%)]\tLoss: 0.097836\n",
      "Training Epoch: 3 [37664/60000 (63%)]\tLoss: 0.395655\n",
      "Training Epoch: 3 [37696/60000 (63%)]\tLoss: 0.330375\n",
      "Training Epoch: 3 [37728/60000 (63%)]\tLoss: 0.518486\n",
      "Training Epoch: 3 [37760/60000 (63%)]\tLoss: 0.656919\n",
      "Training Epoch: 3 [37792/60000 (63%)]\tLoss: 0.272500\n",
      "Training Epoch: 3 [37824/60000 (63%)]\tLoss: 0.374781\n",
      "Training Epoch: 3 [37856/60000 (63%)]\tLoss: 0.252256\n",
      "Training Epoch: 3 [37888/60000 (63%)]\tLoss: 0.276626\n",
      "Training Epoch: 3 [37920/60000 (63%)]\tLoss: 0.302103\n",
      "Training Epoch: 3 [37952/60000 (63%)]\tLoss: 0.308503\n",
      "Training Epoch: 3 [37984/60000 (63%)]\tLoss: 0.421603\n",
      "Training Epoch: 3 [38016/60000 (63%)]\tLoss: 0.356625\n",
      "Training Epoch: 3 [38048/60000 (63%)]\tLoss: 0.300749\n",
      "Training Epoch: 3 [38080/60000 (63%)]\tLoss: 0.095759\n",
      "Training Epoch: 3 [38112/60000 (64%)]\tLoss: 0.417021\n",
      "Training Epoch: 3 [38144/60000 (64%)]\tLoss: 0.395676\n",
      "Training Epoch: 3 [38176/60000 (64%)]\tLoss: 0.319419\n",
      "Training Epoch: 3 [38208/60000 (64%)]\tLoss: 0.193701\n",
      "Training Epoch: 3 [38240/60000 (64%)]\tLoss: 0.619171\n",
      "Training Epoch: 3 [38272/60000 (64%)]\tLoss: 0.206198\n",
      "Training Epoch: 3 [38304/60000 (64%)]\tLoss: 0.130649\n",
      "Training Epoch: 3 [38336/60000 (64%)]\tLoss: 0.349323\n",
      "Training Epoch: 3 [38368/60000 (64%)]\tLoss: 0.209344\n",
      "Training Epoch: 3 [38400/60000 (64%)]\tLoss: 0.201039\n",
      "Training Epoch: 3 [38432/60000 (64%)]\tLoss: 0.531761\n",
      "Training Epoch: 3 [38464/60000 (64%)]\tLoss: 0.704007\n",
      "Training Epoch: 3 [38496/60000 (64%)]\tLoss: 0.241911\n",
      "Training Epoch: 3 [38528/60000 (64%)]\tLoss: 0.281252\n",
      "Training Epoch: 3 [38560/60000 (64%)]\tLoss: 0.290776\n",
      "Training Epoch: 3 [38592/60000 (64%)]\tLoss: 0.549895\n",
      "Training Epoch: 3 [38624/60000 (64%)]\tLoss: 1.395758\n",
      "Training Epoch: 3 [38656/60000 (64%)]\tLoss: 1.114608\n",
      "Training Epoch: 3 [38688/60000 (64%)]\tLoss: 0.545660\n",
      "Training Epoch: 3 [38720/60000 (65%)]\tLoss: 0.460513\n",
      "Training Epoch: 3 [38752/60000 (65%)]\tLoss: 0.208736\n",
      "Training Epoch: 3 [38784/60000 (65%)]\tLoss: 0.242066\n",
      "Training Epoch: 3 [38816/60000 (65%)]\tLoss: 0.210046\n",
      "Training Epoch: 3 [38848/60000 (65%)]\tLoss: 0.189799\n",
      "Training Epoch: 3 [38880/60000 (65%)]\tLoss: 0.319101\n",
      "Training Epoch: 3 [38912/60000 (65%)]\tLoss: 0.296782\n",
      "Training Epoch: 3 [38944/60000 (65%)]\tLoss: 0.290333\n",
      "Training Epoch: 3 [38976/60000 (65%)]\tLoss: 0.346418\n",
      "Training Epoch: 3 [39008/60000 (65%)]\tLoss: 0.101325\n",
      "Training Epoch: 3 [39040/60000 (65%)]\tLoss: 0.279602\n",
      "Training Epoch: 3 [39072/60000 (65%)]\tLoss: 0.388088\n",
      "Training Epoch: 3 [39104/60000 (65%)]\tLoss: 0.654061\n",
      "Training Epoch: 3 [39136/60000 (65%)]\tLoss: 0.177109\n",
      "Training Epoch: 3 [39168/60000 (65%)]\tLoss: 0.377894\n",
      "Training Epoch: 3 [39200/60000 (65%)]\tLoss: 0.177958\n",
      "Training Epoch: 3 [39232/60000 (65%)]\tLoss: 0.383488\n",
      "Training Epoch: 3 [39264/60000 (65%)]\tLoss: 0.353127\n",
      "Training Epoch: 3 [39296/60000 (65%)]\tLoss: 0.360121\n",
      "Training Epoch: 3 [39328/60000 (66%)]\tLoss: 0.190712\n",
      "Training Epoch: 3 [39360/60000 (66%)]\tLoss: 0.523521\n",
      "Training Epoch: 3 [39392/60000 (66%)]\tLoss: 0.613088\n",
      "Training Epoch: 3 [39424/60000 (66%)]\tLoss: 0.638383\n",
      "Training Epoch: 3 [39456/60000 (66%)]\tLoss: 0.596375\n",
      "Training Epoch: 3 [39488/60000 (66%)]\tLoss: 0.184071\n",
      "Training Epoch: 3 [39520/60000 (66%)]\tLoss: 0.175812\n",
      "Training Epoch: 3 [39552/60000 (66%)]\tLoss: 0.326231\n",
      "Training Epoch: 3 [39584/60000 (66%)]\tLoss: 0.257128\n",
      "Training Epoch: 3 [39616/60000 (66%)]\tLoss: 0.440965\n",
      "Training Epoch: 3 [39648/60000 (66%)]\tLoss: 0.146191\n",
      "Training Epoch: 3 [39680/60000 (66%)]\tLoss: 0.638879\n",
      "Training Epoch: 3 [39712/60000 (66%)]\tLoss: 0.497946\n",
      "Training Epoch: 3 [39744/60000 (66%)]\tLoss: 0.349454\n",
      "Training Epoch: 3 [39776/60000 (66%)]\tLoss: 0.431286\n",
      "Training Epoch: 3 [39808/60000 (66%)]\tLoss: 0.196037\n",
      "Training Epoch: 3 [39840/60000 (66%)]\tLoss: 0.381537\n",
      "Training Epoch: 3 [39872/60000 (66%)]\tLoss: 0.330448\n",
      "Training Epoch: 3 [39904/60000 (67%)]\tLoss: 0.251682\n",
      "Training Epoch: 3 [39936/60000 (67%)]\tLoss: 0.523585\n",
      "Training Epoch: 3 [39968/60000 (67%)]\tLoss: 0.276346\n",
      "Training Epoch: 3 [40000/60000 (67%)]\tLoss: 0.256402\n",
      "Training Epoch: 3 [40032/60000 (67%)]\tLoss: 0.245186\n",
      "Training Epoch: 3 [40064/60000 (67%)]\tLoss: 0.129056\n",
      "Training Epoch: 3 [40096/60000 (67%)]\tLoss: 0.224834\n",
      "Training Epoch: 3 [40128/60000 (67%)]\tLoss: 0.064060\n",
      "Training Epoch: 3 [40160/60000 (67%)]\tLoss: 0.395919\n",
      "Training Epoch: 3 [40192/60000 (67%)]\tLoss: 0.240929\n",
      "Training Epoch: 3 [40224/60000 (67%)]\tLoss: 0.350725\n",
      "Training Epoch: 3 [40256/60000 (67%)]\tLoss: 0.464125\n",
      "Training Epoch: 3 [40288/60000 (67%)]\tLoss: 0.781587\n",
      "Training Epoch: 3 [40320/60000 (67%)]\tLoss: 0.451437\n",
      "Training Epoch: 3 [40352/60000 (67%)]\tLoss: 0.910496\n",
      "Training Epoch: 3 [40384/60000 (67%)]\tLoss: 0.133707\n",
      "Training Epoch: 3 [40416/60000 (67%)]\tLoss: 0.198531\n",
      "Training Epoch: 3 [40448/60000 (67%)]\tLoss: 0.227458\n",
      "Training Epoch: 3 [40480/60000 (67%)]\tLoss: 0.391003\n",
      "Training Epoch: 3 [40512/60000 (68%)]\tLoss: 0.629554\n",
      "Training Epoch: 3 [40544/60000 (68%)]\tLoss: 0.465648\n",
      "Training Epoch: 3 [40576/60000 (68%)]\tLoss: 0.548901\n",
      "Training Epoch: 3 [40608/60000 (68%)]\tLoss: 0.394547\n",
      "Training Epoch: 3 [40640/60000 (68%)]\tLoss: 0.438720\n",
      "Training Epoch: 3 [40672/60000 (68%)]\tLoss: 0.121425\n",
      "Training Epoch: 3 [40704/60000 (68%)]\tLoss: 0.699951\n",
      "Training Epoch: 3 [40736/60000 (68%)]\tLoss: 0.924366\n",
      "Training Epoch: 3 [40768/60000 (68%)]\tLoss: 0.286620\n",
      "Training Epoch: 3 [40800/60000 (68%)]\tLoss: 0.408944\n",
      "Training Epoch: 3 [40832/60000 (68%)]\tLoss: 0.232431\n",
      "Training Epoch: 3 [40864/60000 (68%)]\tLoss: 0.506665\n",
      "Training Epoch: 3 [40896/60000 (68%)]\tLoss: 0.680098\n",
      "Training Epoch: 3 [40928/60000 (68%)]\tLoss: 0.100950\n",
      "Training Epoch: 3 [40960/60000 (68%)]\tLoss: 0.375546\n",
      "Training Epoch: 3 [40992/60000 (68%)]\tLoss: 0.493668\n",
      "Training Epoch: 3 [41024/60000 (68%)]\tLoss: 0.213218\n",
      "Training Epoch: 3 [41056/60000 (68%)]\tLoss: 0.159156\n",
      "Training Epoch: 3 [41088/60000 (68%)]\tLoss: 0.412746\n",
      "Training Epoch: 3 [41120/60000 (69%)]\tLoss: 0.593923\n",
      "Training Epoch: 3 [41152/60000 (69%)]\tLoss: 0.720405\n",
      "Training Epoch: 3 [41184/60000 (69%)]\tLoss: 0.259409\n",
      "Training Epoch: 3 [41216/60000 (69%)]\tLoss: 0.114862\n",
      "Training Epoch: 3 [41248/60000 (69%)]\tLoss: 0.411894\n",
      "Training Epoch: 3 [41280/60000 (69%)]\tLoss: 0.240941\n",
      "Training Epoch: 3 [41312/60000 (69%)]\tLoss: 0.749093\n",
      "Training Epoch: 3 [41344/60000 (69%)]\tLoss: 0.479113\n",
      "Training Epoch: 3 [41376/60000 (69%)]\tLoss: 0.293213\n",
      "Training Epoch: 3 [41408/60000 (69%)]\tLoss: 1.137953\n",
      "Training Epoch: 3 [41440/60000 (69%)]\tLoss: 0.841788\n",
      "Training Epoch: 3 [41472/60000 (69%)]\tLoss: 0.418295\n",
      "Training Epoch: 3 [41504/60000 (69%)]\tLoss: 0.261310\n",
      "Training Epoch: 3 [41536/60000 (69%)]\tLoss: 0.434545\n",
      "Training Epoch: 3 [41568/60000 (69%)]\tLoss: 0.217136\n",
      "Training Epoch: 3 [41600/60000 (69%)]\tLoss: 0.279462\n",
      "Training Epoch: 3 [41632/60000 (69%)]\tLoss: 0.501875\n",
      "Training Epoch: 3 [41664/60000 (69%)]\tLoss: 0.394985\n",
      "Training Epoch: 3 [41696/60000 (69%)]\tLoss: 0.558657\n",
      "Training Epoch: 3 [41728/60000 (70%)]\tLoss: 0.692911\n",
      "Training Epoch: 3 [41760/60000 (70%)]\tLoss: 0.126556\n",
      "Training Epoch: 3 [41792/60000 (70%)]\tLoss: 0.170990\n",
      "Training Epoch: 3 [41824/60000 (70%)]\tLoss: 0.174811\n",
      "Training Epoch: 3 [41856/60000 (70%)]\tLoss: 0.665761\n",
      "Training Epoch: 3 [41888/60000 (70%)]\tLoss: 0.576973\n",
      "Training Epoch: 3 [41920/60000 (70%)]\tLoss: 0.893955\n",
      "Training Epoch: 3 [41952/60000 (70%)]\tLoss: 0.359964\n",
      "Training Epoch: 3 [41984/60000 (70%)]\tLoss: 0.724244\n",
      "Training Epoch: 3 [42016/60000 (70%)]\tLoss: 0.348010\n",
      "Training Epoch: 3 [42048/60000 (70%)]\tLoss: 0.213263\n",
      "Training Epoch: 3 [42080/60000 (70%)]\tLoss: 0.216994\n",
      "Training Epoch: 3 [42112/60000 (70%)]\tLoss: 0.525477\n",
      "Training Epoch: 3 [42144/60000 (70%)]\tLoss: 0.475271\n",
      "Training Epoch: 3 [42176/60000 (70%)]\tLoss: 0.573875\n",
      "Training Epoch: 3 [42208/60000 (70%)]\tLoss: 0.538399\n",
      "Training Epoch: 3 [42240/60000 (70%)]\tLoss: 0.226408\n",
      "Training Epoch: 3 [42272/60000 (70%)]\tLoss: 0.536467\n",
      "Training Epoch: 3 [42304/60000 (71%)]\tLoss: 0.357709\n",
      "Training Epoch: 3 [42336/60000 (71%)]\tLoss: 0.790133\n",
      "Training Epoch: 3 [42368/60000 (71%)]\tLoss: 0.835471\n",
      "Training Epoch: 3 [42400/60000 (71%)]\tLoss: 0.113537\n",
      "Training Epoch: 3 [42432/60000 (71%)]\tLoss: 0.588114\n",
      "Training Epoch: 3 [42464/60000 (71%)]\tLoss: 0.459104\n",
      "Training Epoch: 3 [42496/60000 (71%)]\tLoss: 0.640384\n",
      "Training Epoch: 3 [42528/60000 (71%)]\tLoss: 0.584695\n",
      "Training Epoch: 3 [42560/60000 (71%)]\tLoss: 0.312194\n",
      "Training Epoch: 3 [42592/60000 (71%)]\tLoss: 0.191652\n",
      "Training Epoch: 3 [42624/60000 (71%)]\tLoss: 0.245012\n",
      "Training Epoch: 3 [42656/60000 (71%)]\tLoss: 0.155377\n",
      "Training Epoch: 3 [42688/60000 (71%)]\tLoss: 0.487220\n",
      "Training Epoch: 3 [42720/60000 (71%)]\tLoss: 0.374439\n",
      "Training Epoch: 3 [42752/60000 (71%)]\tLoss: 0.310761\n",
      "Training Epoch: 3 [42784/60000 (71%)]\tLoss: 0.536168\n",
      "Training Epoch: 3 [42816/60000 (71%)]\tLoss: 0.770101\n",
      "Training Epoch: 3 [42848/60000 (71%)]\tLoss: 0.231025\n",
      "Training Epoch: 3 [42880/60000 (71%)]\tLoss: 0.359239\n",
      "Training Epoch: 3 [42912/60000 (72%)]\tLoss: 0.637022\n",
      "Training Epoch: 3 [42944/60000 (72%)]\tLoss: 0.395230\n",
      "Training Epoch: 3 [42976/60000 (72%)]\tLoss: 0.199309\n",
      "Training Epoch: 3 [43008/60000 (72%)]\tLoss: 0.372555\n",
      "Training Epoch: 3 [43040/60000 (72%)]\tLoss: 0.391912\n",
      "Training Epoch: 3 [43072/60000 (72%)]\tLoss: 0.808061\n",
      "Training Epoch: 3 [43104/60000 (72%)]\tLoss: 0.488102\n",
      "Training Epoch: 3 [43136/60000 (72%)]\tLoss: 0.523867\n",
      "Training Epoch: 3 [43168/60000 (72%)]\tLoss: 0.743272\n",
      "Training Epoch: 3 [43200/60000 (72%)]\tLoss: 0.530597\n",
      "Training Epoch: 3 [43232/60000 (72%)]\tLoss: 0.476086\n",
      "Training Epoch: 3 [43264/60000 (72%)]\tLoss: 0.518309\n",
      "Training Epoch: 3 [43296/60000 (72%)]\tLoss: 0.349146\n",
      "Training Epoch: 3 [43328/60000 (72%)]\tLoss: 0.397576\n",
      "Training Epoch: 3 [43360/60000 (72%)]\tLoss: 0.225908\n",
      "Training Epoch: 3 [43392/60000 (72%)]\tLoss: 0.354236\n",
      "Training Epoch: 3 [43424/60000 (72%)]\tLoss: 0.363203\n",
      "Training Epoch: 3 [43456/60000 (72%)]\tLoss: 0.294722\n",
      "Training Epoch: 3 [43488/60000 (72%)]\tLoss: 0.315539\n",
      "Training Epoch: 3 [43520/60000 (73%)]\tLoss: 0.166009\n",
      "Training Epoch: 3 [43552/60000 (73%)]\tLoss: 0.387324\n",
      "Training Epoch: 3 [43584/60000 (73%)]\tLoss: 0.392876\n",
      "Training Epoch: 3 [43616/60000 (73%)]\tLoss: 0.257219\n",
      "Training Epoch: 3 [43648/60000 (73%)]\tLoss: 0.571671\n",
      "Training Epoch: 3 [43680/60000 (73%)]\tLoss: 0.357543\n",
      "Training Epoch: 3 [43712/60000 (73%)]\tLoss: 0.618159\n",
      "Training Epoch: 3 [43744/60000 (73%)]\tLoss: 0.476685\n",
      "Training Epoch: 3 [43776/60000 (73%)]\tLoss: 0.907620\n",
      "Training Epoch: 3 [43808/60000 (73%)]\tLoss: 0.129625\n",
      "Training Epoch: 3 [43840/60000 (73%)]\tLoss: 0.481182\n",
      "Training Epoch: 3 [43872/60000 (73%)]\tLoss: 0.231120\n",
      "Training Epoch: 3 [43904/60000 (73%)]\tLoss: 0.400663\n",
      "Training Epoch: 3 [43936/60000 (73%)]\tLoss: 0.206358\n",
      "Training Epoch: 3 [43968/60000 (73%)]\tLoss: 0.378408\n",
      "Training Epoch: 3 [44000/60000 (73%)]\tLoss: 0.429670\n",
      "Training Epoch: 3 [44032/60000 (73%)]\tLoss: 0.254151\n",
      "Training Epoch: 3 [44064/60000 (73%)]\tLoss: 0.370692\n",
      "Training Epoch: 3 [44096/60000 (73%)]\tLoss: 0.205396\n",
      "Training Epoch: 3 [44128/60000 (74%)]\tLoss: 0.557607\n",
      "Training Epoch: 3 [44160/60000 (74%)]\tLoss: 0.194723\n",
      "Training Epoch: 3 [44192/60000 (74%)]\tLoss: 0.282300\n",
      "Training Epoch: 3 [44224/60000 (74%)]\tLoss: 0.331675\n",
      "Training Epoch: 3 [44256/60000 (74%)]\tLoss: 0.583826\n",
      "Training Epoch: 3 [44288/60000 (74%)]\tLoss: 0.521740\n",
      "Training Epoch: 3 [44320/60000 (74%)]\tLoss: 0.479604\n",
      "Training Epoch: 3 [44352/60000 (74%)]\tLoss: 0.552400\n",
      "Training Epoch: 3 [44384/60000 (74%)]\tLoss: 0.184994\n",
      "Training Epoch: 3 [44416/60000 (74%)]\tLoss: 0.237997\n",
      "Training Epoch: 3 [44448/60000 (74%)]\tLoss: 0.696692\n",
      "Training Epoch: 3 [44480/60000 (74%)]\tLoss: 0.175449\n",
      "Training Epoch: 3 [44512/60000 (74%)]\tLoss: 0.488272\n",
      "Training Epoch: 3 [44544/60000 (74%)]\tLoss: 0.282739\n",
      "Training Epoch: 3 [44576/60000 (74%)]\tLoss: 0.339072\n",
      "Training Epoch: 3 [44608/60000 (74%)]\tLoss: 0.409418\n",
      "Training Epoch: 3 [44640/60000 (74%)]\tLoss: 0.634311\n",
      "Training Epoch: 3 [44672/60000 (74%)]\tLoss: 0.324643\n",
      "Training Epoch: 3 [44704/60000 (75%)]\tLoss: 0.497196\n",
      "Training Epoch: 3 [44736/60000 (75%)]\tLoss: 0.196523\n",
      "Training Epoch: 3 [44768/60000 (75%)]\tLoss: 0.293979\n",
      "Training Epoch: 3 [44800/60000 (75%)]\tLoss: 0.789270\n",
      "Training Epoch: 3 [44832/60000 (75%)]\tLoss: 0.497232\n",
      "Training Epoch: 3 [44864/60000 (75%)]\tLoss: 0.172113\n",
      "Training Epoch: 3 [44896/60000 (75%)]\tLoss: 0.581400\n",
      "Training Epoch: 3 [44928/60000 (75%)]\tLoss: 0.533860\n",
      "Training Epoch: 3 [44960/60000 (75%)]\tLoss: 0.326185\n",
      "Training Epoch: 3 [44992/60000 (75%)]\tLoss: 0.074693\n",
      "Training Epoch: 3 [45024/60000 (75%)]\tLoss: 0.258362\n",
      "Training Epoch: 3 [45056/60000 (75%)]\tLoss: 0.266625\n",
      "Training Epoch: 3 [45088/60000 (75%)]\tLoss: 0.140214\n",
      "Training Epoch: 3 [45120/60000 (75%)]\tLoss: 0.225378\n",
      "Training Epoch: 3 [45152/60000 (75%)]\tLoss: 0.499495\n",
      "Training Epoch: 3 [45184/60000 (75%)]\tLoss: 0.156281\n",
      "Training Epoch: 3 [45216/60000 (75%)]\tLoss: 0.658778\n",
      "Training Epoch: 3 [45248/60000 (75%)]\tLoss: 0.563453\n",
      "Training Epoch: 3 [45280/60000 (75%)]\tLoss: 0.241387\n",
      "Training Epoch: 3 [45312/60000 (76%)]\tLoss: 0.222093\n",
      "Training Epoch: 3 [45344/60000 (76%)]\tLoss: 0.218979\n",
      "Training Epoch: 3 [45376/60000 (76%)]\tLoss: 0.308930\n",
      "Training Epoch: 3 [45408/60000 (76%)]\tLoss: 0.310212\n",
      "Training Epoch: 3 [45440/60000 (76%)]\tLoss: 0.446737\n",
      "Training Epoch: 3 [45472/60000 (76%)]\tLoss: 0.252979\n",
      "Training Epoch: 3 [45504/60000 (76%)]\tLoss: 0.206404\n",
      "Training Epoch: 3 [45536/60000 (76%)]\tLoss: 0.872307\n",
      "Training Epoch: 3 [45568/60000 (76%)]\tLoss: 0.544248\n",
      "Training Epoch: 3 [45600/60000 (76%)]\tLoss: 0.210355\n",
      "Training Epoch: 3 [45632/60000 (76%)]\tLoss: 0.400638\n",
      "Training Epoch: 3 [45664/60000 (76%)]\tLoss: 0.545860\n",
      "Training Epoch: 3 [45696/60000 (76%)]\tLoss: 0.704996\n",
      "Training Epoch: 3 [45728/60000 (76%)]\tLoss: 0.388501\n",
      "Training Epoch: 3 [45760/60000 (76%)]\tLoss: 0.493340\n",
      "Training Epoch: 3 [45792/60000 (76%)]\tLoss: 0.723105\n",
      "Training Epoch: 3 [45824/60000 (76%)]\tLoss: 0.260764\n",
      "Training Epoch: 3 [45856/60000 (76%)]\tLoss: 0.600879\n",
      "Training Epoch: 3 [45888/60000 (76%)]\tLoss: 0.407749\n",
      "Training Epoch: 3 [45920/60000 (77%)]\tLoss: 0.361008\n",
      "Training Epoch: 3 [45952/60000 (77%)]\tLoss: 0.294226\n",
      "Training Epoch: 3 [45984/60000 (77%)]\tLoss: 0.397622\n",
      "Training Epoch: 3 [46016/60000 (77%)]\tLoss: 0.546212\n",
      "Training Epoch: 3 [46048/60000 (77%)]\tLoss: 0.293807\n",
      "Training Epoch: 3 [46080/60000 (77%)]\tLoss: 0.351166\n",
      "Training Epoch: 3 [46112/60000 (77%)]\tLoss: 0.176135\n",
      "Training Epoch: 3 [46144/60000 (77%)]\tLoss: 0.634480\n",
      "Training Epoch: 3 [46176/60000 (77%)]\tLoss: 0.286171\n",
      "Training Epoch: 3 [46208/60000 (77%)]\tLoss: 0.507348\n",
      "Training Epoch: 3 [46240/60000 (77%)]\tLoss: 0.422190\n",
      "Training Epoch: 3 [46272/60000 (77%)]\tLoss: 0.159533\n",
      "Training Epoch: 3 [46304/60000 (77%)]\tLoss: 0.303220\n",
      "Training Epoch: 3 [46336/60000 (77%)]\tLoss: 0.359158\n",
      "Training Epoch: 3 [46368/60000 (77%)]\tLoss: 0.685869\n",
      "Training Epoch: 3 [46400/60000 (77%)]\tLoss: 0.623654\n",
      "Training Epoch: 3 [46432/60000 (77%)]\tLoss: 0.245851\n",
      "Training Epoch: 3 [46464/60000 (77%)]\tLoss: 0.493087\n",
      "Training Epoch: 3 [46496/60000 (77%)]\tLoss: 0.300348\n",
      "Training Epoch: 3 [46528/60000 (78%)]\tLoss: 0.520843\n",
      "Training Epoch: 3 [46560/60000 (78%)]\tLoss: 0.252895\n",
      "Training Epoch: 3 [46592/60000 (78%)]\tLoss: 0.406252\n",
      "Training Epoch: 3 [46624/60000 (78%)]\tLoss: 0.197667\n",
      "Training Epoch: 3 [46656/60000 (78%)]\tLoss: 0.627723\n",
      "Training Epoch: 3 [46688/60000 (78%)]\tLoss: 0.214189\n",
      "Training Epoch: 3 [46720/60000 (78%)]\tLoss: 0.224822\n",
      "Training Epoch: 3 [46752/60000 (78%)]\tLoss: 0.343139\n",
      "Training Epoch: 3 [46784/60000 (78%)]\tLoss: 0.300340\n",
      "Training Epoch: 3 [46816/60000 (78%)]\tLoss: 0.452888\n",
      "Training Epoch: 3 [46848/60000 (78%)]\tLoss: 0.217542\n",
      "Training Epoch: 3 [46880/60000 (78%)]\tLoss: 0.264400\n",
      "Training Epoch: 3 [46912/60000 (78%)]\tLoss: 0.200769\n",
      "Training Epoch: 3 [46944/60000 (78%)]\tLoss: 0.236573\n",
      "Training Epoch: 3 [46976/60000 (78%)]\tLoss: 0.287219\n",
      "Training Epoch: 3 [47008/60000 (78%)]\tLoss: 0.710663\n",
      "Training Epoch: 3 [47040/60000 (78%)]\tLoss: 0.740942\n",
      "Training Epoch: 3 [47072/60000 (78%)]\tLoss: 0.272433\n",
      "Training Epoch: 3 [47104/60000 (79%)]\tLoss: 0.188669\n",
      "Training Epoch: 3 [47136/60000 (79%)]\tLoss: 0.265105\n",
      "Training Epoch: 3 [47168/60000 (79%)]\tLoss: 0.292938\n",
      "Training Epoch: 3 [47200/60000 (79%)]\tLoss: 0.282606\n",
      "Training Epoch: 3 [47232/60000 (79%)]\tLoss: 0.492211\n",
      "Training Epoch: 3 [47264/60000 (79%)]\tLoss: 0.672756\n",
      "Training Epoch: 3 [47296/60000 (79%)]\tLoss: 0.452415\n",
      "Training Epoch: 3 [47328/60000 (79%)]\tLoss: 0.177246\n",
      "Training Epoch: 3 [47360/60000 (79%)]\tLoss: 0.321072\n",
      "Training Epoch: 3 [47392/60000 (79%)]\tLoss: 0.835782\n",
      "Training Epoch: 3 [47424/60000 (79%)]\tLoss: 0.292022\n",
      "Training Epoch: 3 [47456/60000 (79%)]\tLoss: 0.457554\n",
      "Training Epoch: 3 [47488/60000 (79%)]\tLoss: 0.507690\n",
      "Training Epoch: 3 [47520/60000 (79%)]\tLoss: 0.659606\n",
      "Training Epoch: 3 [47552/60000 (79%)]\tLoss: 0.350636\n",
      "Training Epoch: 3 [47584/60000 (79%)]\tLoss: 0.171860\n",
      "Training Epoch: 3 [47616/60000 (79%)]\tLoss: 0.089996\n",
      "Training Epoch: 3 [47648/60000 (79%)]\tLoss: 0.593056\n",
      "Training Epoch: 3 [47680/60000 (79%)]\tLoss: 0.355933\n",
      "Training Epoch: 3 [47712/60000 (80%)]\tLoss: 1.312410\n",
      "Training Epoch: 3 [47744/60000 (80%)]\tLoss: 0.576573\n",
      "Training Epoch: 3 [47776/60000 (80%)]\tLoss: 0.417052\n",
      "Training Epoch: 3 [47808/60000 (80%)]\tLoss: 0.168003\n",
      "Training Epoch: 3 [47840/60000 (80%)]\tLoss: 0.653326\n",
      "Training Epoch: 3 [47872/60000 (80%)]\tLoss: 0.725363\n",
      "Training Epoch: 3 [47904/60000 (80%)]\tLoss: 0.316995\n",
      "Training Epoch: 3 [47936/60000 (80%)]\tLoss: 0.214818\n",
      "Training Epoch: 3 [47968/60000 (80%)]\tLoss: 0.118239\n",
      "Training Epoch: 3 [48000/60000 (80%)]\tLoss: 0.974401\n",
      "Training Epoch: 3 [48032/60000 (80%)]\tLoss: 0.587081\n",
      "Training Epoch: 3 [48064/60000 (80%)]\tLoss: 0.582255\n",
      "Training Epoch: 3 [48096/60000 (80%)]\tLoss: 0.312920\n",
      "Training Epoch: 3 [48128/60000 (80%)]\tLoss: 0.224830\n",
      "Training Epoch: 3 [48160/60000 (80%)]\tLoss: 0.548376\n",
      "Training Epoch: 3 [48192/60000 (80%)]\tLoss: 0.392124\n",
      "Training Epoch: 3 [48224/60000 (80%)]\tLoss: 0.165757\n",
      "Training Epoch: 3 [48256/60000 (80%)]\tLoss: 0.103661\n",
      "Training Epoch: 3 [48288/60000 (80%)]\tLoss: 0.500478\n",
      "Training Epoch: 3 [48320/60000 (81%)]\tLoss: 0.291326\n",
      "Training Epoch: 3 [48352/60000 (81%)]\tLoss: 0.146823\n",
      "Training Epoch: 3 [48384/60000 (81%)]\tLoss: 0.470058\n",
      "Training Epoch: 3 [48416/60000 (81%)]\tLoss: 0.258790\n",
      "Training Epoch: 3 [48448/60000 (81%)]\tLoss: 0.376422\n",
      "Training Epoch: 3 [48480/60000 (81%)]\tLoss: 0.157191\n",
      "Training Epoch: 3 [48512/60000 (81%)]\tLoss: 0.163888\n",
      "Training Epoch: 3 [48544/60000 (81%)]\tLoss: 0.696836\n",
      "Training Epoch: 3 [48576/60000 (81%)]\tLoss: 0.148826\n",
      "Training Epoch: 3 [48608/60000 (81%)]\tLoss: 0.347051\n",
      "Training Epoch: 3 [48640/60000 (81%)]\tLoss: 0.236529\n",
      "Training Epoch: 3 [48672/60000 (81%)]\tLoss: 0.877774\n",
      "Training Epoch: 3 [48704/60000 (81%)]\tLoss: 0.387176\n",
      "Training Epoch: 3 [48736/60000 (81%)]\tLoss: 0.304668\n",
      "Training Epoch: 3 [48768/60000 (81%)]\tLoss: 0.533106\n",
      "Training Epoch: 3 [48800/60000 (81%)]\tLoss: 0.483902\n",
      "Training Epoch: 3 [48832/60000 (81%)]\tLoss: 0.528463\n",
      "Training Epoch: 3 [48864/60000 (81%)]\tLoss: 0.817422\n",
      "Training Epoch: 3 [48896/60000 (81%)]\tLoss: 0.449780\n",
      "Training Epoch: 3 [48928/60000 (82%)]\tLoss: 0.123106\n",
      "Training Epoch: 3 [48960/60000 (82%)]\tLoss: 0.475910\n",
      "Training Epoch: 3 [48992/60000 (82%)]\tLoss: 0.416259\n",
      "Training Epoch: 3 [49024/60000 (82%)]\tLoss: 0.184573\n",
      "Training Epoch: 3 [49056/60000 (82%)]\tLoss: 0.498311\n",
      "Training Epoch: 3 [49088/60000 (82%)]\tLoss: 0.327036\n",
      "Training Epoch: 3 [49120/60000 (82%)]\tLoss: 0.166443\n",
      "Training Epoch: 3 [49152/60000 (82%)]\tLoss: 0.405559\n",
      "Training Epoch: 3 [49184/60000 (82%)]\tLoss: 0.089585\n",
      "Training Epoch: 3 [49216/60000 (82%)]\tLoss: 0.602167\n",
      "Training Epoch: 3 [49248/60000 (82%)]\tLoss: 0.555618\n",
      "Training Epoch: 3 [49280/60000 (82%)]\tLoss: 0.617033\n",
      "Training Epoch: 3 [49312/60000 (82%)]\tLoss: 1.019533\n",
      "Training Epoch: 3 [49344/60000 (82%)]\tLoss: 0.191508\n",
      "Training Epoch: 3 [49376/60000 (82%)]\tLoss: 0.298604\n",
      "Training Epoch: 3 [49408/60000 (82%)]\tLoss: 0.270050\n",
      "Training Epoch: 3 [49440/60000 (82%)]\tLoss: 0.326384\n",
      "Training Epoch: 3 [49472/60000 (82%)]\tLoss: 0.607523\n",
      "Training Epoch: 3 [49504/60000 (83%)]\tLoss: 0.393178\n",
      "Training Epoch: 3 [49536/60000 (83%)]\tLoss: 0.251836\n",
      "Training Epoch: 3 [49568/60000 (83%)]\tLoss: 0.394516\n",
      "Training Epoch: 3 [49600/60000 (83%)]\tLoss: 0.524186\n",
      "Training Epoch: 3 [49632/60000 (83%)]\tLoss: 0.511410\n",
      "Training Epoch: 3 [49664/60000 (83%)]\tLoss: 0.443411\n",
      "Training Epoch: 3 [49696/60000 (83%)]\tLoss: 0.199003\n",
      "Training Epoch: 3 [49728/60000 (83%)]\tLoss: 0.131390\n",
      "Training Epoch: 3 [49760/60000 (83%)]\tLoss: 0.393659\n",
      "Training Epoch: 3 [49792/60000 (83%)]\tLoss: 0.177079\n",
      "Training Epoch: 3 [49824/60000 (83%)]\tLoss: 0.780305\n",
      "Training Epoch: 3 [49856/60000 (83%)]\tLoss: 0.343558\n",
      "Training Epoch: 3 [49888/60000 (83%)]\tLoss: 0.280854\n",
      "Training Epoch: 3 [49920/60000 (83%)]\tLoss: 0.376429\n",
      "Training Epoch: 3 [49952/60000 (83%)]\tLoss: 0.410023\n",
      "Training Epoch: 3 [49984/60000 (83%)]\tLoss: 0.536487\n",
      "Training Epoch: 3 [50016/60000 (83%)]\tLoss: 0.418411\n",
      "Training Epoch: 3 [50048/60000 (83%)]\tLoss: 0.147112\n",
      "Training Epoch: 3 [50080/60000 (83%)]\tLoss: 0.208964\n",
      "Training Epoch: 3 [50112/60000 (84%)]\tLoss: 0.282158\n",
      "Training Epoch: 3 [50144/60000 (84%)]\tLoss: 0.856554\n",
      "Training Epoch: 3 [50176/60000 (84%)]\tLoss: 0.337188\n",
      "Training Epoch: 3 [50208/60000 (84%)]\tLoss: 0.329845\n",
      "Training Epoch: 3 [50240/60000 (84%)]\tLoss: 0.535587\n",
      "Training Epoch: 3 [50272/60000 (84%)]\tLoss: 0.377774\n",
      "Training Epoch: 3 [50304/60000 (84%)]\tLoss: 0.407454\n",
      "Training Epoch: 3 [50336/60000 (84%)]\tLoss: 0.412521\n",
      "Training Epoch: 3 [50368/60000 (84%)]\tLoss: 0.547389\n",
      "Training Epoch: 3 [50400/60000 (84%)]\tLoss: 0.329134\n",
      "Training Epoch: 3 [50432/60000 (84%)]\tLoss: 0.644755\n",
      "Training Epoch: 3 [50464/60000 (84%)]\tLoss: 0.197105\n",
      "Training Epoch: 3 [50496/60000 (84%)]\tLoss: 0.889379\n",
      "Training Epoch: 3 [50528/60000 (84%)]\tLoss: 0.649006\n",
      "Training Epoch: 3 [50560/60000 (84%)]\tLoss: 0.502294\n",
      "Training Epoch: 3 [50592/60000 (84%)]\tLoss: 0.460683\n",
      "Training Epoch: 3 [50624/60000 (84%)]\tLoss: 0.296046\n",
      "Training Epoch: 3 [50656/60000 (84%)]\tLoss: 0.487305\n",
      "Training Epoch: 3 [50688/60000 (84%)]\tLoss: 0.610357\n",
      "Training Epoch: 3 [50720/60000 (85%)]\tLoss: 0.198097\n",
      "Training Epoch: 3 [50752/60000 (85%)]\tLoss: 0.606919\n",
      "Training Epoch: 3 [50784/60000 (85%)]\tLoss: 0.682369\n",
      "Training Epoch: 3 [50816/60000 (85%)]\tLoss: 0.351607\n",
      "Training Epoch: 3 [50848/60000 (85%)]\tLoss: 0.253487\n",
      "Training Epoch: 3 [50880/60000 (85%)]\tLoss: 0.538459\n",
      "Training Epoch: 3 [50912/60000 (85%)]\tLoss: 0.342567\n",
      "Training Epoch: 3 [50944/60000 (85%)]\tLoss: 0.543054\n",
      "Training Epoch: 3 [50976/60000 (85%)]\tLoss: 0.384358\n",
      "Training Epoch: 3 [51008/60000 (85%)]\tLoss: 0.187587\n",
      "Training Epoch: 3 [51040/60000 (85%)]\tLoss: 0.200071\n",
      "Training Epoch: 3 [51072/60000 (85%)]\tLoss: 0.224486\n",
      "Training Epoch: 3 [51104/60000 (85%)]\tLoss: 0.369706\n",
      "Training Epoch: 3 [51136/60000 (85%)]\tLoss: 0.702811\n",
      "Training Epoch: 3 [51168/60000 (85%)]\tLoss: 0.303009\n",
      "Training Epoch: 3 [51200/60000 (85%)]\tLoss: 0.872745\n",
      "Training Epoch: 3 [51232/60000 (85%)]\tLoss: 0.850478\n",
      "Training Epoch: 3 [51264/60000 (85%)]\tLoss: 0.600063\n",
      "Training Epoch: 3 [51296/60000 (85%)]\tLoss: 0.387684\n",
      "Training Epoch: 3 [51328/60000 (86%)]\tLoss: 0.348664\n",
      "Training Epoch: 3 [51360/60000 (86%)]\tLoss: 0.195029\n",
      "Training Epoch: 3 [51392/60000 (86%)]\tLoss: 0.415251\n",
      "Training Epoch: 3 [51424/60000 (86%)]\tLoss: 0.393602\n",
      "Training Epoch: 3 [51456/60000 (86%)]\tLoss: 0.258298\n",
      "Training Epoch: 3 [51488/60000 (86%)]\tLoss: 0.375939\n",
      "Training Epoch: 3 [51520/60000 (86%)]\tLoss: 0.171529\n",
      "Training Epoch: 3 [51552/60000 (86%)]\tLoss: 0.552855\n",
      "Training Epoch: 3 [51584/60000 (86%)]\tLoss: 0.879335\n",
      "Training Epoch: 3 [51616/60000 (86%)]\tLoss: 0.501963\n",
      "Training Epoch: 3 [51648/60000 (86%)]\tLoss: 0.348291\n",
      "Training Epoch: 3 [51680/60000 (86%)]\tLoss: 0.541332\n",
      "Training Epoch: 3 [51712/60000 (86%)]\tLoss: 0.190617\n",
      "Training Epoch: 3 [51744/60000 (86%)]\tLoss: 0.334664\n",
      "Training Epoch: 3 [51776/60000 (86%)]\tLoss: 0.402404\n",
      "Training Epoch: 3 [51808/60000 (86%)]\tLoss: 0.289074\n",
      "Training Epoch: 3 [51840/60000 (86%)]\tLoss: 0.405231\n",
      "Training Epoch: 3 [51872/60000 (86%)]\tLoss: 0.332116\n",
      "Training Epoch: 3 [51904/60000 (87%)]\tLoss: 0.389728\n",
      "Training Epoch: 3 [51936/60000 (87%)]\tLoss: 0.559021\n",
      "Training Epoch: 3 [51968/60000 (87%)]\tLoss: 0.372557\n",
      "Training Epoch: 3 [52000/60000 (87%)]\tLoss: 0.904553\n",
      "Training Epoch: 3 [52032/60000 (87%)]\tLoss: 0.316693\n",
      "Training Epoch: 3 [52064/60000 (87%)]\tLoss: 0.511148\n",
      "Training Epoch: 3 [52096/60000 (87%)]\tLoss: 0.280080\n",
      "Training Epoch: 3 [52128/60000 (87%)]\tLoss: 0.446967\n",
      "Training Epoch: 3 [52160/60000 (87%)]\tLoss: 0.537118\n",
      "Training Epoch: 3 [52192/60000 (87%)]\tLoss: 0.309152\n",
      "Training Epoch: 3 [52224/60000 (87%)]\tLoss: 0.283411\n",
      "Training Epoch: 3 [52256/60000 (87%)]\tLoss: 0.185733\n",
      "Training Epoch: 3 [52288/60000 (87%)]\tLoss: 0.269902\n",
      "Training Epoch: 3 [52320/60000 (87%)]\tLoss: 0.350260\n",
      "Training Epoch: 3 [52352/60000 (87%)]\tLoss: 0.218273\n",
      "Training Epoch: 3 [52384/60000 (87%)]\tLoss: 0.178710\n",
      "Training Epoch: 3 [52416/60000 (87%)]\tLoss: 0.620356\n",
      "Training Epoch: 3 [52448/60000 (87%)]\tLoss: 0.099895\n",
      "Training Epoch: 3 [52480/60000 (87%)]\tLoss: 0.138462\n",
      "Training Epoch: 3 [52512/60000 (88%)]\tLoss: 0.441974\n",
      "Training Epoch: 3 [52544/60000 (88%)]\tLoss: 1.124882\n",
      "Training Epoch: 3 [52576/60000 (88%)]\tLoss: 0.577444\n",
      "Training Epoch: 3 [52608/60000 (88%)]\tLoss: 0.284928\n",
      "Training Epoch: 3 [52640/60000 (88%)]\tLoss: 0.152386\n",
      "Training Epoch: 3 [52672/60000 (88%)]\tLoss: 0.380034\n",
      "Training Epoch: 3 [52704/60000 (88%)]\tLoss: 0.532681\n",
      "Training Epoch: 3 [52736/60000 (88%)]\tLoss: 0.136689\n",
      "Training Epoch: 3 [52768/60000 (88%)]\tLoss: 0.187151\n",
      "Training Epoch: 3 [52800/60000 (88%)]\tLoss: 0.402870\n",
      "Training Epoch: 3 [52832/60000 (88%)]\tLoss: 0.596009\n",
      "Training Epoch: 3 [52864/60000 (88%)]\tLoss: 0.135049\n",
      "Training Epoch: 3 [52896/60000 (88%)]\tLoss: 0.442802\n",
      "Training Epoch: 3 [52928/60000 (88%)]\tLoss: 0.118531\n",
      "Training Epoch: 3 [52960/60000 (88%)]\tLoss: 0.387556\n",
      "Training Epoch: 3 [52992/60000 (88%)]\tLoss: 0.278238\n",
      "Training Epoch: 3 [53024/60000 (88%)]\tLoss: 0.503035\n",
      "Training Epoch: 3 [53056/60000 (88%)]\tLoss: 0.234497\n",
      "Training Epoch: 3 [53088/60000 (88%)]\tLoss: 0.425572\n",
      "Training Epoch: 3 [53120/60000 (89%)]\tLoss: 0.508117\n",
      "Training Epoch: 3 [53152/60000 (89%)]\tLoss: 0.314203\n",
      "Training Epoch: 3 [53184/60000 (89%)]\tLoss: 0.742129\n",
      "Training Epoch: 3 [53216/60000 (89%)]\tLoss: 0.640428\n",
      "Training Epoch: 3 [53248/60000 (89%)]\tLoss: 0.394425\n",
      "Training Epoch: 3 [53280/60000 (89%)]\tLoss: 1.053096\n",
      "Training Epoch: 3 [53312/60000 (89%)]\tLoss: 0.222799\n",
      "Training Epoch: 3 [53344/60000 (89%)]\tLoss: 0.329225\n",
      "Training Epoch: 3 [53376/60000 (89%)]\tLoss: 0.288592\n",
      "Training Epoch: 3 [53408/60000 (89%)]\tLoss: 0.505483\n",
      "Training Epoch: 3 [53440/60000 (89%)]\tLoss: 0.409356\n",
      "Training Epoch: 3 [53472/60000 (89%)]\tLoss: 0.471430\n",
      "Training Epoch: 3 [53504/60000 (89%)]\tLoss: 0.635945\n",
      "Training Epoch: 3 [53536/60000 (89%)]\tLoss: 0.343459\n",
      "Training Epoch: 3 [53568/60000 (89%)]\tLoss: 0.514842\n",
      "Training Epoch: 3 [53600/60000 (89%)]\tLoss: 0.482281\n",
      "Training Epoch: 3 [53632/60000 (89%)]\tLoss: 0.850662\n",
      "Training Epoch: 3 [53664/60000 (89%)]\tLoss: 0.458543\n",
      "Training Epoch: 3 [53696/60000 (89%)]\tLoss: 0.584908\n",
      "Training Epoch: 3 [53728/60000 (90%)]\tLoss: 0.347030\n",
      "Training Epoch: 3 [53760/60000 (90%)]\tLoss: 0.656268\n",
      "Training Epoch: 3 [53792/60000 (90%)]\tLoss: 0.163276\n",
      "Training Epoch: 3 [53824/60000 (90%)]\tLoss: 0.347400\n",
      "Training Epoch: 3 [53856/60000 (90%)]\tLoss: 0.563200\n",
      "Training Epoch: 3 [53888/60000 (90%)]\tLoss: 0.537732\n",
      "Training Epoch: 3 [53920/60000 (90%)]\tLoss: 0.208028\n",
      "Training Epoch: 3 [53952/60000 (90%)]\tLoss: 0.402688\n",
      "Training Epoch: 3 [53984/60000 (90%)]\tLoss: 0.338914\n",
      "Training Epoch: 3 [54016/60000 (90%)]\tLoss: 0.547508\n",
      "Training Epoch: 3 [54048/60000 (90%)]\tLoss: 0.202523\n",
      "Training Epoch: 3 [54080/60000 (90%)]\tLoss: 0.320694\n",
      "Training Epoch: 3 [54112/60000 (90%)]\tLoss: 0.546939\n",
      "Training Epoch: 3 [54144/60000 (90%)]\tLoss: 0.329935\n",
      "Training Epoch: 3 [54176/60000 (90%)]\tLoss: 0.202696\n",
      "Training Epoch: 3 [54208/60000 (90%)]\tLoss: 0.355780\n",
      "Training Epoch: 3 [54240/60000 (90%)]\tLoss: 0.341682\n",
      "Training Epoch: 3 [54272/60000 (90%)]\tLoss: 0.273974\n",
      "Training Epoch: 3 [54304/60000 (91%)]\tLoss: 0.700836\n",
      "Training Epoch: 3 [54336/60000 (91%)]\tLoss: 0.563302\n",
      "Training Epoch: 3 [54368/60000 (91%)]\tLoss: 0.460154\n",
      "Training Epoch: 3 [54400/60000 (91%)]\tLoss: 0.215684\n",
      "Training Epoch: 3 [54432/60000 (91%)]\tLoss: 0.421026\n",
      "Training Epoch: 3 [54464/60000 (91%)]\tLoss: 0.286413\n",
      "Training Epoch: 3 [54496/60000 (91%)]\tLoss: 0.454943\n",
      "Training Epoch: 3 [54528/60000 (91%)]\tLoss: 0.538867\n",
      "Training Epoch: 3 [54560/60000 (91%)]\tLoss: 0.186023\n",
      "Training Epoch: 3 [54592/60000 (91%)]\tLoss: 0.508761\n",
      "Training Epoch: 3 [54624/60000 (91%)]\tLoss: 0.495603\n",
      "Training Epoch: 3 [54656/60000 (91%)]\tLoss: 0.579323\n",
      "Training Epoch: 3 [54688/60000 (91%)]\tLoss: 0.579045\n",
      "Training Epoch: 3 [54720/60000 (91%)]\tLoss: 0.306839\n",
      "Training Epoch: 3 [54752/60000 (91%)]\tLoss: 0.171332\n",
      "Training Epoch: 3 [54784/60000 (91%)]\tLoss: 0.461424\n",
      "Training Epoch: 3 [54816/60000 (91%)]\tLoss: 0.428130\n",
      "Training Epoch: 3 [54848/60000 (91%)]\tLoss: 0.686560\n",
      "Training Epoch: 3 [54880/60000 (91%)]\tLoss: 0.355289\n",
      "Training Epoch: 3 [54912/60000 (92%)]\tLoss: 0.553331\n",
      "Training Epoch: 3 [54944/60000 (92%)]\tLoss: 0.762064\n",
      "Training Epoch: 3 [54976/60000 (92%)]\tLoss: 0.316504\n",
      "Training Epoch: 3 [55008/60000 (92%)]\tLoss: 0.254895\n",
      "Training Epoch: 3 [55040/60000 (92%)]\tLoss: 0.337706\n",
      "Training Epoch: 3 [55072/60000 (92%)]\tLoss: 0.220156\n",
      "Training Epoch: 3 [55104/60000 (92%)]\tLoss: 0.523484\n",
      "Training Epoch: 3 [55136/60000 (92%)]\tLoss: 0.136264\n",
      "Training Epoch: 3 [55168/60000 (92%)]\tLoss: 0.417182\n",
      "Training Epoch: 3 [55200/60000 (92%)]\tLoss: 0.261256\n",
      "Training Epoch: 3 [55232/60000 (92%)]\tLoss: 0.289948\n",
      "Training Epoch: 3 [55264/60000 (92%)]\tLoss: 0.355395\n",
      "Training Epoch: 3 [55296/60000 (92%)]\tLoss: 0.244254\n",
      "Training Epoch: 3 [55328/60000 (92%)]\tLoss: 0.387389\n",
      "Training Epoch: 3 [55360/60000 (92%)]\tLoss: 0.287473\n",
      "Training Epoch: 3 [55392/60000 (92%)]\tLoss: 0.688833\n",
      "Training Epoch: 3 [55424/60000 (92%)]\tLoss: 0.322239\n",
      "Training Epoch: 3 [55456/60000 (92%)]\tLoss: 0.270555\n",
      "Training Epoch: 3 [55488/60000 (92%)]\tLoss: 0.138961\n",
      "Training Epoch: 3 [55520/60000 (93%)]\tLoss: 0.296550\n",
      "Training Epoch: 3 [55552/60000 (93%)]\tLoss: 0.043922\n",
      "Training Epoch: 3 [55584/60000 (93%)]\tLoss: 0.505986\n",
      "Training Epoch: 3 [55616/60000 (93%)]\tLoss: 0.300172\n",
      "Training Epoch: 3 [55648/60000 (93%)]\tLoss: 0.210332\n",
      "Training Epoch: 3 [55680/60000 (93%)]\tLoss: 0.482946\n",
      "Training Epoch: 3 [55712/60000 (93%)]\tLoss: 0.824052\n",
      "Training Epoch: 3 [55744/60000 (93%)]\tLoss: 0.081939\n",
      "Training Epoch: 3 [55776/60000 (93%)]\tLoss: 0.443050\n",
      "Training Epoch: 3 [55808/60000 (93%)]\tLoss: 0.430171\n",
      "Training Epoch: 3 [55840/60000 (93%)]\tLoss: 0.228752\n",
      "Training Epoch: 3 [55872/60000 (93%)]\tLoss: 0.269950\n",
      "Training Epoch: 3 [55904/60000 (93%)]\tLoss: 0.249696\n",
      "Training Epoch: 3 [55936/60000 (93%)]\tLoss: 0.163494\n",
      "Training Epoch: 3 [55968/60000 (93%)]\tLoss: 0.536552\n",
      "Training Epoch: 3 [56000/60000 (93%)]\tLoss: 0.282576\n",
      "Training Epoch: 3 [56032/60000 (93%)]\tLoss: 0.409487\n",
      "Training Epoch: 3 [56064/60000 (93%)]\tLoss: 0.228483\n",
      "Training Epoch: 3 [56096/60000 (93%)]\tLoss: 0.407771\n",
      "Training Epoch: 3 [56128/60000 (94%)]\tLoss: 0.251769\n",
      "Training Epoch: 3 [56160/60000 (94%)]\tLoss: 0.493992\n",
      "Training Epoch: 3 [56192/60000 (94%)]\tLoss: 0.300598\n",
      "Training Epoch: 3 [56224/60000 (94%)]\tLoss: 0.688497\n",
      "Training Epoch: 3 [56256/60000 (94%)]\tLoss: 0.196273\n",
      "Training Epoch: 3 [56288/60000 (94%)]\tLoss: 0.747170\n",
      "Training Epoch: 3 [56320/60000 (94%)]\tLoss: 0.477300\n",
      "Training Epoch: 3 [56352/60000 (94%)]\tLoss: 0.487322\n",
      "Training Epoch: 3 [56384/60000 (94%)]\tLoss: 0.201595\n",
      "Training Epoch: 3 [56416/60000 (94%)]\tLoss: 0.127436\n",
      "Training Epoch: 3 [56448/60000 (94%)]\tLoss: 0.751539\n",
      "Training Epoch: 3 [56480/60000 (94%)]\tLoss: 0.307019\n",
      "Training Epoch: 3 [56512/60000 (94%)]\tLoss: 0.314519\n",
      "Training Epoch: 3 [56544/60000 (94%)]\tLoss: 0.242236\n",
      "Training Epoch: 3 [56576/60000 (94%)]\tLoss: 0.425164\n",
      "Training Epoch: 3 [56608/60000 (94%)]\tLoss: 0.343294\n",
      "Training Epoch: 3 [56640/60000 (94%)]\tLoss: 0.616114\n",
      "Training Epoch: 3 [56672/60000 (94%)]\tLoss: 0.608818\n",
      "Training Epoch: 3 [56704/60000 (95%)]\tLoss: 0.382775\n",
      "Training Epoch: 3 [56736/60000 (95%)]\tLoss: 0.248380\n",
      "Training Epoch: 3 [56768/60000 (95%)]\tLoss: 0.073796\n",
      "Training Epoch: 3 [56800/60000 (95%)]\tLoss: 0.512820\n",
      "Training Epoch: 3 [56832/60000 (95%)]\tLoss: 0.331563\n",
      "Training Epoch: 3 [56864/60000 (95%)]\tLoss: 0.189472\n",
      "Training Epoch: 3 [56896/60000 (95%)]\tLoss: 0.320731\n",
      "Training Epoch: 3 [56928/60000 (95%)]\tLoss: 0.108575\n",
      "Training Epoch: 3 [56960/60000 (95%)]\tLoss: 0.213022\n",
      "Training Epoch: 3 [56992/60000 (95%)]\tLoss: 0.257313\n",
      "Training Epoch: 3 [57024/60000 (95%)]\tLoss: 0.337908\n",
      "Training Epoch: 3 [57056/60000 (95%)]\tLoss: 0.613690\n",
      "Training Epoch: 3 [57088/60000 (95%)]\tLoss: 0.500565\n",
      "Training Epoch: 3 [57120/60000 (95%)]\tLoss: 0.447960\n",
      "Training Epoch: 3 [57152/60000 (95%)]\tLoss: 0.205520\n",
      "Training Epoch: 3 [57184/60000 (95%)]\tLoss: 0.208714\n",
      "Training Epoch: 3 [57216/60000 (95%)]\tLoss: 0.246047\n",
      "Training Epoch: 3 [57248/60000 (95%)]\tLoss: 0.520075\n",
      "Training Epoch: 3 [57280/60000 (95%)]\tLoss: 0.234002\n",
      "Training Epoch: 3 [57312/60000 (96%)]\tLoss: 0.668824\n",
      "Training Epoch: 3 [57344/60000 (96%)]\tLoss: 0.090979\n",
      "Training Epoch: 3 [57376/60000 (96%)]\tLoss: 0.110032\n",
      "Training Epoch: 3 [57408/60000 (96%)]\tLoss: 0.402036\n",
      "Training Epoch: 3 [57440/60000 (96%)]\tLoss: 0.225952\n",
      "Training Epoch: 3 [57472/60000 (96%)]\tLoss: 0.270039\n",
      "Training Epoch: 3 [57504/60000 (96%)]\tLoss: 0.617812\n",
      "Training Epoch: 3 [57536/60000 (96%)]\tLoss: 0.233359\n",
      "Training Epoch: 3 [57568/60000 (96%)]\tLoss: 0.722236\n",
      "Training Epoch: 3 [57600/60000 (96%)]\tLoss: 0.374970\n",
      "Training Epoch: 3 [57632/60000 (96%)]\tLoss: 0.172814\n",
      "Training Epoch: 3 [57664/60000 (96%)]\tLoss: 0.190392\n",
      "Training Epoch: 3 [57696/60000 (96%)]\tLoss: 0.574958\n",
      "Training Epoch: 3 [57728/60000 (96%)]\tLoss: 0.253320\n",
      "Training Epoch: 3 [57760/60000 (96%)]\tLoss: 0.432190\n",
      "Training Epoch: 3 [57792/60000 (96%)]\tLoss: 0.405523\n",
      "Training Epoch: 3 [57824/60000 (96%)]\tLoss: 0.298593\n",
      "Training Epoch: 3 [57856/60000 (96%)]\tLoss: 0.129024\n",
      "Training Epoch: 3 [57888/60000 (96%)]\tLoss: 0.220133\n",
      "Training Epoch: 3 [57920/60000 (97%)]\tLoss: 0.124512\n",
      "Training Epoch: 3 [57952/60000 (97%)]\tLoss: 0.199752\n",
      "Training Epoch: 3 [57984/60000 (97%)]\tLoss: 0.561046\n",
      "Training Epoch: 3 [58016/60000 (97%)]\tLoss: 0.105225\n",
      "Training Epoch: 3 [58048/60000 (97%)]\tLoss: 0.082727\n",
      "Training Epoch: 3 [58080/60000 (97%)]\tLoss: 0.239265\n",
      "Training Epoch: 3 [58112/60000 (97%)]\tLoss: 0.274959\n",
      "Training Epoch: 3 [58144/60000 (97%)]\tLoss: 0.366006\n",
      "Training Epoch: 3 [58176/60000 (97%)]\tLoss: 0.256398\n",
      "Training Epoch: 3 [58208/60000 (97%)]\tLoss: 0.616912\n",
      "Training Epoch: 3 [58240/60000 (97%)]\tLoss: 0.324076\n",
      "Training Epoch: 3 [58272/60000 (97%)]\tLoss: 0.230074\n",
      "Training Epoch: 3 [58304/60000 (97%)]\tLoss: 0.782744\n",
      "Training Epoch: 3 [58336/60000 (97%)]\tLoss: 0.293224\n",
      "Training Epoch: 3 [58368/60000 (97%)]\tLoss: 0.406309\n",
      "Training Epoch: 3 [58400/60000 (97%)]\tLoss: 0.550120\n",
      "Training Epoch: 3 [58432/60000 (97%)]\tLoss: 0.344491\n",
      "Training Epoch: 3 [58464/60000 (97%)]\tLoss: 0.204916\n",
      "Training Epoch: 3 [58496/60000 (97%)]\tLoss: 0.499401\n",
      "Training Epoch: 3 [58528/60000 (98%)]\tLoss: 0.874118\n",
      "Training Epoch: 3 [58560/60000 (98%)]\tLoss: 0.242597\n",
      "Training Epoch: 3 [58592/60000 (98%)]\tLoss: 0.400341\n",
      "Training Epoch: 3 [58624/60000 (98%)]\tLoss: 0.125690\n",
      "Training Epoch: 3 [58656/60000 (98%)]\tLoss: 0.128893\n",
      "Training Epoch: 3 [58688/60000 (98%)]\tLoss: 0.238143\n",
      "Training Epoch: 3 [58720/60000 (98%)]\tLoss: 0.563491\n",
      "Training Epoch: 3 [58752/60000 (98%)]\tLoss: 0.336141\n",
      "Training Epoch: 3 [58784/60000 (98%)]\tLoss: 0.472844\n",
      "Training Epoch: 3 [58816/60000 (98%)]\tLoss: 0.565564\n",
      "Training Epoch: 3 [58848/60000 (98%)]\tLoss: 0.295791\n",
      "Training Epoch: 3 [58880/60000 (98%)]\tLoss: 0.543041\n",
      "Training Epoch: 3 [58912/60000 (98%)]\tLoss: 0.473826\n",
      "Training Epoch: 3 [58944/60000 (98%)]\tLoss: 0.484018\n",
      "Training Epoch: 3 [58976/60000 (98%)]\tLoss: 0.546735\n",
      "Training Epoch: 3 [59008/60000 (98%)]\tLoss: 0.208872\n",
      "Training Epoch: 3 [59040/60000 (98%)]\tLoss: 0.169214\n",
      "Training Epoch: 3 [59072/60000 (98%)]\tLoss: 0.256423\n",
      "Training Epoch: 3 [59104/60000 (99%)]\tLoss: 0.042667\n",
      "Training Epoch: 3 [59136/60000 (99%)]\tLoss: 0.460962\n",
      "Training Epoch: 3 [59168/60000 (99%)]\tLoss: 1.105950\n",
      "Training Epoch: 3 [59200/60000 (99%)]\tLoss: 0.202873\n",
      "Training Epoch: 3 [59232/60000 (99%)]\tLoss: 0.718936\n",
      "Training Epoch: 3 [59264/60000 (99%)]\tLoss: 0.602264\n",
      "Training Epoch: 3 [59296/60000 (99%)]\tLoss: 0.716691\n",
      "Training Epoch: 3 [59328/60000 (99%)]\tLoss: 0.436858\n",
      "Training Epoch: 3 [59360/60000 (99%)]\tLoss: 0.135344\n",
      "Training Epoch: 3 [59392/60000 (99%)]\tLoss: 0.245887\n",
      "Training Epoch: 3 [59424/60000 (99%)]\tLoss: 0.284434\n",
      "Training Epoch: 3 [59456/60000 (99%)]\tLoss: 0.368722\n",
      "Training Epoch: 3 [59488/60000 (99%)]\tLoss: 0.626789\n",
      "Training Epoch: 3 [59520/60000 (99%)]\tLoss: 0.313008\n",
      "Training Epoch: 3 [59552/60000 (99%)]\tLoss: 0.323743\n",
      "Training Epoch: 3 [59584/60000 (99%)]\tLoss: 0.340476\n",
      "Training Epoch: 3 [59616/60000 (99%)]\tLoss: 0.118719\n",
      "Training Epoch: 3 [59648/60000 (99%)]\tLoss: 0.636775\n",
      "Training Epoch: 3 [59680/60000 (99%)]\tLoss: 0.402229\n",
      "Training Epoch: 3 [59712/60000 (100%)]\tLoss: 0.524942\n",
      "Training Epoch: 3 [59744/60000 (100%)]\tLoss: 0.398728\n",
      "Training Epoch: 3 [59776/60000 (100%)]\tLoss: 0.394258\n",
      "Training Epoch: 3 [59808/60000 (100%)]\tLoss: 0.280386\n",
      "Training Epoch: 3 [59840/60000 (100%)]\tLoss: 0.209457\n",
      "Training Epoch: 3 [59872/60000 (100%)]\tLoss: 0.360685\n",
      "Training Epoch: 3 [59904/60000 (100%)]\tLoss: 0.482675\n",
      "Training Epoch: 3 [59936/60000 (100%)]\tLoss: 0.301440\n",
      "Training Epoch: 3 [59968/60000 (100%)]\tLoss: 0.369007\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9668/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    train(epoch)\n",
    "    eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot training and test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Matplotlib works with NumPy arrays, not tensors on the GPU (MPS in our case). Convert to numpy arrays\n",
    "test_counter_cpu = np.array(test_counter)\n",
    "test_losses_cpu = np.array(test_losses)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot training and test loss curves\n",
    "plt.plot(train_counter, train_losses, color='blue')\n",
    "plt.scatter(test_counter, test_losses, color='red')\n",
    "plt.legend(['Train Loss', 'Test Loss'], loc='upper right')\n",
    "\n",
    "# Add labels and title\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Number of Training examples seen')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHMCAYAAABY7eV3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0yklEQVR4nO3deVyVZf7/8c9BEGQ1WcTdJBWcXFJnosbChfymYmZWjxxNtM3Syhy3aipBS7+aNk7m8nMadSzGR05mNZrZWK655JKNTptamnuuQIoIcv3+8CtFns/t4QAHuHg9Hw//8H6f+z4Xxsfe3nBfuIwxRgAAAFDp+ZX3AgAAAFA6KHYAAACWoNgBAABYgmIHAABgCYodAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCWsLnbz588Xl8tV+Mvf31/q168vgwYNkkOHDvlkDY0bN5aBAwcW/n716tXicrlk9erVxbrOhg0bJC0tTc6cOXNF1rFjR+nYsWOJ1lmasrOzZfTo0dK1a1eJjo4Wl8slaWlp5b0seIEZKh+ffPKJPPDAAxIfHy8hISFSr1496dWrl2zbtq28l4ZiYH7Kz+effy533nmn1K1bV4KDgyU+Pl7GjRsn586dK++llTn/8l6AL8ybN0/i4+MlJydH1q5dKxMnTpQ1a9bIzp07JSQkxKdradu2rWzcuFFatGhRrPM2bNgg6enpMnDgQKlZs2aRbObMmaW4wpI7efKkzJkzR1q3bi133nmnvP766+W9JJQQM+Rbs2bNkpMnT8qwYcOkRYsWcvz4cZk6daokJibKihUrpHPnzuW9RBQD8+NbX375pdx8883SvHlzmTZtmkRFRcnatWtl3Lhxsm3bNnnvvffKe4llqkoUu+uvv17at28vIiKdOnWSixcvyvjx4+Xdd9+Vfv36uT3n3LlzEhwcXOprCQ8Pl8TExFK9ZnEHtKw1atRITp8+LS6XS06cOEGxswAz5FszZsyQmJiYIsduv/12ue6662TChAkUu0qG+fGtf/zjH3L+/HlZvHixxMXFiYhI586d5ciRIzJnzhw5ffq0XHPNNeW8yrJj9ZdiNZc/qffv3y8iIgMHDpTQ0FDZuXOndO3aVcLCwqRLly4iInLhwgV58cUXJT4+XgIDAyU6OloGDRokx48fL3LNvLw8GT16tMTGxkpwcLB06NBBPvvssyveW7sNvnnzZunZs6dERkZKUFCQxMXFyVNPPSUiImlpaTJq1CgREbn22msLb+tfvoa72+CnTp2SIUOGSL169aR69erSpEkT+dOf/iS5ublFXudyueTxxx+XN954QxISEiQ4OFhat24tS5cuLfaf6y+v6XK5vD4fFR8z9LOymKFflzoRkdDQUGnRooUcOHDA6+uiYmB+flYW8xMQECAiIhEREUWO16xZU/z8/KR69epeX7syqBJ37H5tz549IiISHR1deOzChQtyxx13yODBg+Xpp5+W/Px8KSgokF69esm6detk9OjRcvPNN8v+/ftl7Nix0rFjR9m6davUqFFDREQefvhhWbBggYwcOVJuu+022bVrl9x1112SnZ191fWsWLFCevbsKQkJCfLKK69Iw4YNZd++ffLRRx+JiMhDDz0kp06dkunTp8s777wjderUERH9X0nnz5+XTp06yd69eyU9PV1atWol69atk4kTJ8qOHTtk2bJlRV6/bNky2bJli4wbN05CQ0Nl8uTJ0rt3b/nmm2+kSZMmha9zuVySlJRU7O/NgH2YId/PUGZmpmzfvp27dRZgfsp2flJTU2XatGny2GOPyaRJkyQ6OlrWrFkj/+///T8ZOnSoz7/87XPGYvPmzTMiYjZt2mTy8vJMdna2Wbp0qYmOjjZhYWHm6NGjxhhjUlNTjYiYuXPnFjl/4cKFRkTM4sWLixzfsmWLEREzc+ZMY4wxX331lRERM3z48CKvy8jIMCJiUlNTC4+tWrXKiIhZtWpV4bG4uDgTFxdncnJy1I/l5ZdfNiJivv/++yuypKQkk5SUVPj72bNnGxExixYtKvK6SZMmGRExH330UeExETG1a9c2WVlZhceOHj1q/Pz8zMSJE4ucX61aNdO5c2d1je4cP37ciIgZO3Zssc5DxcAMlf8MXdavXz/j7+9vtm7d6tX58D3mp/zm56uvvjLx8fFGRAp/Pfnkk6agoMCj8yuzKvGl2MTERAkICJCwsDBJSUmR2NhYWb58udSuXbvI6/r06VPk90uXLpWaNWtKz549JT8/v/BXmzZtJDY2tvBfDatWrRIRueJ7Je69917x93e+Kfrtt9/K3r175cEHH5SgoKASfqSXfPLJJxISEiJ33313keOXn4z6+OOPixzv1KmThIWFFf6+du3aEhMTU/hlgsvy8/OvOBdVAzN0SXnN0PPPPy8ZGRny5z//Wdq1a1fs81G+mJ9LfDU/+/btK/yy8ttvvy1r1qyRyZMny/z58+Whhx7y8qOqPKrEl2IXLFggCQkJ4u/vL7Vr1y68jfxLwcHBEh4eXuTYsWPH5MyZM+rX40+cOCEil54CFRGJjY0tkvv7+0tkZKTj2i5/n0T9+vU9+2A8cPLkSYmNjb3i+9xiYmLE39+/cL2XuVtjYGCg5OTklNqaULkxQ5eUxwylp6fLiy++KC+99JI8/vjjJb4efI/5ucRX8/P0009LVlaW7Nixo/DLrrfeeqtERUXJAw88IAMGDJCkpCSvrl0ZVIlil5CQUPhEksbdN/tHRUVJZGSkfPjhh27PufwvjMuflEePHpV69eoV5vn5+Vd8Av/a5e+xOHjwoOPriiMyMlI2b94sxpgiH9ePP/4o+fn5EhUVVWrvhaqBGbrE1zOUnp4uaWlpkpaWJs8++6xP3hOlj/m5xFfzs2PHDmnRosUV30v329/+VkREdu3aZXWxqxJfivVWSkqKnDx5Ui5evCjt27e/4lfz5s1FRAqfBsrIyChy/qJFiyQ/P9/xPZo1ayZxcXEyd+7cK54W+qXAwEAREY/+BdOlSxf56aef5N133y1yfMGCBYU54AvMkPfGjx8vaWlp8txzz8nYsWPL/P1Q8TA/3qlbt67897//lZ9++qnI8Y0bN4pI6d6drIiqxB07b913332SkZEh3bt3l2HDhsnvfvc7CQgIkIMHD8qqVaukV69e0rt3b0lISJD+/fvLtGnTJCAgQJKTk2XXrl0yZcqUK26tuzNjxgzp2bOnJCYmyvDhw6Vhw4byww8/yIoVKwoHtWXLliIi8pe//EVSU1MlICBAmjdvXuT7Ei4bMGCAzJgxQ1JTU2Xfvn3SsmVLWb9+vUyYMEG6d+8uycnJXv15+Pv7S1JSkkff47B8+XI5e/Zs4RNZX375pbz99tsiItK9e/cy2Z8JFQ8zVJSnMzR16lR54YUX5Pbbb5cePXrIpk2biuSlvQ8ZKibmpyhP5+epp56SO++8U2677TYZPny4REVFyaZNm2TixInSokUL6datm1fvX2mU88MbZeryE0lbtmxxfF1qaqoJCQlxm+Xl5ZkpU6aY1q1bm6CgIBMaGmri4+PN4MGDze7duwtfl5uba0aMGGFiYmJMUFCQSUxMNBs3bjSNGjW66hNJxhizceNG061bNxMREWECAwNNXFzcFU84PfPMM6Zu3brGz8+vyDV+/USSMcacPHnSPProo6ZOnTrG39/fNGrUyDzzzDPm/PnzRV4nImbo0KFXfNy/Xvfl1/76fTSNGjUq8jTSL3+5e6oKFRMzVD4zlJSUpM6P5X9tW4X5Kb//B33yySema9euJjY21tSoUcM0a9bMjBgxwpw4ccKj8yszlzHG+KZCAgAAoCzxPXYAAACWoNgBAABYgmIHAABgCYodAACAJSh2AAAAlqDYAQAAWMKjDYoLCgrk8OHDEhYW5vbHngC2M8ZIdna21K1bV/z8ivfvIeYHYIaAkijO/HhU7A4fPiwNGjQolcUBldmBAweK/eNomB/gZ8wQ4D1P5sejfza5+5EhQFXkzSwwP8DPmCHAe57MgkfFjlvfwCXezALzA/yMGQK858ks8PAEAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCYodAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCf/yXgAqpzZt2qjZ+PHj3R5PSUlRz4mMjFSzU6dOebwu+F7NmjXV7KGHHlKz6dOnq1l0dLSajR071qv389aePXvUrEuXLmp25MgRNcvLyyvRmgBAwx07AAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACzBdidQBQQEqNmIESPUrEePHm6PX7hwQT3HGOP5wuBzLpdLzV5++WU1e/DBB9Vs8uTJJVqTO06fRxcvXlSznJwcNatbt66aOW2FsmTJEjXr27evmhUUFKgZ7BQYGKhm1atXV7PatWurWWpqqldrcdoyKCYmxqtrOlm5cqWa3XXXXWp29uzZUl+LLbhjBwAAYAmKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAl2O6kinN6lP6vf/2rmvXr10/NcnNz3R6/99571XNOnz6tZih/9913n5o5bWlSFrKzs9UsLy9PzSZNmqRmTlu2NG7cWM2uv/56NRs7dqxX5/3nP/9RM1RejRo1UjOnrXFatWpVFsvxSllsS5WcnKxmR44cUbPevXur2ccff1yiNVV23LEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsARPxVZxTk873n///V5dc/bs2W6P/+tf//LqevCdoKAgt8effvppr673ww8/qNmGDRvUbN26dWq2fPlyNdu3b59H6yoOp2s6ZaGhoWr23HPPqVnfvn3dHr948aJ6Diq+7777Ts3K4mlTGwQHB6vZnDlz1Kx169Zuj//0008lXlNlwB07AAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACzBdidVQIcOHdRs6tSpXl3z7NmzajZlyhSvronyp22p8eqrr3p1vfXr16vZN99849U1bXD33Xer2ciRI90ed9o6BlVTTk6Omnk7X07bEKWkpKhZw4YNvXo/b8XGxqpZYmKi2+MrV65Uz2nWrJmatWvXTs3uvPNONfvwww/VbN68eWpWUtyxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASbHdiiZCQEDV77bXX1CwyMlLNsrOz1axfv35qdujQITVDxZaXl+f2+N/+9jcfr6TqqlWrltvjbHeCXxs/fryaTZ48udTfz+ma119/vZrde++9apaamurVWt5//30127Nnj9vjffv2Vc9x2tLpmmuu8Xxhv9C+fXs1W7hwoZqdP3/eq/e7jDt2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFiC7U4qEactTWbNmqVmrVq1UrOsrCw1GzBggJotXbpUzQB4b+jQoW6PP/zwwz5eCSq6m266yafvd/DgQa+yPn36lPpa7rnnHjXT/lzq169f6utw0qhRIzXr1q2bmi1ZsqRE78sdOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAswXYnlUhKSoqa9e/f36trZmRkqNn777/v1TUBiMTGxnp13qJFi0p5JagI5s6dq2aDBg3y6prJyclqNmbMGDX79NNP1Wz9+vVeraVv375qlpqaqmZ5eXlq9sUXX6hZu3bt1MzX25p445///Kea+fuXrJpxxw4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEhQ7AAAAS7iMMeZqL8rKypKIiAhfrKfKS09PV7MhQ4aoWWRkpJq9++67aub0mH1mZqaaVVWZmZkSHh5erHOYH3vVqlVLzbZs2aJm1157rZo1btzY7fEffvjB43VVZFV1hho0aKBm33//vQ9XInLu3Dk1e/jhh9XsrbfeUrOdO3eqWUJCgpq5XC4186CelJrp06er2YwZM9Rs8ODBalazZk2v1uL038CT+eGOHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWYLuTcpCcnKxmCxcuVDOnLU1Onz6tZrfccouaffnll2qGK1XVrRrg3lNPPaVmr7zyipodO3ZMzdq0aVPscyqTqjpDgYGBaua01cYDDzxQFstR5eTkqFlWVpaaRUdHq5mfn34PyZfbnWzevFnNOnXqpGYXLlwo1XWUBNudAAAAVCEUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABL+Jf3AmzVpEkTNSuLLU0GDBigZmxpAnjPaRuHoUOHenXN2bNnq5kt25qgqNzcXDUbMWKEmrVv317NWrVqVaI1uVOjRg2vsopk1qxZbo+PHj1aPacibWlSUtyxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASbHdSAm3atFGzMWPGqJnTliZbtmxRs3HjxqnZsmXL1AwoqerVq6tZ79691Wz48OFevd+6devU7LrrrlOz119/Xc369++vZtdee62a1axZU83i4uLUzMnhw4fVrEePHl5d0xt79uxRs2+++cZn66jqsrOz1axt27Zq5rTdzquvvlqiNfmKn59+f+nEiRNq9sEHH6jZE088UaI1VXbcsQMAALAExQ4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEi5jjLnai7KysiQiIsIX66lwgoOD1czpcetbb71Vzc6cOaNmt912m5pt27ZNzeAbmZmZEh4eXqxzKtP8aFsrzJs3Tz2nZcuWZbUclLEXX3xRzV544YUyeU/bZ8iXOnTooGarV6/23UJKwOVyqdmwYcPU7LXXXiuL5VR4nswPd+wAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsIR/eS+gIqhRo4aaOW3z4LSlSWZmppr169dPzdjSBGWtWrVqajZ27Fi3x73d0uTs2bNqtnTpUq+u6a2OHTuqWe3atUv9/X744Qc1q1WrlpqFhoaW+lpQecXHx6vZ3//+dx+uxPf69u2rZjt27FCz9evXl8FqKg/u2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCbY7EedtS+655x6vrvnBBx+o2fLly726JlAannzySTXr2bOn2+O5ubnqOYMGDVKzTZs2qdm+ffvUrCzUq1dPzZYsWaJmderUUTOn7Rj27NmjZoGBgWo2YsQIt8cjIiLUc5z+nKdMmaJmt9xyi5qhYhg1apSaNWrUqNTfb8yYMWq2f/9+NXviiSfU7KabblIzp+2XbrzxRjUbOXKkmrHdCQAAAKxAsQMAALAExQ4AAMASFDsAAABLUOwAAAAsQbEDAACwRJXZ7iQoKEjNnn76aa+u6bRFwiOPPOLVNYGy1rlzZzXLy8tze/wPf/iDeo7THJSFqKgoNRs6dKiaOW3Lkp+fr2baFjAiIjt27FAzbzltR+ONpUuXqll2dnapvhd0Ttt63HbbbWqWnJxc6mtxmtm//vWvapaVlaVmb7/9tprNmTNHzR588EE1cxIfH+/VeVUBd+wAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsESV2e7Ez0/vsKGhoV5ds3Hjxmp29uxZr64JlLWUlBQ1O3/+vNvj3333nVfvFRYWpmZ33HGHmrlcLjXr16+fmrVp00bN3nnnHTVz2uKhLLY08aUDBw6U9xIgIsHBwWrmtCWNE6ftR9566y01GzNmjFfXrF+/vpo5bWnSvHlzNfPWm2++WerXtAV37AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAsUWWeiq1Ro4aatWvXzqtrevs0LVBRBQYGuj2+fv169RynH1rv9HT43//+dzV7//331ezVV19Vs2PHjqnZ9u3b1QyojAICAtRs3759ajZkyBA1u+eee9SsVq1aatagQQM1c5Kfn69mTk+yO/2dVNVxxw4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEhQ7AAAAS7iMMeZqL8rKypKIiAhfrKfMREZGqtnx48e9uqbTH925c+fUbNasWWo2evRor9YC38jMzJTw8PBinVPR5mfDhg1qlpiYWOzrXbhwQc38/PR/O/r767st3XfffWq2aNEizxaGCsmGGfJGWFiYmp0+fdqHK6lYBg4cqGZvvvmm7xZSSXgyP9yxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMAS+n4Dljl16pSaRUVFqdnKlSvVrGHDhmqWnp6uZjNmzFAzoKzdcsstanbrrbe6PR4SEqKec+LECTXbtm2b5wv7hYsXL3p1HlBROW0L9J///EfNWrVqVRbLKXVffPGFmk2aNEnN2L6o9HHHDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABLuIwx5movysrKkoiICF+sB6jQMjMzJTw8vFjnMD/Az5ihK6WkpKhZt27d1Gzw4MFevd/evXvVzGn7kZ07d6rZe++9p2a5ubmeLQxX5cn8cMcOAADAEhQ7AAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEuw3QlQDGzVAJQMMwR4j+1OAAAAqhCKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCYodAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACW8KjYGWPKeh1ApeDNLDA/wM+YIcB7nsyCR8UuOzu7xIsBbODNLDA/wM+YIcB7nsyCy3hQ/woKCuTw4cMSFhYmLperVBYHVCbGGMnOzpa6deuKn1/xvoOB+QGYIaAkijM/HhU7AAAAVHw8PAEAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCYodAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJawutjNnz9fXC5X4S9/f3+pX7++DBo0SA4dOuSTNTRu3FgGDhxY+PvVq1eLy+WS1atXF+s6GzZskLS0NDlz5swVWceOHaVjx44lWmdpuvwxuvu1adOm8l4eioEZKh+ffPKJPPDAAxIfHy8hISFSr1496dWrl2zbtq28l4ZiYH7K1/r166V79+5yzTXXSI0aNaRp06Yyfvz48l5WmfMv7wX4wrx58yQ+Pl5ycnJk7dq1MnHiRFmzZo3s3LlTQkJCfLqWtm3bysaNG6VFixbFOm/Dhg2Snp4uAwcOlJo1axbJZs6cWYorLD0TJkyQTp06FTl2/fXXl9NqUBLMkG/NmjVLTp48KcOGDZMWLVrI8ePHZerUqZKYmCgrVqyQzp07l/cSUQzMj+/94x//kPvvv1/uvfdeWbBggYSGhsrevXvl8OHD5b20Mlclit31118v7du3FxGRTp06ycWLF2X8+PHy7rvvSr9+/dyec+7cOQkODi71tYSHh0tiYmKpXrO4A+orTZs2LfWPFeWDGfKtGTNmSExMTJFjt99+u1x33XUyYcIEil0lw/z41qFDh+SRRx6RwYMHFymdv77RYCurvxSrufxJvX//fhERGThwoISGhsrOnTula9euEhYWJl26dBERkQsXLsiLL74o8fHxEhgYKNHR0TJo0CA5fvx4kWvm5eXJ6NGjJTY2VoKDg6VDhw7y2WefXfHe2m3wzZs3S8+ePSUyMlKCgoIkLi5OnnrqKRERSUtLk1GjRomIyLXXXlt4W//yNdzdBj916pQMGTJE6tWrJ9WrV5cmTZrIn/70J8nNzS3yOpfLJY8//ri88cYbkpCQIMHBwdK6dWtZunRpsf9cUXUwQz8rixn6dakTEQkNDZUWLVrIgQMHvL4uKgbm52dlMT+vv/66nD17VsaMGeP1NSqzKnHH7tf27NkjIiLR0dGFxy5cuCB33HGHDB48WJ5++mnJz8+XgoIC6dWrl6xbt05Gjx4tN998s+zfv1/Gjh0rHTt2lK1bt0qNGjVEROThhx+WBQsWyMiRI+W2226TXbt2yV133SXZ2dlXXc+KFSukZ8+ekpCQIK+88oo0bNhQ9u3bJx999JGIiDz00ENy6tQpmT59urzzzjtSp04dEdH/lXT+/Hnp1KmT7N27V9LT06VVq1aybt06mThxouzYsUOWLVtW5PXLli2TLVu2yLhx4yQ0NFQmT54svXv3lm+++UaaNGlS+DqXyyVJSUkef2/G0KFD5b777pPg4GC56aab5Pnnn5cOHTp4dC4qNmbINzP0S5mZmbJ9+3bu1lmA+Snb+Vm7dq3UqlVLvv76a+nVq5fs2rVLatWqJXfddZdMnjxZwsPDr/pnUqkZi82bN8+IiNm0aZPJy8sz2dnZZunSpSY6OtqEhYWZo0ePGmOMSU1NNSJi5s6dW+T8hQsXGhExixcvLnJ8y5YtRkTMzJkzjTHGfPXVV0ZEzPDhw4u8LiMjw4iISU1NLTy2atUqIyJm1apVhcfi4uJMXFycycnJUT+Wl19+2YiI+f7776/IkpKSTFJSUuHvZ8+ebUTELFq0qMjrJk2aZETEfPTRR4XHRMTUrl3bZGVlFR47evSo8fPzMxMnTixyfrVq1Uznzp3VNV62fft2M2zYMLNkyRKzdu1aM3fuXJOQkGCqVatmPvzww6uej4qDGSqfGXKnX79+xt/f32zdutWr8+F7zE/5zE/z5s1NUFCQCQsLMxMmTDCrVq0ykydPNjVq1DC///3vTUFBwVWvUZlViS/FJiYmSkBAgISFhUlKSorExsbK8uXLpXbt2kVe16dPnyK/X7p0qdSsWVN69uwp+fn5hb/atGkjsbGxhf9qWLVqlYjIFd8rce+994q/v/NN0W+//Vb27t0rDz74oAQFBZXwI73kk08+kZCQELn77ruLHL/8ZNTHH39c5HinTp0kLCys8Pe1a9eWmJiYwi8TXJafn3/Fue7ccMMNMm3aNLnzzjvllltukUGDBsmGDRukTp06Mnr0aC8/KpQnZugSX83Qrz3//POSkZEhf/7zn6Vdu3bFPh/li/m5xFfzU1BQIOfPn5dnn31WnnnmGenYsaOMGjVKJk6cKJ9++qlXM1iZVIkvxS5YsEASEhLE399fateuXXgb+ZeCg4OvuD177NgxOXPmjFSvXt3tdU+cOCEiIidPnhQRkdjY2CK5v7+/REZGOq7t8vdJ1K9f37MPxgMnT56U2NhYcblcRY7HxMSIv79/4Xovc7fGwMBAycnJKbU11axZU1JSUmT27NmSk5NT+OUDVA7M0CXlMUPp6eny4osvyksvvSSPP/54ia8H32N+LvHV/ERGRsru3bvlf/7nf4oc79atmzz11FOyfft2SU5O9uralUGVKHYJCQmFTyRpfv0JKCISFRUlkZGR8uGHH7o95/K/MC5/Uh49elTq1atXmOfn51/xCfxrl7/H4uDBg46vK47IyEjZvHmzGGOKfFw//vij5OfnS1RUVKm9V3EYY0TE/Z81KjZm6BJfz1B6erqkpaVJWlqaPPvssz55T5Q+5ucSX81Pq1at3O6Zevn/QX5+dn+x0u6ProRSUlLk5MmTcvHiRWnfvv0Vv5o3by4iUvg0UEZGRpHzFy1aJPn5+Y7v0axZM4mLi5O5c+de8bTQLwUGBoqIePQvmC5dushPP/0k7777bpHjCxYsKMx97fTp07J06VJp06ZNqd3uR8XHDHlv/PjxkpaWJs8995yMHTu2zN8PFQ/z453LX9Jevnx5keMffPCBiIj123BViTt23rrvvvskIyNDunfvLsOGDZPf/e53EhAQIAcPHpRVq1ZJr169pHfv3pKQkCD9+/eXadOmSUBAgCQnJ8uuXbtkypQpHj19M2PGDOnZs6ckJibK8OHDpWHDhvLDDz/IihUrCge1ZcuWIiLyl7/8RVJTUyUgIECaN29e5PsSLhswYIDMmDFDUlNTZd++fdKyZUtZv369TJgwQbp37+71LWh/f39JSkq66vcn/OEPf5CGDRtK+/btJSoqSnbv3i1Tp06VY8eOyfz58716b1ROzFBRns7Q1KlT5YUXXpDbb79devToccXdB9v/x4RLmJ+iPJ2frl27Ss+ePWXcuHFSUFAgiYmJsnXrVklPT5eUlBT7d2co10c3ytjlJ5K2bNni+LrU1FQTEhLiNsvLyzNTpkwxrVu3NkFBQSY0NNTEx8ebwYMHm927dxe+Ljc314wYMcLExMSYoKAgk5iYaDZu3GgaNWp01SeSjDFm48aNplu3biYiIsIEBgaauLi4K55weuaZZ0zdunWNn59fkWv8+okkY4w5efKkefTRR02dOnWMv7+/adSokXnmmWfM+fPni7xORMzQoUOv+Lh/ve7Lr/31+7gzceJE06ZNGxMREWGqVatmoqOjTe/evc1nn3121XNRsTBD5TNDSUlJRkTUX6gcmJ/ymR9jjDl37pwZM2aMadCggfH39zcNGzZ0+/42chnzf190BgAAQKXG99gBAABYgmIHAABgCYodAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmPNiguKCiQw4cPS1hYGD8OClWSMUays7Olbt26xf5xNMwPwAwBJVGc+fGo2B0+fFgaNGhQKosDKrMDBw4U+4dlMz/Az5ghwHuezI9H/2xy9yNDgKrIm1lgfoCfMUOA9zyZBY+KHbe+gUu8mQXmB/gZMwR4z5NZ4OEJAAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAs4V/eCwAAACgtqampbo/PnDlTPWf//v1q1qVLFzU7cuSI5wvzEe7YAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJtjsBAAAVTkBAgJr16NFDzebNm+f2+K5du7y6XkXc0sQJd+wAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsATbnQCoEgIDA9WsevXqata7d281a9q0qVdr+fbbb9UsIyPD7fGCggKv3guorJKTk9Vs8eLFapabm+v2+F133aWec+DAAc8XVsFxxw4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEhQ7AAAAS7DdSQk8+eSTahYQEKBmU6dOLYvlANYICQlRs/DwcDVLSkpSs9GjR6tZq1atPFuYD3Tu3Nnt8WeffVY958iRI2W1HKBMOW1psnDhQq+uef/997s9vmfPHq+uV9lwxw4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEhQ7AAAAS7DdyVXUqFFDzYYOHapmc+bMKYvlANZw2rZk8eLFatapUyc1c7lcamaMUbO8vDw1279/v5oFBQWpWf369dXMibZVQ/v27dVzunbtqmZsheKdyMhINWvXrp2a7du3z+3xb7/9tqRLqrSc/iz/93//V80CAwPVzOn/sW+//bZnC7MUd+wAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALMFTsVcxcuRINWvatKkPVwLYZciQIWrm9OSrk8OHD6vZG2+8oWZr165Vsw8//FDNoqKi1Kxjx45qNnz4cDW78cYb3R5PSEhQz0lPT1ezRx55RM2gCwsLU7OWLVuqWW5urtvjtj8Ve80116jZihUr1OyGG25Qs4yMDDV77LHHPFtYFcQdOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAs4TJOPxn7/2RlZUlERIQv1lMu6tatq2abNm1SM6cf8u30g8N9/UO5V69erWbVq1dXs5kzZ6rZhQsX1OzgwYMerasyyszMdPzh9e7YPj/eWrlypZo5bRVy4MABNUtOTlazvXv3erQuX3DazuX99993e7xGjRpevZe/f8Xa1YoZqrxCQkLUzGnLnT/+8Y9qtmPHDjX7/e9/r2Y5OTlqZjNP5oc7dgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgu1ORGT48OFqNnXqVB+upPI4fvy4mvXp08ft8fXr15fVcnyGrRpKT+3atdWsadOmaua03YnTNkOVxTfffOP2eFxcnFfXY7sTlJYZM2ao2aOPPqpm27dvV7O7775bzWyY59LGdicAAABVCMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsATFDgAAwBIV6zn4cnLPPfeU+jUPHTqkZjk5OWp23XXXqdmZM2fU7Pvvv/doXb7QvHlzt8dt2O4EpefYsWNeZQBKJjIyUs1WrFihZm3btlWzTz/9VM1SU1PVjC1NSh937AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAsQbEDAACwBNudlJFp06ap2T//+U81c9p6Zffu3Wr2/vvve7QuAOWvcePGahYWFlbs682aNasEq0FV47SlyQ033KBmTluTPPzww2r23XffebYwlAru2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCbY7KSNjx45Vs5EjR5b6+yUkJKjZ1q1bS/39PvvsMzXLzs4u9fdD5dS3b18169y5s5rFxMSo2cqVK9Vs+vTpni2snD366KNq5vSxaz7++OOSLAeVVLVq1dRszpw5ata2bVs1c9rSpEePHmr29ddfqxl8izt2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFiC7U7KSGhoqFeZtyZOnFjq13Qye/ZsNRsyZIgPV4LylpaWpmbPPvusmvn5effvSqctFwIDA9VsypQpXr2ft7p27apmf/zjH4t9vVmzZqnZe++9V+zrofJz2tJk4MCBXl3T6XPzyy+/9Oqa8C3u2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWoNgBAABYgmIHAABgCZcxxlztRVlZWRIREeGL9ZSLZs2aqdnXX3/tw5WIbNu2Tc32799f6u936623qllUVJSaOX3a5OXluT3evHlz9Zyy+NjKQmZmpoSHhxfrnMo0P9WqVXN7/Pnnn1fPcdrSZMaMGWr2448/qlm7du3UrHfv3mrmtIXKk08+qWbTp09XMyfan5eIyNq1a9UsMTFRzbS/Azp16qSec/bsWTWraGyfodI2btw4NXvuuee8uqbTVjwrV6706ppOIiMj1cxp1oOCgtSsT58+apaRkeH2+Jo1a9RzcnNz1awi8WR+uGMHAABgCYodAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCX8y3sBFcGhQ4fUzOmR6rLw+eefq9m+fftK/f06dOigZtHR0Wo2depUNWvcuLHb4y6Xy+N1oez4++tjr21dMnr0aPWcYcOGqdmsWbM8X5iH3n77bTVz2grFaYuHv/3tb2p27tw5NXvhhRfU7MYbb/TqmpMnT3Z7vDJtaYLicdpya/jw4WrmtO2U0zZE3m5pEhMTo2b9+vVTM6dtWWrWrOnVWpzcf//9bo/36NFDPWf58uWlvo7ywh07AAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACzBdifivI3AkiVLfLgS31u/fr1X5z3//PPFPqdjx45qNn/+fK/WAfe82dJERGTMmDFuj48aNUo9pyy2NHEybtw4NXPa7qRbt25q9sQTT6iZ05YSAwcOVDOnLU2ctldx2s4FdpoxY4aaBQcHq9lHH32kZjNnzvRqLWFhYWr28ssvq1n//v3VbMeOHWrm9Pm+bNkyNWvbtq2azZ071+3xJk2aqOfYhDt2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJag2AEAAFjCZZye5f8/WVlZEhER4Yv1oAIJDQ1Vs40bN6rZb37zm2Kf8/vf/97zhZWjzMxMCQ8PL9Y55TE/jz32mJpNmTJFzbTtTl577bUSr8kXDh8+rGYxMTE+XInzFj4PPfSQ7xZSwVSWGSptycnJaua0bcmpU6fU7IYbblCzAwcOqFmHDh3UbM6cOWrWoEEDNZs6daqapaWlqZmTGjVqqNmmTZvUrGXLlm6P16lTRz3n2LFjni+sHHkyP9yxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMAS/uW9AJQvpy1NFi5cqGbaliZOli5dWuxzoHPaCmDUqFFqtmHDBjWrLNuaaP7973+rWb9+/Ur9/f773/+qmdN/A1Q9Tp8PLpdLzZ544gk1c9rS5J577lGzt956S802b96sZk7bUp0+fVrNnDRs2FDNFi9erGbaliYiIo8++qjb45VlS5OS4o4dAACAJSh2AAAAlqDYAQAAWIJiBwAAYAmKHQAAgCUodgAAAJZgu5Mqbvr06WrWo0cPNXN6PN8Y4/a40/YpKL5x48apmdMWAk7n+VJSUpKa3XrrrWqWnJysZh06dFAz7fOyrDjNCKoep88/p+zzzz9Xs7CwMDWbNGmSmn377bdqdvfdd6tZs2bN1KxPnz5qVqdOHTW7/fbb1eynn37y6v2WLFmiZlUBd+wAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsATbnVgiICBAzV544QU169ixo1fv5/R4/sqVK90eP3PmjFfvBfe6d+/u1Xnt2rVTs7Nnzxb7ev3791ez+vXrq1lCQoKaVa9evdjrEBHZtWuXmn355Zdq9tvf/lbNGjdurGa/+c1v1GzDhg1q9tprr6nZsWPH3B7fvXu3es6OHTvUDBXD9u3b1axr165q9u9//1vNnP5ObdSokZo5bSPy8ccfq1nTpk3VzInT5+crr7yiZvPnz1ezI0eOeLWWqoA7dgAAAJag2AEAAFiCYgcAAGAJih0AAIAlKHYAAACWcBkPfjJ2VlaWRERE+GI9cJCYmKhmI0aMUDOnH5bsreXLl6tZv3793B634anYzMxMCQ8PL9Y5ZTU/DRo0ULPvv/++1N/Pl0aNGqVm//rXv9Ts5MmTanb69Gk1i4mJUbMBAwao2RNPPKFm9erVUzNvZGZmqpnT58K5c+dKdR0lVZFmqKJIS0tTM6e/v1u0aOHV+7355ptq5lQJli1bpmZOf787PdmL4vFkfrhjBwAAYAmKHQAAgCUodgAAAJag2AEAAFiCYgcAAGAJih0AAIAl2O7kKrp166ZmKSkpavbOO++oWXJyspo5PcY8ePBgNfPzK/2O7vTDoPv27atmJ06cKPW1VBQVaasGp//m2pYzIiKPPPKImm3durXY63DaWuWtt94q9vVEnD+HLl686NU1y0KrVq3ULD09Xc169uxZ7Pf64osv1Ozmm29Ws9zc3GK/V1mqSDMEVDZsdwIAAFCFUOwAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALMF2J1fx/PPPq5nTdgbnz59Xs8DAQDVzuVyeLexXtm/frmZHjhxRs5deeknNnLZXyMnJ8WxhlrFhq4bq1aur2YULF3y4Erv5+/urmdPfAZr8/Hw1q2hbmjixYYaA8sJ2JwAAAFUIxQ4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEvrz+BARkSVLlqiZ0/YDTtuIOJkzZ46arVmzRs3Wr1+vZgcOHPBqLbATW5r4htPfD04ZAJQEd+wAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsITLGGOu9qKsrCyJiIjwxXqACi0zM1PCw8OLdQ7zA/yMGQK858n8cMcOAADAEhQ7AAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAsQbEDAACwBMUOAADAEhQ7AAAAS1DsAAAALEGxAwAAsATFDgAAwBIUOwAAAEtQ7AAAACzhUbEzxpT1OoBKwZtZYH6AnzFDgPc8mQWPil12dnaJFwPYwJtZYH6AnzFDgPc8mQWX8aD+FRQUyOHDhyUsLExcLlepLA6oTIwxkp2dLXXr1hU/v+J9BwPzAzBDQEkUZ348KnYAAACo+Hh4AgAAwBIUOwAAAEtQ7AAAACxBsQMAALAExQ4AAMASFDsAAABLUOwAAAAs8f8BQtIZ0I9aBVUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "  ex = example_data.to(device)\n",
    "  output = clf(ex)\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Prediction: {}\".format(\n",
    "    output.data.max(1, keepdim=True)[1][i].item()))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
