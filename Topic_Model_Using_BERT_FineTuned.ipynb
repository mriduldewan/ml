{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICTIgtJ5jH3c",
        "outputId": "125f59f6-e5b2-4190-f4fc-dfe24fa79474"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "from torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report \n",
        "\n",
        "# Choose the appropriate device based on availability (CUDA or CPU)\n",
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # For Windows\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # For M1 Mac\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "faLZtUDGjGJU"
      },
      "outputs": [],
      "source": [
        "# Define helper functions\n",
        "def prepare_data(text, tokenizer, max_len=512):\n",
        "  encoded_text = tokenizer(text,\n",
        "                           padding=\"max_length\",\n",
        "                           truncation=True,\n",
        "                           max_length=max_len)\n",
        "  return encoded_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "VZ5UUQ6HYmP8",
        "outputId": "ff960ab7-a4b8-4382-ae86-7d9881bb3cb7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>filename</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>tech</td>\n",
              "      <td>168.txt</td>\n",
              "      <td>A decade of good website design</td>\n",
              "      <td>The web looks very different today than it di...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>577</th>\n",
              "      <td>entertainment</td>\n",
              "      <td>068.txt</td>\n",
              "      <td>Tautou 'to star in Da Vinci film'</td>\n",
              "      <td>French actress Audrey Tautou, star of hit fil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1118</th>\n",
              "      <td>politics</td>\n",
              "      <td>223.txt</td>\n",
              "      <td>Kennedy questions trust of Blair</td>\n",
              "      <td>Lib Dem leader Charles Kennedy has said voter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1797</th>\n",
              "      <td>sport</td>\n",
              "      <td>485.txt</td>\n",
              "      <td>Koubek suspended after drugs test</td>\n",
              "      <td>Stefan Koubek says he has been banned for thr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1144</th>\n",
              "      <td>politics</td>\n",
              "      <td>249.txt</td>\n",
              "      <td>No election TV debate, says Blair</td>\n",
              "      <td>Tony Blair has said he will not take part in ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           category filename                              title  \\\n",
              "1991           tech  168.txt    A decade of good website design   \n",
              "577   entertainment  068.txt  Tautou 'to star in Da Vinci film'   \n",
              "1118       politics  223.txt   Kennedy questions trust of Blair   \n",
              "1797          sport  485.txt  Koubek suspended after drugs test   \n",
              "1144       politics  249.txt  No election TV debate, says Blair   \n",
              "\n",
              "                                                content  \n",
              "1991   The web looks very different today than it di...  \n",
              "577    French actress Audrey Tautou, star of hit fil...  \n",
              "1118   Lib Dem leader Charles Kennedy has said voter...  \n",
              "1797   Stefan Koubek says he has been banned for thr...  \n",
              "1144   Tony Blair has said he will not take part in ...  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Import input csv\n",
        "in_filepath = os.getcwd() + \"/data/in/Topic Model/\"\n",
        "df = pd.read_csv(in_filepath+'bbc-news-data.csv', sep='\\t', encoding='ISO-8859-1')\n",
        "\n",
        "# Shuffle the DataFrame rows\n",
        "df = df.sample(frac=1)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "category\n",
              "sport            511\n",
              "business         510\n",
              "politics         417\n",
              "tech             401\n",
              "entertainment    386\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Balancing classes\n",
        "df['category'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsdDpHZ0JV8-",
        "outputId": "c4b08a16-6661-47c0-ff57-db5e4699e70e"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Load pre-trained BERT model and tokenizer\u001b[39;00m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m, do_lower_case\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m BertForSequenceClassification\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m, num_labels\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(label_encoder\u001b[39m.\u001b[39mclasses_))\n\u001b[1;32m      8\u001b[0m model\u001b[39m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:2449\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2448\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 2449\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mconfig_class\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m   2450\u001b[0m         config_path,\n\u001b[1;32m   2451\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m   2452\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   2453\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m   2454\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[1;32m   2455\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m   2456\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m   2457\u001b[0m         token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m   2458\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m   2459\u001b[0m         subfolder\u001b[39m=\u001b[39msubfolder,\n\u001b[1;32m   2460\u001b[0m         _from_auto\u001b[39m=\u001b[39mfrom_auto_class,\n\u001b[1;32m   2461\u001b[0m         _from_pipeline\u001b[39m=\u001b[39mfrom_pipeline,\n\u001b[1;32m   2462\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2463\u001b[0m     )\n\u001b[1;32m   2464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2465\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:591\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m    587\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrevision\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m revision\n\u001b[1;32m    589\u001b[0m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_set_token_in_kwargs(kwargs, token)\n\u001b[0;32m--> 591\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    592\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    593\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    594\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    595\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    596\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:620\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    619\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    621\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    622\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/configuration_utils.py:675\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    673\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    674\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    676\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    677\u001b[0m         configuration_file,\n\u001b[1;32m    678\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m    679\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m    680\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m    681\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[1;32m    682\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    683\u001b[0m         token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m    684\u001b[0m         user_agent\u001b[39m=\u001b[39muser_agent,\n\u001b[1;32m    685\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    686\u001b[0m         subfolder\u001b[39m=\u001b[39msubfolder,\n\u001b[1;32m    687\u001b[0m         _commit_hash\u001b[39m=\u001b[39mcommit_hash,\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    689\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    691\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    692\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/hub.py:428\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m user_agent \u001b[39m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    426\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    427\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    429\u001b[0m         path_or_repo_id,\n\u001b[1;32m    430\u001b[0m         filename,\n\u001b[1;32m    431\u001b[0m         subfolder\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(subfolder) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m subfolder,\n\u001b[1;32m    432\u001b[0m         repo_type\u001b[39m=\u001b[39mrepo_type,\n\u001b[1;32m    433\u001b[0m         revision\u001b[39m=\u001b[39mrevision,\n\u001b[1;32m    434\u001b[0m         cache_dir\u001b[39m=\u001b[39mcache_dir,\n\u001b[1;32m    435\u001b[0m         user_agent\u001b[39m=\u001b[39muser_agent,\n\u001b[1;32m    436\u001b[0m         force_download\u001b[39m=\u001b[39mforce_download,\n\u001b[1;32m    437\u001b[0m         proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m    438\u001b[0m         resume_download\u001b[39m=\u001b[39mresume_download,\n\u001b[1;32m    439\u001b[0m         token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m    440\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[39mexcept\u001b[39;00m GatedRepoError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    443\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to access a gated repo.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mMake sure to request access at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m and pass a token having permission to this repo either \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1195\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1194\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m         metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1196\u001b[0m             url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   1197\u001b[0m             token\u001b[39m=\u001b[39mtoken,\n\u001b[1;32m   1198\u001b[0m             proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m   1199\u001b[0m             timeout\u001b[39m=\u001b[39metag_timeout,\n\u001b[1;32m   1200\u001b[0m         )\n\u001b[1;32m   1201\u001b[0m     \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1202\u001b[0m         \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m         commit_hash \u001b[39m=\u001b[39m http_error\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39mheaders\u001b[39m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1532\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1529\u001b[0m headers[\u001b[39m\"\u001b[39m\u001b[39mAccept-Encoding\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[39m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1532\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1533\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1534\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   1535\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m   1536\u001b[0m     allow_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1537\u001b[0m     follow_relative_redirects\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1538\u001b[0m     proxies\u001b[39m=\u001b[39mproxies,\n\u001b[1;32m   1539\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1540\u001b[0m )\n\u001b[1;32m   1541\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1543\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:407\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m# 2. Force relative redirection\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 407\u001b[0m     response \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m    408\u001b[0m         method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    409\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    410\u001b[0m         max_retries\u001b[39m=\u001b[39mmax_retries,\n\u001b[1;32m    411\u001b[0m         base_wait_time\u001b[39m=\u001b[39mbase_wait_time,\n\u001b[1;32m    412\u001b[0m         max_wait_time\u001b[39m=\u001b[39mmax_wait_time,\n\u001b[1;32m    413\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    414\u001b[0m         follow_relative_redirects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    415\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    418\u001b[0m     \u001b[39m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    419\u001b[0m     \u001b[39m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    420\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m300\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus_code \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m399\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:442\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n\u001b[1;32m    441\u001b[0m \u001b[39m# 3. Exponential backoff\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m \u001b[39mreturn\u001b[39;00m http_backoff(\n\u001b[1;32m    443\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[1;32m    444\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    445\u001b[0m     max_retries\u001b[39m=\u001b[39mmax_retries,\n\u001b[1;32m    446\u001b[0m     base_wait_time\u001b[39m=\u001b[39mbase_wait_time,\n\u001b[1;32m    447\u001b[0m     max_wait_time\u001b[39m=\u001b[39mmax_wait_time,\n\u001b[1;32m    448\u001b[0m     retry_on_exceptions\u001b[39m=\u001b[39m(ConnectTimeout, ProxyError),\n\u001b[1;32m    449\u001b[0m     retry_on_status_codes\u001b[39m=\u001b[39m(),\n\u001b[1;32m    450\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    451\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    452\u001b[0m )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:212\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mseek(io_obj_initial_pos)\n\u001b[1;32m    211\u001b[0m \u001b[39m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m response \u001b[39m=\u001b[39m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m retry_on_status_codes:\n\u001b[1;32m    214\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39msend(request, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
            "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1378\u001b[0m         response\u001b[39m.\u001b[39mbegin()\n\u001b[1;32m   1379\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1380\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1311\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1308\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1309\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1310\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1311\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1312\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1167\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1166\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1168\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1169\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df_labels = label_encoder.fit_transform(df['category'])\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtfDmwoUJrqk",
        "outputId": "f6318d05-c559-44e2-fb2f-c982d421be11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of training set: 1780\n",
            "Length of testing set: 445\n"
          ]
        }
      ],
      "source": [
        "# Add a new column with encoded labels\n",
        "df['target'] = df_labels\n",
        "\n",
        "# Split data into training and testing sets\n",
        "\"\"\"\n",
        "train_df, val_df = train_test_split(df, \n",
        "                                    test_size=0.2, \n",
        "                                    random_state=42, \n",
        "                                    stratify=df.target.values)\n",
        "\"\"\"\n",
        "                                    \n",
        "X_train, X_val, Y_train, Y_val = train_test_split(df.index.values,\n",
        "                                                  df.target.values,\n",
        "                                                  test_size=0.2, \n",
        "                                                  random_state=42, \n",
        "                                                  stratify=df.target.values)\n",
        "\n",
        "print(f\"Length of training set: {len(X_train)}\")\n",
        "print(f\"Length of testing set: {len(X_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>title</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>category</th>\n",
              "      <th>target</th>\n",
              "      <th>data_type</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">business</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">0</th>\n",
              "      <th>train</th>\n",
              "      <td>408</td>\n",
              "      <td>408</td>\n",
              "      <td>408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>102</td>\n",
              "      <td>102</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">entertainment</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">1</th>\n",
              "      <th>train</th>\n",
              "      <td>309</td>\n",
              "      <td>309</td>\n",
              "      <td>309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>77</td>\n",
              "      <td>77</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">politics</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">2</th>\n",
              "      <th>train</th>\n",
              "      <td>333</td>\n",
              "      <td>333</td>\n",
              "      <td>333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>84</td>\n",
              "      <td>84</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">sport</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">3</th>\n",
              "      <th>train</th>\n",
              "      <td>409</td>\n",
              "      <td>409</td>\n",
              "      <td>409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>102</td>\n",
              "      <td>102</td>\n",
              "      <td>102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th rowspan=\"2\" valign=\"top\">tech</th>\n",
              "      <th rowspan=\"2\" valign=\"top\">4</th>\n",
              "      <th>train</th>\n",
              "      <td>321</td>\n",
              "      <td>321</td>\n",
              "      <td>321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "      <td>80</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                filename  title  content\n",
              "category      target data_type                          \n",
              "business      0      train           408    408      408\n",
              "                     val             102    102      102\n",
              "entertainment 1      train           309    309      309\n",
              "                     val              77     77       77\n",
              "politics      2      train           333    333      333\n",
              "                     val              84     84       84\n",
              "sport         3      train           409    409      409\n",
              "                     val             102    102      102\n",
              "tech          4      train           321    321      321\n",
              "                     val              80     80       80"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['data_type'] = ['not_set']*df.shape[0]\n",
        "df.loc[X_train, 'data_type'] = 'train'\n",
        "df.loc[X_val, 'data_type'] = 'val'\n",
        "\n",
        "\n",
        "df.groupby(['category', 'target', 'data_type']).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "My_phDy8JxGO"
      },
      "outputs": [],
      "source": [
        "# Tokenize text and convert to input features\n",
        "train_tokens = tokenizer.batch_encode_plus(df[df.data_type==\"train\"].content.values,\n",
        "                                           max_length=256,\n",
        "                                           padding='max_length',\n",
        "                                           truncation=True,\n",
        "                                           return_tensors='pt',\n",
        "                                           add_special_tokens=True)\n",
        "test_tokens = tokenizer.batch_encode_plus(df[df.data_type==\"val\"].content.values,\n",
        "                                          max_length=256,\n",
        "                                          padding='max_length',\n",
        "                                          truncation=True,\n",
        "                                          return_tensors='pt',\n",
        "                                          add_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "5oefSBlmMLX7"
      },
      "outputs": [],
      "source": [
        "# Convert the input ids, attention mask and target labels to tensor type\n",
        "train_input_ids = train_tokens['input_ids']\n",
        "train_attention_mask = train_tokens['attention_mask']\n",
        "train_labels = torch.tensor(df[df.data_type==\"train\"].target.values)\n",
        "\n",
        "val_input_ids = test_tokens['input_ids']\n",
        "val_attention_mask = test_tokens['attention_mask']\n",
        "val_labels = torch.tensor(df[df.data_type==\"val\"].target.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufozuaCnN9zs"
      },
      "source": [
        "\n",
        "\n",
        "Construct a TensorDataset object, which is a built-in PyTorch dataset type designed to handle tensors. It takes multiple tensors as input, where each tensor represents a different feature of the data.\n",
        "\n",
        "Within this dataset, each sample will consist of three tensors:\n",
        "\n",
        "\n",
        "- train_input_ids: Tensor containing tokenized input text for training.\n",
        "\n",
        "- train_attention_mask: Tensor indicating which tokens are relevant for attention-based models.\n",
        "\n",
        "- train_labels: Tensor holding the corresponding labels for each input sample.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "8wVCM0JJNY2X"
      },
      "outputs": [],
      "source": [
        "# Set batch size and create data loaders\n",
        "batch_size = 32\n",
        "\n",
        "# Create a tensor dataset\n",
        "train_data = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n",
        "val_data = TensorDataset(val_input_ids, val_attention_mask, val_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp5Y6uxwOoHZ"
      },
      "source": [
        "Initialise a RandomSampler object, which shuffles the dataset's indices randomly. This ensures that samples are presented in a different order during each epoch of training, helping to prevent model overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "35N2NWipOZdk"
      },
      "outputs": [],
      "source": [
        "# Create a random sampler to help avoid overfitting\n",
        "train_sampler = RandomSampler(train_data)\n",
        "val_sampler = SequentialSampler(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LiO1RVlO3an"
      },
      "source": [
        "Constructs a DataLoader object, which is responsible for loading data in batches for training.\n",
        "\n",
        "It takes the following arguments:\n",
        "- train_data: The dataset to load data from.\n",
        "- sampler: The sampler object to use for shuffling (in this case, train_sampler).\n",
        "- batch_size: The number of samples to include in each batch.\n",
        "\n",
        "The DataLoader iterates over the dataset, yielding batches of tensors in the format (input_ids, attention_mask, labels) for each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "gLF3WSgTOaD9"
      },
      "outputs": [],
      "source": [
        "# Load data into the dataloader\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "-I2eWSfSVQfg"
      },
      "outputs": [],
      "source": [
        "def get_loss_on_validation_set(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on validation dataset and returns the loss.\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate.\n",
        "        dataloader: The DataLoader object for the test dataset.\n",
        "        device: The device (CPU or GPU) to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        Validation loss\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    total_val_loss = 0\n",
        "\n",
        "    # Iterate through validation data\n",
        "    for batch in dataloader:\n",
        "\n",
        "      val_input_ids = batch[0].to(device)\n",
        "      val_attention_mask = batch[1].to(device)\n",
        "      val_labels = batch[2].to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "        outputs = model(val_input_ids, attention_mask=val_attention_mask, labels=val_labels)\n",
        "\n",
        "      val_loss = outputs.loss\n",
        "      total_val_loss += val_loss.item()\n",
        "\n",
        "      # Calculate and return average validation loss\n",
        "      avg_val_loss = total_val_loss / len(dataloader)\n",
        "      return avg_val_loss\n",
        "    \n",
        "\n",
        "# Evaluate the model\n",
        "def evaluate_model(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a given dataset and creates a classification report.\n",
        "\n",
        "    Args:\n",
        "        model: The model to evaluate.\n",
        "        dataloader: The DataLoader object for the test dataset.\n",
        "        device: The device (CPU or GPU) to use for evaluation.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    # Make predictions on this model on validation set\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Extract tensors for inputs, attention masks, and labels from the batch and moves them to the designated device (CPU or GPU)\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Disable gradient calculation for efficiency, as gradients are not required during evaluation.\n",
        "        with torch.no_grad():\n",
        "            # Pass the input tensors through the model to obtain predictions\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the raw prediction scores (logits) from model outputs.\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Convert model predictions to the class with the highest probability for each\n",
        "        predictions.extend(torch.argmax(logits, axis=1).tolist())\n",
        "\n",
        "        # Append the true labels from the batch to the true_labels list\n",
        "        true_labels.extend(labels.tolist())\n",
        "\n",
        "    # Decode numerical predictions back to their original textual labels using a label_encoder object\n",
        "    predicted_labels = label_encoder.inverse_transform(predictions)\n",
        "    true_labels = label_encoder.inverse_transform(true_labels)\n",
        "\n",
        "    # Print a classification report summarizing model performance, including precision, recall, F1-score, and support for each class\n",
        "    print(classification_report(true_labels, predicted_labels))\n",
        "      \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc2mfiyKPC0s",
        "outputId": "df4bd952-2852-448e-8142-300cc6bc98b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1, Training Loss:  0.00895, Validation Loss:  0.03363, Learning Rate: 0.0000500\n",
            "Epoch: 2, Training Loss:  0.00851, Validation Loss:  0.03501, Learning Rate: 0.0000500\n",
            "Epoch: 3, Training Loss:  0.05028, Validation Loss:  0.02252, Learning Rate: 0.0000500\n",
            "Epoch: 4, Training Loss:  0.00885, Validation Loss:  0.01925, Learning Rate: 0.0000500\n",
            "Epoch: 5, Training Loss:  0.00061, Validation Loss:  0.02017, Learning Rate: 0.0000500\n",
            "Epoch: 6, Training Loss:  0.00026, Validation Loss:  0.02070, Learning Rate: 0.0000500\n",
            "Epoch: 7, Training Loss:  0.00018, Validation Loss:  0.02116, Learning Rate: 0.0000050\n",
            "Early stopping triggered at epoch 6\n"
          ]
        }
      ],
      "source": [
        "max_iters = 10 # Number of epochs\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 3  # Number of epochs to wait without improvement\n",
        "best_loss = float('inf')  # Initialize best loss as infinity\n",
        "early_stopping_count = 0\n",
        "\n",
        "# L2 regularization weight (optional)\n",
        "weight_decay = 0.0001  # Adjust as needed\n",
        "\n",
        "# Set optimizer and learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
        "\n",
        "# Define learning rate scheduler (ReduceLROnPlateau monitors validation loss)\n",
        "scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=2)  # Adjust parameters as needed\n",
        "\n",
        "# Train model\n",
        "model.train()\n",
        "\n",
        "for epoch in range(max_iters):\n",
        "    \n",
        "    # Set training loss as 0 in the beginning\n",
        "    total_train_loss = 0\n",
        "    \n",
        "    for batch in train_dataloader:\n",
        "        # Clear the gradients accumulated from previous iterations, ensuring gradients are calculated for the current batch only.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Extracts the tensors for input, attention mask, and labels from the batch and moves them to the specified device (CPU or GPU)\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_mask = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "\n",
        "        # Forward pass to get outputs\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        # Get the loss value from the model's outputs, indicating how well the predictions align with the true labels.\n",
        "        #loss = outputs.loss\n",
        "        loss = outputs[0]\n",
        "        total_train_loss += loss.item()\n",
        "        avg_train_loss = total_train_loss/len(train_dataloader)\n",
        "\n",
        "        # Perform backpropagation, calculating gradients of the loss with respect to the model's parameters.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the model's parameters based on the calculated gradients, adjusting them to minimize the loss.\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    # ---- Early stopping and Validation (explanation in the next cell) ----\n",
        "\n",
        "    # L2 regularization (optional)\n",
        "    if weight_decay > 0:\n",
        "        l2_reg = 0\n",
        "        for param in model.parameters():\n",
        "            l2_reg += torch.norm(param)**2\n",
        "        loss += weight_decay * l2_reg\n",
        "\n",
        "    # Validation\n",
        "    avg_val_loss = get_loss_on_validation_set(model, val_dataloader, device)\n",
        "\n",
        "    if avg_val_loss < best_loss:\n",
        "        best_loss = avg_val_loss\n",
        "        early_stopping_count = 0  # Reset counter on improvement\n",
        "    else:\n",
        "        early_stopping_count += 1\n",
        "\n",
        "    # Checking for adjustment of learning rate\n",
        "    # Update learning rate based on validation loss (using scheduler)\n",
        "    scheduler.step(avg_val_loss)  # Pass validation loss to scheduler\n",
        "\n",
        "    # Access learning rate\n",
        "    learning_rate = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # Print the loss after each epoch\n",
        "    print(f\"Epoch: {epoch+1}, Training Loss: {avg_train_loss: .5f}, Validation Loss: {avg_val_loss: .5f}, Learning Rate: {learning_rate:.7f}\")\n",
        "\n",
        "    # Stop if val loss is higher than training loss after accounting for patience\n",
        "    if early_stopping_count >= patience:\n",
        "        print('Early stopping triggered at epoch', epoch)\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO2OyQGjaOmN"
      },
      "source": [
        "**Explanation of the early stopping and L2 regularisation**\n",
        "\n",
        "**L2 Regularization:**\n",
        "\n",
        "This section is only executed if weight_decay is greater than 0. It calculates the L2 norm of all model parameters. L2 norm represents the sum of squares of each element in the parameter tensor. The calculated L2 norm is multiplied by weight_decay (a hyperparameter). This value is then added to the existing training loss (loss).\n",
        "\n",
        "\n",
        "**Validation and Early Stopping:**\n",
        "\n",
        "- The evaluate_model function calculates the validation loss on the val_dataloader dataset.\n",
        "\n",
        "- The validation loss (val_loss) is compared to the current best_loss.\n",
        ">- If the validation loss is lower than the best_loss : best_loss is updated to the current validation loss and early_stopping_count is reset to 0.\n",
        ">- Otherwise (else block) early_stopping_count is incremented.\n",
        "\n",
        "- If early_stopping_count reaches or exceeds patience (number of epochs to wait without improvement):\n",
        ">- Early stopping is triggered, and training is terminated.\n",
        ">- A message indicating early stopping at the current epoch is printed.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               precision    recall  f1-score   support\n",
            "\n",
            "     business       0.99      1.00      1.00       102\n",
            "entertainment       1.00      0.97      0.99        77\n",
            "     politics       1.00      1.00      1.00        84\n",
            "        sport       1.00      1.00      1.00       102\n",
            "         tech       0.99      1.00      0.99        80\n",
            "\n",
            "     accuracy                           1.00       445\n",
            "    macro avg       1.00      0.99      1.00       445\n",
            " weighted avg       1.00      1.00      1.00       445\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on the validation dataset\n",
        "evaluate_model(model=model, dataloader=val_dataloader, device=device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), f'saved_model/Topic Model Supervised/finetuned_BERT.model')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run on an unseen dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABC Corp is looking to acquire XYZ in a new la...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A new prototype has been invented that could r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leonil Messy becomes the ultimate GOAT in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It is still 6 months till elections, but the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chillian Turphy wins his thirs grammy after th...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content\n",
              "0  ABC Corp is looking to acquire XYZ in a new la...\n",
              "1  A new prototype has been invented that could r...\n",
              "2  Leonil Messy becomes the ultimate GOAT in the ...\n",
              "3  It is still 6 months till elections, but the c...\n",
              "4  Chillian Turphy wins his thirs grammy after th..."
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a test dataset\n",
        "test_data = [\n",
        "    \"ABC Corp is looking to acquire XYZ in a new landmark deal worth billions\",\n",
        "    \"A new prototype has been invented that could revolutionise how we build rockets and ships\",\n",
        "    \"Leonil Messy becomes the ultimate GOAT in the final appearance where he scored 4 times in 18 minutes\",\n",
        "    \"It is still 6 months till elections, but the candidates are already ramping up their propaganda machines\",\n",
        "    \"Chillian Turphy wins his thirs grammy after this album 'its not the real me'\"\n",
        "]\n",
        "\n",
        "# Convert to dataframe\n",
        "df_test = pd.DataFrame(test_data, columns=[\"content\"])\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = tokenizer.batch_encode_plus(df_test.content.values,\n",
        "                                          max_length=256,\n",
        "                                          padding='max_length',\n",
        "                                          truncation=True,\n",
        "                                          return_tensors='pt',\n",
        "                                          add_special_tokens=True)\n",
        "\n",
        "\n",
        "# Convert the input ids, attention mask and target labels to tensor type\n",
        "test_input_ids = tokens['input_ids']\n",
        "test_attention_mask = tokens['attention_mask']\n",
        "\n",
        "# Create a tensor dataset\n",
        "test_data = TensorDataset(test_input_ids, test_attention_mask)\n",
        "\n",
        "# Load data into the dataloader\n",
        "test_dataloader = DataLoader(test_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "preds = []\n",
        "\n",
        "for dat in test_dataloader:\n",
        "    input_ids = dat[0].to(device)\n",
        "    attention_mask = dat[1].to(device)\n",
        "\n",
        "    # Disable gradient calculation for efficiency, as gradients are not required during evaluation.\n",
        "    with torch.no_grad():\n",
        "        # Pass the input tensors through the model to obtain predictions\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Extract the raw prediction scores (logits) from model outputs.\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Convert model predictions to the class with the highest probability for each\n",
        "    preds.extend(torch.argmax(logits, axis=1).tolist())\n",
        "\n",
        "# Convert target label to text\n",
        "predicted_labels = label_encoder.inverse_transform(preds)\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>predicted label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ABC Corp is looking to acquire XYZ in a new la...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A new prototype has been invented that could r...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Leonil Messy becomes the ultimate GOAT in the ...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It is still 6 months till elections, but the c...</td>\n",
              "      <td>politics</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chillian Turphy wins his thirs grammy after th...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             content predicted label\n",
              "0  ABC Corp is looking to acquire XYZ in a new la...        business\n",
              "1  A new prototype has been invented that could r...            tech\n",
              "2  Leonil Messy becomes the ultimate GOAT in the ...           sport\n",
              "3  It is still 6 months till elections, but the c...        politics\n",
              "4  Chillian Turphy wins his thirs grammy after th...   entertainment"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Append the predictions to the test dataframe\n",
        "df_test['predicted label'] = predicted_labels\n",
        "df_test.head(10)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
